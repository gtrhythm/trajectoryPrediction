{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trajectoryDataFile=open('/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt')\n",
    "\n",
    "count=0\n",
    "allLineList=[]\n",
    "count=0\n",
    "for count,line in enumerate(trajectoryDataFile):\n",
    "    #read a single line, remove space and enter\n",
    "    lineList=line.split(' ')\n",
    "    try:\n",
    "        while True:\n",
    "            lineList.remove('')\n",
    "    except:\n",
    "        try:\n",
    "            lineList.remove('\\n')\n",
    "        except:\n",
    "            pass\n",
    "        pass\n",
    "    for i in range(0,lineList.__len__()):\n",
    "        # convert string to float\n",
    "        lineList[i]=float(lineList[i])\n",
    "    allLineList.append(lineList)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#read a single line, remove space and enter\n",
    "# line=trajectoryDataFile.readline()\n",
    "# lineList=line.split(' ')\n",
    "# try:\n",
    "#     while True:\n",
    "#         lineList.remove('')\n",
    "# except:\n",
    "#     try:\n",
    "#         lineList.remove('\\n')\n",
    "#     except:\n",
    "#         pass\n",
    "#     pass\n",
    "# print(lineList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.475 0 2195.462 0 95.3 0 11.2 -11.2\n",
      "done\n",
      "dict_keys([1118846980200.0, 1118846980300.0, 1118846980400.0, 1118846980500.0, 1118846980600.0, 1118846980700.0, 1118846980800.0, 1118846980900.0, 1118846981000.0, 1118846981100.0, 1118846981200.0, 1118846981300.0, 1118846981400.0, 1118846981500.0, 1118846981600.0, 1118846981700.0, 1118846981800.0, 1118846981900.0, 1118846982000.0, 1118846982100.0, 1118846982200.0, 1118846982300.0, 1118846982400.0, 1118846982500.0, 1118846982600.0, 1118846982700.0, 1118846982800.0, 1118846982900.0, 1118846983000.0, 1118846983100.0, 1118846983200.0, 1118846983300.0, 1118846983400.0, 1118846983500.0, 1118846983600.0, 1118846983700.0, 1118846983800.0, 1118846983900.0, 1118846984000.0, 1118846984100.0, 1118846984200.0, 1118846984300.0, 1118846984400.0, 1118846984500.0, 1118846984600.0, 1118846984700.0, 1118846984800.0, 1118846984900.0, 1118846985000.0, 1118846985100.0, 1118846985200.0, 1118846985300.0, 1118846985400.0, 1118846985500.0, 1118846985600.0, 1118846985700.0, 1118846985800.0, 1118846985900.0, 1118846986000.0, 1118846986100.0, 1118846986200.0, 1118846986300.0, 1118846986400.0, 1118846986500.0, 1118846986600.0, 1118846986700.0, 1118846986800.0, 1118846986900.0, 1118846987000.0, 1118846987100.0, 1118846987200.0, 1118846987300.0, 1118846987400.0, 1118846987500.0, 1118846987600.0, 1118846987700.0, 1118846987800.0, 1118846987900.0, 1118846988000.0, 1118846988100.0, 1118846988200.0, 1118846988300.0, 1118846988400.0, 1118846988500.0, 1118846988600.0, 1118846988700.0, 1118846988800.0, 1118846988900.0, 1118846989000.0, 1118846989100.0, 1118846989200.0, 1118846989300.0, 1118846989400.0, 1118846989500.0, 1118846989600.0, 1118846989700.0, 1118846989800.0, 1118846989900.0, 1118846990000.0, 1118846990100.0, 1118846990200.0, 1118846990300.0, 1118846990400.0, 1118846990500.0, 1118846990600.0, 1118846990700.0, 1118846990800.0, 1118846990900.0, 1118846991000.0, 1118846991100.0, 1118846991200.0, 1118846991300.0, 1118846991400.0, 1118846991500.0, 1118846991600.0, 1118846991700.0, 1118846991800.0, 1118846991900.0, 1118846992000.0, 1118846992100.0, 1118846992200.0, 1118846992300.0, 1118846992400.0, 1118846992500.0, 1118846992600.0, 1118846992700.0, 1118846992800.0, 1118846992900.0, 1118846993000.0, 1118846993100.0, 1118846993200.0, 1118846993300.0, 1118846993400.0, 1118846993500.0, 1118846993600.0, 1118846993700.0, 1118846993800.0, 1118846993900.0, 1118846994000.0, 1118846994100.0, 1118846994200.0, 1118846994300.0, 1118846994400.0, 1118846994500.0, 1118846994600.0, 1118846994700.0, 1118846994800.0, 1118846994900.0, 1118846995000.0, 1118846995100.0, 1118846995200.0, 1118846995300.0, 1118846995400.0, 1118846995500.0, 1118846995600.0, 1118846995700.0, 1118846995800.0, 1118846995900.0, 1118846996000.0, 1118846996100.0, 1118846996200.0, 1118846996300.0, 1118846996400.0, 1118846996500.0, 1118846996600.0, 1118846996700.0, 1118846996800.0, 1118846996900.0, 1118846997000.0, 1118846997100.0, 1118846997200.0, 1118846997300.0, 1118846997400.0, 1118846997500.0, 1118846997600.0, 1118846997700.0, 1118846997800.0, 1118846997900.0, 1118846998000.0, 1118846998100.0, 1118846998200.0, 1118846998300.0, 1118846998400.0, 1118846998500.0, 1118846998600.0, 1118846998700.0, 1118846998800.0, 1118846998900.0, 1118846999000.0, 1118846999100.0, 1118846999200.0, 1118846999300.0, 1118846999400.0, 1118846999500.0, 1118846999600.0, 1118846999700.0, 1118846999800.0, 1118846999900.0, 1118847000000.0, 1118847000100.0, 1118847000200.0, 1118847000300.0, 1118847000400.0, 1118847000500.0, 1118847000600.0, 1118847000700.0, 1118847000800.0, 1118847000900.0, 1118847001000.0, 1118847001100.0, 1118847001200.0, 1118847001300.0, 1118847001400.0, 1118847001500.0, 1118847001600.0, 1118847001700.0, 1118847001800.0, 1118847001900.0, 1118847002000.0, 1118847002100.0, 1118847002200.0, 1118847002300.0, 1118847002400.0, 1118847002500.0, 1118847002600.0, 1118847002700.0, 1118847002800.0, 1118847002900.0, 1118847003000.0, 1118847003100.0, 1118847003200.0, 1118847003300.0, 1118847003400.0, 1118847003500.0, 1118847003600.0, 1118847003700.0, 1118847003800.0, 1118847003900.0, 1118847004000.0, 1118847004100.0, 1118847004200.0, 1118847004300.0, 1118847004400.0, 1118847004500.0, 1118847004600.0, 1118847004700.0, 1118847004800.0, 1118847004900.0, 1118847005000.0, 1118847005100.0, 1118847005200.0, 1118847005300.0, 1118847005400.0, 1118847005500.0, 1118847005600.0, 1118847005700.0, 1118847005800.0, 1118847005900.0, 1118847006000.0, 1118847006100.0, 1118847006200.0, 1118847006300.0, 1118847006400.0, 1118847006500.0, 1118847006600.0, 1118847006700.0, 1118847006800.0, 1118847006900.0, 1118847007000.0, 1118847007100.0, 1118847007200.0, 1118847007300.0, 1118847007400.0, 1118847007500.0, 1118847007600.0, 1118847007700.0, 1118847007800.0, 1118847007900.0, 1118847008000.0, 1118847008100.0, 1118847008200.0, 1118847008300.0, 1118847008400.0, 1118847008500.0, 1118847008600.0, 1118847008700.0, 1118847008800.0, 1118847008900.0, 1118847009000.0, 1118847009100.0, 1118847009200.0, 1118847009300.0, 1118847009400.0, 1118847009500.0, 1118847009600.0, 1118847009700.0, 1118847009800.0, 1118847009900.0, 1118847010000.0, 1118847010100.0, 1118847010200.0, 1118847010300.0, 1118847010400.0, 1118847010500.0, 1118847010600.0, 1118847010700.0, 1118847010800.0, 1118847010900.0, 1118847011000.0, 1118847011100.0, 1118847011200.0, 1118847011300.0, 1118847011400.0, 1118847011500.0, 1118847011600.0, 1118847011700.0, 1118847011800.0, 1118847011900.0, 1118847012000.0, 1118847012100.0, 1118847012200.0, 1118847012300.0, 1118847012400.0, 1118847012500.0, 1118847012600.0, 1118847012700.0, 1118847012800.0, 1118847012900.0, 1118847013000.0, 1118847013100.0, 1118847013200.0, 1118847013300.0, 1118847013400.0, 1118847013500.0, 1118847013600.0, 1118847013700.0, 1118847013800.0, 1118847013900.0, 1118847014000.0, 1118847014100.0, 1118847014200.0, 1118847014300.0, 1118847014400.0, 1118847014500.0, 1118847014600.0, 1118847014700.0, 1118847014800.0, 1118847014900.0, 1118847015000.0, 1118847015100.0, 1118847015200.0, 1118847015300.0, 1118847015400.0, 1118847015500.0, 1118847015600.0, 1118847015700.0, 1118847015800.0, 1118847015900.0, 1118847016000.0, 1118847016100.0, 1118847016200.0, 1118847016300.0, 1118847016400.0, 1118847016500.0, 1118847016600.0, 1118847016700.0, 1118847016800.0, 1118847016900.0, 1118847017000.0, 1118847017100.0, 1118847017200.0, 1118847017300.0, 1118847017400.0, 1118847017500.0, 1118847017600.0, 1118847017700.0, 1118847017800.0, 1118847017900.0, 1118847018000.0, 1118847018100.0, 1118847018200.0, 1118847018300.0, 1118847018400.0, 1118847018500.0, 1118847018600.0, 1118847018700.0, 1118847018800.0, 1118847018900.0, 1118847019000.0, 1118847019100.0, 1118847019200.0, 1118847019300.0, 1118847019400.0, 1118847019500.0, 1118847019600.0, 1118847019700.0, 1118847019800.0, 1118847019900.0, 1118847020000.0, 1118847020100.0, 1118847020200.0, 1118847020300.0, 1118847020400.0, 1118847020500.0, 1118847020600.0, 1118847020700.0, 1118847020800.0, 1118847020900.0, 1118847021000.0, 1118847021100.0, 1118847021200.0, 1118847021300.0, 1118847021400.0, 1118847021500.0, 1118847021600.0, 1118847021700.0, 1118847021800.0, 1118847021900.0, 1118847022000.0, 1118847022100.0, 1118847022200.0, 1118847022300.0, 1118847022400.0, 1118847022500.0, 1118847022600.0, 1118847022700.0, 1118847022800.0, 1118847022900.0, 1118847023000.0, 1118847023100.0, 1118847023200.0, 1118847023300.0, 1118847023400.0, 1118847023500.0, 1118847023600.0, 1118847023700.0, 1118847023800.0, 1118847023900.0, 1118847024000.0, 1118847024100.0, 1118847024200.0, 1118847024300.0, 1118847024400.0, 1118847024500.0, 1118847024600.0, 1118847024700.0, 1118847024800.0, 1118847024900.0, 1118847025000.0, 1118847025100.0, 1118847025200.0, 1118847025300.0, 1118847025400.0, 1118847025500.0, 1118847025600.0, 1118847025700.0, 1118847025800.0, 1118847025900.0, 1118847026000.0, 1118847026100.0, 1118847026200.0, 1118847026300.0, 1118847026400.0, 1118847026500.0, 1118847026600.0, 1118847026700.0, 1118846979700.0, 1118846979800.0, 1118846979900.0, 1118846980000.0, 1118846980100.0, 1118847026800.0, 1118847026900.0, 1118847027000.0, 1118847027100.0, 1118847027200.0, 1118847027300.0, 1118847027400.0, 1118847027500.0, 1118847027600.0, 1118847027700.0, 1118847027800.0, 1118847027900.0, 1118847028000.0, 1118847028100.0, 1118847028200.0, 1118847028300.0, 1118847028400.0, 1118847028500.0, 1118847028600.0, 1118847028700.0, 1118847028800.0, 1118847028900.0, 1118847029000.0, 1118847029100.0, 1118847029200.0, 1118847029300.0, 1118847029400.0, 1118847029500.0, 1118847029600.0, 1118847029700.0, 1118847029800.0, 1118847029900.0, 1118847030000.0, 1118847030100.0, 1118847030200.0, 1118847030300.0, 1118847030400.0, 1118847030500.0, 1118847030600.0, 1118847030700.0, 1118847030800.0, 1118847030900.0, 1118847031000.0, 1118847031100.0, 1118847031200.0, 1118847031300.0, 1118847031400.0, 1118847031500.0, 1118847031600.0, 1118847031700.0, 1118847031800.0, 1118847031900.0, 1118847032000.0, 1118847032100.0, 1118847032200.0, 1118847032300.0, 1118847032400.0, 1118847032500.0, 1118847032600.0, 1118847032700.0, 1118847032800.0, 1118847032900.0, 1118847033000.0, 1118847033100.0, 1118847033200.0, 1118847033300.0, 1118847033400.0, 1118847033500.0, 1118847033600.0, 1118847033700.0, 1118847033800.0, 1118847033900.0, 1118847034000.0, 1118847034100.0, 1118847034200.0, 1118847034300.0, 1118847034400.0, 1118847034500.0, 1118847034600.0, 1118847034700.0, 1118847034800.0, 1118847034900.0, 1118847035000.0, 1118847035100.0, 1118847035200.0, 1118847035300.0, 1118847035400.0, 1118847035500.0, 1118847035600.0, 1118847035700.0, 1118847035800.0, 1118847035900.0, 1118847036000.0, 1118847036100.0, 1118847036200.0, 1118847036300.0, 1118847036400.0, 1118847036500.0, 1118847036600.0, 1118847036700.0, 1118847036800.0, 1118847036900.0, 1118847037000.0, 1118847037100.0, 1118847037200.0, 1118847037300.0, 1118847037400.0, 1118847037500.0, 1118847037600.0, 1118847037700.0, 1118847037800.0, 1118847037900.0, 1118847038000.0, 1118847038100.0, 1118847038200.0, 1118847038300.0, 1118847038400.0, 1118847038500.0, 1118847038600.0, 1118847038700.0, 1118847038800.0, 1118847038900.0, 1118847039000.0, 1118847039100.0, 1118847039200.0, 1118847039300.0, 1118847039400.0, 1118847039500.0, 1118847039600.0, 1118847039700.0, 1118847039800.0, 1118847039900.0, 1118847040000.0, 1118847040100.0, 1118847040200.0, 1118847040300.0, 1118847040400.0, 1118847040500.0, 1118847040600.0, 1118847040700.0, 1118847040800.0, 1118847040900.0, 1118847041000.0, 1118847041100.0, 1118847041200.0, 1118847041300.0, 1118847041400.0, 1118847041500.0, 1118847041600.0, 1118847041700.0, 1118847041800.0, 1118847041900.0, 1118847042000.0, 1118847042100.0, 1118847042200.0, 1118847042300.0, 1118847042400.0, 1118847042500.0, 1118847042600.0, 1118847042700.0, 1118847042800.0, 1118847042900.0, 1118847043000.0, 1118847043100.0, 1118847043200.0, 1118847043300.0, 1118847043400.0, 1118847043500.0, 1118847043600.0, 1118847043700.0, 1118847043800.0, 1118847043900.0, 1118847044000.0, 1118847044100.0, 1118847044200.0, 1118847044300.0, 1118847044400.0, 1118847044500.0, 1118847044600.0, 1118847044700.0, 1118847044800.0, 1118847044900.0, 1118847045000.0, 1118847045100.0, 1118847045200.0, 1118847045300.0, 1118847045400.0, 1118847045500.0, 1118847045600.0, 1118847045700.0, 1118847045800.0, 1118847045900.0, 1118847046000.0, 1118847046100.0, 1118847046200.0, 1118847046300.0, 1118847046400.0, 1118847046500.0, 1118847046600.0, 1118847046700.0, 1118847046800.0, 1118847046900.0, 1118847047000.0, 1118847047100.0, 1118847047200.0, 1118847047300.0, 1118847047400.0, 1118847047500.0, 1118847047600.0, 1118847047700.0, 1118847047800.0, 1118847047900.0, 1118847048000.0, 1118847048100.0, 1118847048200.0, 1118847048300.0, 1118847048400.0, 1118847048500.0, 1118847048600.0, 1118847048700.0, 1118847048800.0, 1118847048900.0, 1118847049000.0, 1118847049100.0, 1118847049200.0, 1118847049300.0, 1118847049400.0, 1118847049500.0, 1118847049600.0, 1118847049700.0, 1118847049800.0, 1118847049900.0, 1118847050000.0, 1118847050100.0, 1118847050200.0, 1118847050300.0, 1118847050400.0, 1118847050500.0, 1118847050600.0, 1118847050700.0, 1118847050800.0, 1118847050900.0, 1118847051000.0, 1118847051100.0, 1118847051200.0, 1118847051300.0, 1118847051400.0, 1118847051500.0, 1118847051600.0, 1118847051700.0, 1118847051800.0, 1118847051900.0, 1118847052000.0, 1118847052100.0, 1118847052200.0, 1118847052300.0, 1118847052400.0, 1118847052500.0, 1118847052600.0, 1118847052700.0, 1118847052800.0, 1118847052900.0, 1118847053000.0, 1118847053100.0, 1118847053200.0, 1118847053300.0, 1118847053400.0, 1118847053500.0, 1118847053600.0, 1118847053700.0, 1118847053800.0, 1118847053900.0, 1118847054000.0, 1118847054100.0, 1118847054200.0, 1118847054300.0, 1118847054400.0, 1118847054500.0, 1118847054600.0, 1118847054700.0, 1118847054800.0, 1118847054900.0, 1118847055000.0, 1118847055100.0, 1118847055200.0, 1118847055300.0, 1118847055400.0, 1118847055500.0, 1118847055600.0, 1118847055700.0, 1118847055800.0, 1118847055900.0, 1118847056000.0, 1118847056100.0, 1118847056200.0, 1118847056300.0, 1118847056400.0, 1118847056500.0, 1118847056600.0, 1118847056700.0, 1118847056800.0, 1118847056900.0, 1118847057000.0, 1118847057100.0, 1118847057200.0, 1118847057300.0, 1118847057400.0, 1118847057500.0, 1118847057600.0, 1118847057700.0, 1118847057800.0, 1118847057900.0, 1118847058000.0, 1118847058100.0, 1118847058200.0, 1118847058300.0, 1118847058400.0, 1118847058500.0, 1118847058600.0, 1118847058700.0, 1118847058800.0, 1118847058900.0, 1118847059000.0, 1118847059100.0, 1118847059200.0, 1118847059300.0, 1118847059400.0, 1118847059500.0, 1118847059600.0, 1118847059700.0, 1118847059800.0, 1118847059900.0, 1118847060000.0, 1118847060100.0, 1118847060200.0, 1118847060300.0, 1118847060400.0, 1118847060500.0, 1118847060600.0, 1118847060700.0, 1118847060800.0, 1118847060900.0, 1118847061000.0, 1118847061100.0, 1118847061200.0, 1118847061300.0, 1118847061400.0, 1118847061500.0, 1118847061600.0, 1118847061700.0, 1118847061800.0, 1118847061900.0, 1118847062000.0, 1118847062100.0, 1118847062200.0, 1118847062300.0, 1118847062400.0, 1118847062500.0, 1118847062600.0, 1118847062700.0, 1118847062800.0, 1118847062900.0, 1118847063000.0, 1118847063100.0, 1118847063200.0, 1118847063300.0, 1118847063400.0, 1118847063500.0, 1118847063600.0, 1118847063700.0, 1118847063800.0, 1118847063900.0, 1118847064000.0, 1118847064100.0, 1118847064200.0, 1118847064300.0, 1118847064400.0, 1118847064500.0, 1118847064600.0, 1118847064700.0, 1118847064800.0, 1118847064900.0, 1118847065000.0, 1118847065100.0, 1118847065200.0, 1118847065300.0, 1118847065400.0, 1118847065500.0, 1118847065600.0, 1118847065700.0, 1118847065800.0, 1118847065900.0, 1118847066000.0, 1118847066100.0, 1118847066200.0, 1118847066300.0, 1118847066400.0, 1118847066500.0, 1118847066600.0, 1118847066700.0, 1118847066800.0, 1118847066900.0, 1118847067000.0, 1118847067100.0, 1118847067200.0, 1118847067300.0, 1118847067400.0, 1118847067500.0, 1118847067600.0, 1118847067700.0, 1118847067800.0, 1118847067900.0, 1118847068000.0, 1118847068100.0, 1118847068200.0, 1118847068300.0, 1118847068400.0, 1118847068500.0, 1118847068600.0, 1118847068700.0, 1118847068800.0, 1118847068900.0, 1118847069000.0, 1118847069100.0, 1118847069200.0, 1118847069300.0, 1118847069400.0, 1118847069500.0, 1118847069600.0, 1118847069700.0, 1118847069800.0, 1118847069900.0, 1118847070000.0, 1118847070100.0, 1118847070200.0, 1118847070300.0, 1118847070400.0, 1118847070500.0, 1118847070600.0, 1118847070700.0, 1118847070800.0, 1118847070900.0, 1118847071000.0, 1118847071100.0, 1118847071200.0, 1118847071300.0, 1118847071400.0, 1118847071500.0, 1118847071600.0, 1118847071700.0, 1118847071800.0, 1118847071900.0, 1118847072000.0, 1118847072100.0, 1118847072200.0, 1118847072300.0, 1118847072400.0, 1118847072500.0, 1118847072600.0, 1118847072700.0, 1118847072800.0, 1118847072900.0, 1118847073000.0, 1118847073100.0, 1118847073200.0, 1118847073300.0, 1118847073400.0, 1118847073500.0, 1118847073600.0, 1118847073700.0, 1118847073800.0, 1118847073900.0, 1118847074000.0, 1118847074100.0, 1118847074200.0, 1118847074300.0, 1118847074400.0, 1118847074500.0, 1118847074600.0, 1118847074700.0, 1118847074800.0, 1118847074900.0, 1118847075000.0, 1118847075100.0, 1118847075200.0, 1118847075300.0, 1118847075400.0, 1118847075500.0, 1118847075600.0, 1118847075700.0, 1118847075800.0, 1118847075900.0, 1118847076000.0, 1118847076100.0, 1118847076200.0, 1118847076300.0, 1118847076400.0, 1118847076500.0, 1118847076600.0, 1118847076700.0, 1118847076800.0, 1118847076900.0, 1118847077000.0, 1118847077100.0, 1118847077200.0, 1118847077300.0, 1118847077400.0, 1118847077500.0, 1118847077600.0, 1118847077700.0, 1118847077800.0, 1118847077900.0, 1118847078000.0, 1118847078100.0, 1118847078200.0, 1118847078300.0, 1118847078400.0, 1118847078500.0, 1118847078600.0, 1118847078700.0, 1118847078800.0, 1118847078900.0, 1118847079000.0, 1118847079100.0, 1118847079200.0, 1118847079300.0, 1118847079400.0, 1118847079500.0, 1118847079600.0, 1118847079700.0, 1118847079800.0, 1118847079900.0, 1118847080000.0, 1118847080100.0, 1118847080200.0, 1118847080300.0, 1118847080400.0, 1118847080500.0, 1118847080600.0, 1118847080700.0, 1118847080800.0, 1118847080900.0, 1118847081000.0, 1118847081100.0, 1118847081200.0, 1118847081300.0, 1118847081400.0, 1118847081500.0, 1118847081600.0, 1118847081700.0, 1118847081800.0, 1118847081900.0, 1118847082000.0, 1118847082100.0, 1118847082200.0, 1118847082300.0, 1118847082400.0, 1118847082500.0, 1118847082600.0, 1118847082700.0, 1118847082800.0, 1118847082900.0, 1118847083000.0, 1118847083100.0, 1118847083200.0, 1118847083300.0, 1118847083400.0, 1118847083500.0, 1118847083600.0, 1118847083700.0, 1118847083800.0, 1118847083900.0, 1118847084000.0, 1118847084100.0, 1118847084200.0, 1118847084300.0, 1118847084400.0, 1118847084500.0, 1118847084600.0, 1118847084700.0, 1118847084800.0, 1118847084900.0, 1118847085000.0, 1118847085100.0, 1118847085200.0, 1118847085300.0, 1118847085400.0, 1118847085500.0, 1118847085600.0, 1118847085700.0, 1118847085800.0, 1118847085900.0, 1118847086000.0, 1118847086100.0, 1118847086200.0, 1118847086300.0, 1118847086400.0, 1118847086500.0, 1118847086600.0, 1118847086700.0, 1118847086800.0, 1118847086900.0, 1118847087000.0, 1118847087100.0, 1118847087200.0, 1118847087300.0, 1118847087400.0, 1118847087500.0, 1118847087600.0, 1118847087700.0, 1118847087800.0, 1118847087900.0, 1118847088000.0, 1118847088100.0, 1118847088200.0, 1118847088300.0, 1118847088400.0, 1118847088500.0, 1118847088600.0, 1118847088700.0, 1118847088800.0, 1118847088900.0, 1118847089000.0, 1118847089100.0, 1118847089200.0, 1118847089300.0, 1118847089400.0, 1118847089500.0, 1118847089600.0, 1118847089700.0, 1118847089800.0, 1118847089900.0, 1118847090000.0, 1118847090100.0, 1118847090200.0, 1118847090300.0, 1118847090400.0, 1118847090500.0, 1118847090600.0, 1118847090700.0, 1118847090800.0, 1118847090900.0, 1118847091000.0, 1118847091100.0, 1118847091200.0, 1118847091300.0, 1118847091400.0, 1118847091500.0, 1118847091600.0, 1118847091700.0, 1118847091800.0, 1118847091900.0, 1118847092000.0, 1118847092100.0, 1118847092200.0, 1118847092300.0, 1118847092400.0, 1118847092500.0, 1118847092600.0, 1118847092700.0, 1118847092800.0, 1118847092900.0, 1118847093000.0, 1118847093100.0, 1118847093200.0, 1118847093300.0, 1118847093400.0, 1118847093500.0, 1118847093600.0, 1118847093700.0, 1118847093800.0, 1118847093900.0, 1118847094000.0, 1118847094100.0, 1118847094200.0, 1118847094300.0, 1118847094400.0, 1118847094500.0, 1118847094600.0, 1118847094700.0, 1118847094800.0, 1118847094900.0, 1118847095000.0, 1118847095100.0, 1118847095200.0, 1118847095300.0, 1118847095400.0, 1118847095500.0, 1118847095600.0, 1118847095700.0, 1118847095800.0, 1118847095900.0, 1118847096000.0, 1118847096100.0, 1118847096200.0, 1118847096300.0, 1118847096400.0, 1118847096500.0, 1118847096600.0, 1118847096700.0, 1118847096800.0, 1118847096900.0, 1118847097000.0, 1118847097100.0, 1118847097200.0, 1118847097300.0, 1118847097400.0, 1118847097500.0, 1118847097600.0, 1118847097700.0, 1118847097800.0, 1118847097900.0, 1118847098000.0, 1118847098100.0, 1118847098200.0, 1118847098300.0, 1118847098400.0, 1118847098500.0, 1118847098600.0, 1118847098700.0, 1118847098800.0, 1118847098900.0, 1118847099000.0, 1118847099100.0, 1118847099200.0, 1118847099300.0, 1118847099400.0, 1118847099500.0, 1118847099600.0, 1118847099700.0, 1118847099800.0, 1118847099900.0, 1118847100000.0, 1118847100100.0, 1118847100200.0, 1118847100300.0, 1118847100400.0, 1118847100500.0, 1118847100600.0, 1118847100700.0, 1118847100800.0, 1118847100900.0, 1118847101000.0, 1118847101100.0, 1118847101200.0, 1118847101300.0, 1118847101400.0, 1118847101500.0, 1118847101600.0, 1118847101700.0, 1118847101800.0, 1118847101900.0, 1118847102000.0, 1118847102100.0, 1118847102200.0, 1118847102300.0, 1118847102400.0, 1118847102500.0, 1118847102600.0, 1118847102700.0, 1118847102800.0, 1118847102900.0, 1118847103000.0, 1118847103100.0, 1118847103200.0, 1118847103300.0, 1118847103400.0, 1118847103500.0, 1118847103600.0, 1118847103700.0, 1118847103800.0, 1118847103900.0, 1118847104000.0, 1118847104100.0, 1118847104200.0, 1118847104300.0, 1118847104400.0, 1118847104500.0, 1118847104600.0, 1118847104700.0, 1118847104800.0, 1118847104900.0, 1118847105000.0, 1118847105100.0, 1118847105200.0, 1118847105300.0, 1118847105400.0, 1118847105500.0, 1118847105600.0, 1118847105700.0, 1118847105800.0, 1118847105900.0, 1118847106000.0, 1118847106100.0, 1118847106200.0, 1118847106300.0, 1118847106400.0, 1118847106500.0, 1118847106600.0, 1118847106700.0, 1118847106800.0, 1118847106900.0, 1118847107000.0, 1118847107100.0, 1118847107200.0, 1118847107300.0, 1118847107400.0, 1118847107500.0, 1118847107600.0, 1118847107700.0, 1118847107800.0, 1118847107900.0, 1118847108000.0, 1118847108100.0, 1118847108200.0, 1118847108300.0, 1118847108400.0, 1118847108500.0, 1118847108600.0, 1118847108700.0, 1118847108800.0, 1118847108900.0, 1118847109000.0, 1118847109100.0, 1118847109200.0, 1118847109300.0, 1118847109400.0, 1118847109500.0, 1118847109600.0, 1118847109700.0, 1118847109800.0, 1118847109900.0, 1118847110000.0, 1118847110100.0, 1118847110200.0, 1118847110300.0, 1118847110400.0, 1118847110500.0, 1118847110600.0, 1118847110700.0, 1118847110800.0, 1118847110900.0, 1118847111000.0, 1118847111100.0, 1118847111200.0, 1118847111300.0, 1118847111400.0, 1118847111500.0, 1118847111600.0, 1118847111700.0, 1118847111800.0, 1118847111900.0, 1118847112000.0, 1118847112100.0, 1118847112200.0, 1118847112300.0, 1118847112400.0, 1118847112500.0, 1118847112600.0, 1118847112700.0, 1118847112800.0, 1118847112900.0, 1118847113000.0, 1118847113100.0, 1118847113200.0, 1118847113300.0, 1118847113400.0, 1118847113500.0, 1118847113600.0, 1118847113700.0, 1118847113800.0, 1118847113900.0, 1118847114000.0, 1118847114100.0, 1118847114200.0, 1118847114300.0, 1118847114400.0, 1118847114500.0, 1118847114600.0, 1118847114700.0, 1118847114800.0, 1118847114900.0, 1118847115000.0, 1118847115100.0, 1118847115200.0, 1118847115300.0, 1118847115400.0, 1118847115500.0, 1118847115600.0, 1118847115700.0, 1118847115800.0, 1118847115900.0, 1118847116000.0, 1118847116100.0, 1118847116200.0, 1118847116300.0, 1118847116400.0, 1118847116500.0, 1118847116600.0, 1118847116700.0, 1118847116800.0, 1118847116900.0, 1118847117000.0, 1118847117100.0, 1118847117200.0, 1118847117300.0, 1118847117400.0, 1118847117500.0, 1118847117600.0, 1118847117700.0, 1118847117800.0, 1118847117900.0, 1118847118000.0, 1118847118100.0, 1118847118200.0, 1118847118300.0, 1118847118400.0, 1118847118500.0, 1118847118600.0, 1118847118700.0, 1118847118800.0, 1118847118900.0, 1118847119000.0, 1118847119100.0, 1118847119200.0, 1118847119300.0, 1118847119400.0, 1118847119500.0, 1118847119600.0, 1118847119700.0, 1118847119800.0, 1118847119900.0, 1118847120000.0, 1118847120100.0, 1118847120200.0, 1118847120300.0, 1118847120400.0, 1118847120500.0, 1118847120600.0, 1118847120700.0, 1118847120800.0, 1118847120900.0, 1118847121000.0, 1118847121100.0, 1118847121200.0, 1118847121300.0, 1118847121400.0, 1118847121500.0, 1118847121600.0, 1118847121700.0, 1118847121800.0, 1118847121900.0, 1118847122000.0, 1118847122100.0, 1118847122200.0, 1118847122300.0, 1118847122400.0, 1118847122500.0, 1118847122600.0, 1118847122700.0, 1118847122800.0, 1118847122900.0, 1118847123000.0, 1118847123100.0, 1118847123200.0, 1118847123300.0, 1118847123400.0, 1118847123500.0, 1118847123600.0, 1118847123700.0, 1118847123800.0, 1118847123900.0, 1118847124000.0, 1118847124100.0, 1118847124200.0, 1118847124300.0, 1118847124400.0, 1118847124500.0, 1118847124600.0, 1118847124700.0, 1118847124800.0, 1118847124900.0, 1118847125000.0, 1118847125100.0, 1118847125200.0, 1118847125300.0, 1118847125400.0, 1118847125500.0, 1118847125600.0, 1118847125700.0, 1118847125800.0, 1118847125900.0, 1118847126000.0, 1118847126100.0, 1118847126200.0, 1118847126300.0, 1118847126400.0, 1118847126500.0, 1118847126600.0, 1118847126700.0, 1118847126800.0, 1118847126900.0, 1118847127000.0, 1118847127100.0, 1118847127200.0, 1118847127300.0, 1118847127400.0, 1118847127500.0, 1118847127600.0, 1118847127700.0, 1118847127800.0, 1118847127900.0, 1118847128000.0, 1118847128100.0, 1118847128200.0, 1118847128300.0, 1118847128400.0, 1118847128500.0, 1118847128600.0, 1118847128700.0, 1118847128800.0, 1118847128900.0, 1118847129000.0, 1118847129100.0, 1118847129200.0, 1118847129300.0, 1118847129400.0, 1118847129500.0, 1118847129600.0, 1118847129700.0, 1118847129800.0, 1118847129900.0, 1118847130000.0, 1118847130100.0, 1118847130200.0, 1118847130300.0, 1118847130400.0, 1118847130500.0, 1118847130600.0, 1118847130700.0, 1118847130800.0, 1118847130900.0, 1118847131000.0, 1118847131100.0, 1118847131200.0, 1118847131300.0, 1118847131400.0, 1118847131500.0, 1118847131600.0, 1118847131700.0, 1118847131800.0, 1118847131900.0, 1118847132000.0, 1118847132100.0, 1118847132200.0, 1118847132300.0, 1118847132400.0, 1118847132500.0, 1118847132600.0, 1118847132700.0, 1118847132800.0, 1118847132900.0, 1118847133000.0, 1118847133100.0, 1118847133200.0, 1118847133300.0, 1118847133400.0, 1118847133500.0, 1118847133600.0, 1118847133700.0, 1118847133800.0, 1118847133900.0, 1118847134000.0, 1118847134100.0, 1118847134200.0, 1118847134300.0, 1118847134400.0, 1118847134500.0, 1118847134600.0, 1118847134700.0, 1118847134800.0, 1118847134900.0, 1118847135000.0, 1118847135100.0, 1118847135200.0, 1118847135300.0, 1118847135400.0, 1118847135500.0, 1118847135600.0, 1118847135700.0, 1118847135800.0, 1118847135900.0, 1118847136000.0, 1118847136100.0, 1118847136200.0, 1118847136300.0, 1118847136400.0, 1118847136500.0, 1118847136600.0, 1118847136700.0, 1118847136800.0, 1118847136900.0, 1118847137000.0, 1118847137100.0, 1118847137200.0, 1118847137300.0, 1118847137400.0, 1118847137500.0, 1118847137600.0, 1118847137700.0, 1118847137800.0, 1118847137900.0, 1118847138000.0, 1118847138100.0, 1118847138200.0, 1118847138300.0, 1118847138400.0, 1118847138500.0, 1118847138600.0, 1118847138700.0, 1118847138800.0, 1118847138900.0, 1118847139000.0, 1118847139100.0, 1118847139200.0, 1118847139300.0, 1118847139400.0, 1118847139500.0, 1118847139600.0, 1118847139700.0, 1118847139800.0, 1118847139900.0, 1118847140000.0, 1118847140100.0, 1118847140200.0, 1118847140300.0, 1118847140400.0, 1118847140500.0, 1118847140600.0, 1118847140700.0, 1118847140800.0, 1118847140900.0, 1118847141000.0, 1118847141100.0, 1118847141200.0, 1118847141300.0, 1118847141400.0, 1118847141500.0, 1118847141600.0, 1118847141700.0, 1118847141800.0, 1118847141900.0, 1118847142000.0, 1118847142100.0, 1118847142200.0, 1118847142300.0, 1118847142400.0, 1118847142500.0, 1118847142600.0, 1118847142700.0, 1118847142800.0, 1118847142900.0, 1118847143000.0, 1118847143100.0, 1118847143200.0, 1118847143300.0, 1118847143400.0, 1118847143500.0, 1118847143600.0, 1118847143700.0, 1118847143800.0, 1118847143900.0, 1118847144000.0, 1118847144100.0, 1118847144200.0, 1118847144300.0, 1118847144400.0, 1118847144500.0, 1118847144600.0, 1118847144700.0, 1118847144800.0, 1118847144900.0, 1118847145000.0, 1118847145100.0, 1118847145200.0, 1118847145300.0, 1118847145400.0, 1118847145500.0, 1118847145600.0, 1118847145700.0, 1118847145800.0, 1118847145900.0, 1118847146000.0, 1118847146100.0, 1118847146200.0, 1118847146300.0, 1118847146400.0, 1118847146500.0, 1118847146600.0, 1118847146700.0, 1118847146800.0, 1118847146900.0, 1118847147000.0, 1118847147100.0, 1118847147200.0, 1118847147300.0, 1118847147400.0, 1118847147500.0, 1118847147600.0, 1118847147700.0, 1118847147800.0, 1118847147900.0, 1118847148000.0, 1118847148100.0, 1118847148200.0, 1118847148300.0, 1118847148400.0, 1118847148500.0, 1118847148600.0, 1118847148700.0, 1118847148800.0, 1118847148900.0, 1118847149000.0, 1118847149100.0, 1118847149200.0, 1118847149300.0, 1118847149400.0, 1118847149500.0, 1118847149600.0, 1118847149700.0, 1118847149800.0, 1118847149900.0, 1118847150000.0, 1118847150100.0, 1118847150200.0, 1118847150300.0, 1118847150400.0, 1118847150500.0, 1118847150600.0, 1118847150700.0, 1118847150800.0, 1118847150900.0, 1118847151000.0, 1118847151100.0, 1118847151200.0, 1118847151300.0, 1118847151400.0, 1118847151500.0, 1118847151600.0, 1118847151700.0, 1118847151800.0, 1118847151900.0, 1118847152000.0, 1118847152100.0, 1118847152200.0, 1118847152300.0, 1118847152400.0, 1118847152500.0, 1118847152600.0, 1118847152700.0, 1118847152800.0, 1118847152900.0, 1118847153000.0, 1118847153100.0, 1118847153200.0, 1118847153300.0, 1118847153400.0, 1118847153500.0, 1118847153600.0, 1118847153700.0, 1118847153800.0, 1118847153900.0, 1118847154000.0, 1118847154100.0, 1118847154200.0, 1118847154300.0, 1118847154400.0, 1118847154500.0, 1118847154600.0, 1118847154700.0, 1118847154800.0, 1118847154900.0, 1118847155000.0, 1118847155100.0, 1118847155200.0, 1118847155300.0, 1118847155400.0, 1118847155500.0, 1118847155600.0, 1118847155700.0, 1118847155800.0, 1118847155900.0, 1118847156000.0, 1118847156100.0, 1118847156200.0, 1118847156300.0, 1118847156400.0, 1118847156500.0, 1118847156600.0, 1118847156700.0, 1118847156800.0, 1118847156900.0, 1118847157000.0, 1118847157100.0, 1118847157200.0, 1118847157300.0, 1118847157400.0, 1118847157500.0, 1118847157600.0, 1118847157700.0, 1118847157800.0, 1118847157900.0, 1118847158000.0, 1118847158100.0, 1118847158200.0, 1118847158300.0, 1118847158400.0, 1118847158500.0, 1118847158600.0, 1118847158700.0, 1118847158800.0, 1118847158900.0, 1118847159000.0, 1118847159100.0, 1118847159200.0, 1118847159300.0, 1118847159400.0, 1118847159500.0, 1118847159600.0, 1118847159700.0, 1118847159800.0, 1118847159900.0, 1118847160000.0, 1118847160100.0, 1118847160200.0, 1118847160300.0, 1118847160400.0, 1118847160500.0, 1118847160600.0, 1118847160700.0, 1118847160800.0, 1118847160900.0, 1118847161000.0, 1118847161100.0, 1118847161200.0, 1118847161300.0, 1118847161400.0, 1118847161500.0, 1118847161600.0, 1118847161700.0, 1118847161800.0, 1118847161900.0, 1118847162000.0, 1118847162100.0, 1118847162200.0, 1118847162300.0, 1118847162400.0, 1118847162500.0, 1118847162600.0, 1118847162700.0, 1118847162800.0, 1118847162900.0, 1118847163000.0, 1118847163100.0, 1118847163200.0, 1118847163300.0, 1118847163400.0, 1118847163500.0, 1118847163600.0, 1118847163700.0, 1118847163800.0, 1118847163900.0, 1118847164000.0, 1118847164100.0, 1118847164200.0, 1118847164300.0, 1118847164400.0, 1118847164500.0, 1118847164600.0, 1118847164700.0, 1118847164800.0, 1118847164900.0, 1118847165000.0, 1118847165100.0, 1118847165200.0, 1118847165300.0, 1118847165400.0, 1118847165500.0, 1118847165600.0, 1118847165700.0, 1118847165800.0, 1118847165900.0, 1118847166000.0, 1118847166100.0, 1118847166200.0, 1118847166300.0, 1118847166400.0, 1118847166500.0, 1118847166600.0, 1118847166700.0, 1118847166800.0, 1118847166900.0, 1118847167000.0, 1118847167100.0, 1118847167200.0, 1118847167300.0, 1118847167400.0, 1118847167500.0, 1118847167600.0, 1118847167700.0, 1118847167800.0, 1118847167900.0, 1118847168000.0, 1118847168100.0, 1118847168200.0, 1118847168300.0, 1118847168400.0, 1118847168500.0, 1118847168600.0, 1118847168700.0, 1118847168800.0, 1118847168900.0, 1118847169000.0, 1118847169100.0, 1118847169200.0, 1118847169300.0, 1118847169400.0, 1118847169500.0, 1118847169600.0, 1118847169700.0, 1118847169800.0, 1118847169900.0, 1118847170000.0, 1118847170100.0, 1118847170200.0, 1118847170300.0, 1118847170400.0, 1118847170500.0, 1118847170600.0, 1118847170700.0, 1118847170800.0, 1118847170900.0, 1118847171000.0, 1118847171100.0, 1118847171200.0, 1118847171300.0, 1118847171400.0, 1118847171500.0, 1118847171600.0, 1118847171700.0, 1118847171800.0, 1118847171900.0, 1118847172000.0, 1118847172100.0, 1118847172200.0, 1118847172300.0, 1118847172400.0, 1118847172500.0, 1118847172600.0, 1118847172700.0, 1118847172800.0, 1118847172900.0, 1118847173000.0, 1118847173100.0, 1118847173200.0, 1118847173300.0, 1118847173400.0, 1118847173500.0, 1118847173600.0, 1118847173700.0, 1118847173800.0, 1118847173900.0, 1118847174000.0, 1118847174100.0, 1118847174200.0, 1118847174300.0, 1118847174400.0, 1118847174500.0, 1118847174600.0, 1118847174700.0, 1118847174800.0, 1118847174900.0, 1118847175000.0, 1118847175100.0, 1118847175200.0, 1118847175300.0, 1118847175400.0, 1118847175500.0, 1118847175600.0, 1118847175700.0, 1118847175800.0, 1118847175900.0, 1118847176000.0, 1118847176100.0, 1118847176200.0, 1118847176300.0, 1118847176400.0, 1118847176500.0, 1118847176600.0, 1118847176700.0, 1118847176800.0, 1118847176900.0, 1118847177000.0, 1118847177100.0, 1118847177200.0, 1118847177300.0, 1118847177400.0, 1118847177500.0, 1118847177600.0, 1118847177700.0, 1118847177800.0, 1118847177900.0, 1118847178000.0, 1118847178100.0, 1118847178200.0, 1118847178300.0, 1118847178400.0, 1118847178500.0, 1118847178600.0, 1118847178700.0, 1118847178800.0, 1118847178900.0, 1118847179000.0, 1118847179100.0, 1118847179200.0, 1118847179300.0, 1118847179400.0, 1118847179500.0, 1118847179600.0, 1118847179700.0, 1118847179800.0, 1118847179900.0, 1118847180000.0, 1118847180100.0, 1118847180200.0, 1118847180300.0, 1118847180400.0, 1118847180500.0, 1118847180600.0, 1118847180700.0, 1118847180800.0, 1118847180900.0, 1118847181000.0, 1118847181100.0, 1118847181200.0, 1118847181300.0, 1118847181400.0, 1118847181500.0, 1118847181600.0, 1118847181700.0, 1118847181800.0, 1118847181900.0, 1118847182000.0, 1118847182100.0, 1118847182200.0, 1118847182300.0, 1118847182400.0, 1118847182500.0, 1118847182600.0, 1118847182700.0, 1118847182800.0, 1118847182900.0, 1118847183000.0, 1118847183100.0, 1118847183200.0, 1118847183300.0, 1118847183400.0, 1118847183500.0, 1118847183600.0, 1118847183700.0, 1118847183800.0, 1118847183900.0, 1118847184000.0, 1118847184100.0, 1118847184200.0, 1118847184300.0, 1118847184400.0, 1118847184500.0, 1118847184600.0, 1118847184700.0, 1118847184800.0, 1118847184900.0, 1118847185000.0, 1118847185100.0, 1118847185200.0, 1118847185300.0, 1118847185400.0, 1118847185500.0, 1118847185600.0, 1118847185700.0, 1118847185800.0, 1118847185900.0, 1118847186000.0, 1118847186100.0, 1118847186200.0, 1118847186300.0, 1118847186400.0, 1118847186500.0, 1118847186600.0, 1118847186700.0, 1118847186800.0, 1118847186900.0, 1118847187000.0, 1118847187100.0, 1118847187200.0, 1118847187300.0, 1118847187400.0, 1118847187500.0, 1118847187600.0, 1118847187700.0, 1118847187800.0, 1118847187900.0, 1118847188000.0, 1118847188100.0, 1118847188200.0, 1118847188300.0, 1118847188400.0, 1118847188500.0, 1118847188600.0, 1118847188700.0, 1118847188800.0, 1118847188900.0, 1118847189000.0, 1118847189100.0, 1118847189200.0, 1118847189300.0, 1118847189400.0, 1118847189500.0, 1118847189600.0, 1118847189700.0, 1118847189800.0, 1118847189900.0, 1118847190000.0, 1118847190100.0, 1118847190200.0, 1118847190300.0, 1118847190400.0, 1118847190500.0, 1118847190600.0, 1118847190700.0, 1118847190800.0, 1118847190900.0, 1118847191000.0, 1118847191100.0, 1118847191200.0, 1118847191300.0, 1118847191400.0, 1118847191500.0, 1118847191600.0, 1118847191700.0, 1118847191800.0, 1118847191900.0, 1118847192000.0, 1118847192100.0, 1118847192200.0, 1118847192300.0, 1118847192400.0, 1118847192500.0, 1118847192600.0, 1118847192700.0, 1118847192800.0, 1118847192900.0, 1118847193000.0, 1118847193100.0, 1118847193200.0, 1118847193300.0, 1118847193400.0, 1118847193500.0, 1118847193600.0, 1118847193700.0, 1118847193800.0, 1118847193900.0, 1118847194000.0, 1118847194100.0, 1118847194200.0, 1118847194300.0, 1118847194400.0, 1118847194500.0, 1118847194600.0, 1118847194700.0, 1118847194800.0, 1118847194900.0, 1118847195000.0, 1118847195100.0, 1118847195200.0, 1118847195300.0, 1118847195400.0, 1118847195500.0, 1118847195600.0, 1118847195700.0, 1118847195800.0, 1118847195900.0, 1118847196000.0, 1118847196100.0, 1118847196200.0, 1118847196300.0, 1118847196400.0, 1118847196500.0, 1118847196600.0, 1118847196700.0, 1118847196800.0, 1118847196900.0, 1118847197000.0, 1118847197100.0, 1118847197200.0, 1118847197300.0, 1118847197400.0, 1118847197500.0, 1118847197600.0, 1118847197700.0, 1118847197800.0, 1118847197900.0, 1118847198000.0, 1118847198100.0, 1118847198200.0, 1118847198300.0, 1118847198400.0, 1118847198500.0, 1118847198600.0, 1118847198700.0, 1118847198800.0, 1118847198900.0, 1118847199000.0, 1118847199100.0, 1118847199200.0, 1118847199300.0, 1118847199400.0, 1118847199500.0, 1118847199600.0, 1118847199700.0, 1118847199800.0, 1118847199900.0, 1118847200000.0, 1118847200100.0, 1118847200200.0, 1118847200300.0, 1118847200400.0, 1118847200500.0, 1118847200600.0, 1118847200700.0, 1118847200800.0, 1118847200900.0, 1118847201000.0, 1118847201100.0, 1118847201200.0, 1118847201300.0, 1118847201400.0, 1118847201500.0, 1118847201600.0, 1118847201700.0, 1118847201800.0, 1118847201900.0, 1118847202000.0, 1118847202100.0, 1118847202200.0, 1118847202300.0, 1118847202400.0, 1118847202500.0, 1118847202600.0, 1118847202700.0, 1118847202800.0, 1118847202900.0, 1118847203000.0, 1118847203100.0, 1118847203200.0, 1118847203300.0, 1118847203400.0, 1118847203500.0, 1118847203600.0, 1118847203700.0, 1118847203800.0, 1118847203900.0, 1118847204000.0, 1118847204100.0, 1118847204200.0, 1118847204300.0, 1118847204400.0, 1118847204500.0, 1118847204600.0, 1118847204700.0, 1118847204800.0, 1118847204900.0, 1118847205000.0, 1118847205100.0, 1118847205200.0, 1118847205300.0, 1118847205400.0, 1118847205500.0, 1118847205600.0, 1118847205700.0, 1118847205800.0, 1118847205900.0, 1118847206000.0, 1118847206100.0, 1118847206200.0, 1118847206300.0, 1118847206400.0, 1118847206500.0, 1118847206600.0, 1118847206700.0, 1118847206800.0, 1118847206900.0, 1118847207000.0, 1118847207100.0, 1118847207200.0, 1118847207300.0, 1118847207400.0, 1118847207500.0, 1118847207600.0, 1118847207700.0, 1118847207800.0, 1118847207900.0, 1118847208000.0, 1118847208100.0, 1118847208200.0, 1118847208300.0, 1118847208400.0, 1118847208500.0, 1118847208600.0, 1118847208700.0, 1118847208800.0, 1118847208900.0, 1118847209000.0, 1118847209100.0, 1118847209200.0, 1118847209300.0, 1118847209400.0, 1118847209500.0, 1118847209600.0, 1118847209700.0, 1118847209800.0, 1118847209900.0, 1118847210000.0, 1118847210100.0, 1118847210200.0, 1118847210300.0, 1118847210400.0, 1118847210500.0, 1118847210600.0, 1118847210700.0, 1118847210800.0, 1118847210900.0, 1118847211000.0, 1118847211100.0, 1118847211200.0, 1118847211300.0, 1118847211400.0, 1118847211500.0, 1118847211600.0, 1118847211700.0, 1118847211800.0, 1118847211900.0, 1118847212000.0, 1118847212100.0, 1118847212200.0, 1118847212300.0, 1118847212400.0, 1118847212500.0, 1118847212600.0, 1118847212700.0, 1118847212800.0, 1118847212900.0, 1118847213000.0, 1118847213100.0, 1118847213200.0, 1118847213300.0, 1118847213400.0, 1118847213500.0, 1118847213600.0, 1118847213700.0, 1118847213800.0, 1118847213900.0, 1118847214000.0, 1118847214100.0, 1118847214200.0, 1118847214300.0, 1118847214400.0, 1118847214500.0, 1118847214600.0, 1118847214700.0, 1118847214800.0, 1118847214900.0, 1118847215000.0, 1118847215100.0, 1118847215200.0, 1118847215300.0, 1118847215400.0, 1118847215500.0, 1118847215600.0, 1118847215700.0, 1118847215800.0, 1118847215900.0, 1118847216000.0, 1118847216100.0, 1118847216200.0, 1118847216300.0, 1118847216400.0, 1118847216500.0, 1118847216600.0, 1118847216700.0, 1118847216800.0, 1118847216900.0, 1118847217000.0, 1118847217100.0, 1118847217200.0, 1118847217300.0, 1118847217400.0, 1118847217500.0, 1118847217600.0, 1118847217700.0, 1118847217800.0, 1118847217900.0, 1118847218000.0, 1118847218100.0, 1118847218200.0, 1118847218300.0, 1118847218400.0, 1118847218500.0, 1118847218600.0, 1118847218700.0, 1118847218800.0, 1118847218900.0, 1118847219000.0, 1118847219100.0, 1118847219200.0, 1118847219300.0, 1118847219400.0, 1118847219500.0, 1118847219600.0, 1118847219700.0, 1118847219800.0, 1118847219900.0, 1118847220000.0, 1118847220100.0, 1118847220200.0, 1118847220300.0, 1118847220400.0, 1118847220500.0, 1118847220600.0, 1118847220700.0, 1118847220800.0, 1118847220900.0, 1118847221000.0, 1118847221100.0, 1118847221200.0, 1118847221300.0, 1118847221400.0, 1118847221500.0, 1118847221600.0, 1118847221700.0, 1118847221800.0, 1118847221900.0, 1118847222000.0, 1118847222100.0, 1118847222200.0, 1118847222300.0, 1118847222400.0, 1118847222500.0, 1118847222600.0, 1118847222700.0, 1118847222800.0, 1118847222900.0, 1118847223000.0, 1118847223100.0, 1118847223200.0, 1118847223300.0, 1118847223400.0, 1118847223500.0, 1118847223600.0, 1118847223700.0, 1118847223800.0, 1118847223900.0, 1118847224000.0, 1118847224100.0, 1118847224200.0, 1118847224300.0, 1118847224400.0, 1118847224500.0, 1118847224600.0, 1118847224700.0, 1118847224800.0, 1118847224900.0, 1118847225000.0, 1118847225100.0, 1118847225200.0, 1118847225300.0, 1118847225400.0, 1118847225500.0, 1118847225600.0, 1118847225700.0, 1118847225800.0, 1118847225900.0, 1118847226000.0, 1118847226100.0, 1118847226200.0, 1118847226300.0, 1118847226400.0, 1118847226500.0, 1118847226600.0, 1118847226700.0, 1118847226800.0, 1118847226900.0, 1118847227000.0, 1118847227100.0, 1118847227200.0, 1118847227300.0, 1118847227400.0, 1118847227500.0, 1118847227600.0, 1118847227700.0, 1118847227800.0, 1118847227900.0, 1118847228000.0, 1118847228100.0, 1118847228200.0, 1118847228300.0, 1118847228400.0, 1118847228500.0, 1118847228600.0, 1118847228700.0, 1118847228800.0, 1118847228900.0, 1118847229000.0, 1118847229100.0, 1118847229200.0, 1118847229300.0, 1118847229400.0, 1118847229500.0, 1118847229600.0, 1118847229700.0, 1118847229800.0, 1118847229900.0, 1118847230000.0, 1118847230100.0, 1118847230200.0, 1118847230300.0, 1118847230400.0, 1118847230500.0, 1118847230600.0, 1118847230700.0, 1118847230800.0, 1118847230900.0, 1118847231000.0, 1118847231100.0, 1118847231200.0, 1118847231300.0, 1118847231400.0, 1118847231500.0, 1118847231600.0, 1118847231700.0, 1118847231800.0, 1118847231900.0, 1118847232000.0, 1118847232100.0, 1118847232200.0, 1118847232300.0, 1118847232400.0, 1118847232500.0, 1118847232600.0, 1118847232700.0, 1118847232800.0, 1118847232900.0, 1118847233000.0, 1118847233100.0, 1118847233200.0, 1118847233300.0, 1118847233400.0, 1118847233500.0, 1118847233600.0, 1118847233700.0, 1118847233800.0, 1118847233900.0, 1118847234000.0, 1118847234100.0, 1118847234200.0, 1118847234300.0, 1118847234400.0, 1118847234500.0, 1118847234600.0, 1118847234700.0, 1118847234800.0, 1118847234900.0, 1118847235000.0, 1118847235100.0, 1118847235200.0, 1118847235300.0, 1118847235400.0, 1118847235500.0, 1118847235600.0, 1118847235700.0, 1118847235800.0, 1118847235900.0, 1118847236000.0, 1118847236100.0, 1118847236200.0, 1118847236300.0, 1118847236400.0, 1118847236500.0, 1118847236600.0, 1118847236700.0, 1118847236800.0, 1118847236900.0, 1118847237000.0, 1118847237100.0, 1118847237200.0, 1118847237300.0, 1118847237400.0, 1118847237500.0, 1118847237600.0, 1118847237700.0, 1118847237800.0, 1118847237900.0, 1118847238000.0, 1118847238100.0, 1118847238200.0, 1118847238300.0, 1118847238400.0, 1118847238500.0, 1118847238600.0, 1118847238700.0, 1118847238800.0, 1118847238900.0, 1118847239000.0, 1118847239100.0, 1118847239200.0, 1118847239300.0, 1118847239400.0, 1118847239500.0, 1118847239600.0, 1118847239700.0, 1118847239800.0, 1118847239900.0, 1118847240000.0, 1118847240100.0, 1118847240200.0, 1118847240300.0, 1118847240400.0, 1118847240500.0, 1118847240600.0, 1118847240700.0, 1118847240800.0, 1118847240900.0, 1118847241000.0, 1118847241100.0, 1118847241200.0, 1118847241300.0, 1118847241400.0, 1118847241500.0, 1118847241600.0, 1118847241700.0, 1118847241800.0, 1118847241900.0, 1118847242000.0, 1118847242100.0, 1118847242200.0, 1118847242300.0, 1118847242400.0, 1118847242500.0, 1118847242600.0, 1118847242700.0, 1118847242800.0, 1118847242900.0, 1118847243000.0, 1118847243100.0, 1118847243200.0, 1118847243300.0, 1118847243400.0, 1118847243500.0, 1118847243600.0, 1118847243700.0, 1118847243800.0, 1118847243900.0, 1118847244000.0, 1118847244100.0, 1118847244200.0, 1118847244300.0, 1118847244400.0, 1118847244500.0, 1118847244600.0, 1118847244700.0, 1118847244800.0, 1118847244900.0, 1118847245000.0, 1118847245100.0, 1118847245200.0, 1118847245300.0, 1118847245400.0, 1118847245500.0, 1118847245600.0, 1118847245700.0, 1118847245800.0, 1118847245900.0, 1118847246000.0, 1118847246100.0, 1118847246200.0, 1118847246300.0, 1118847246400.0, 1118847246500.0, 1118847246600.0, 1118847246700.0, 1118847246800.0, 1118847246900.0, 1118847247000.0, 1118847247100.0, 1118847247200.0, 1118847247300.0, 1118847247400.0, 1118847247500.0, 1118847247600.0, 1118847247700.0, 1118847247800.0, 1118847247900.0, 1118847248000.0, 1118847248100.0, 1118847248200.0, 1118847248300.0, 1118847248400.0, 1118847248500.0, 1118847248600.0, 1118847248700.0, 1118847248800.0, 1118847248900.0, 1118847249000.0, 1118847249100.0, 1118847249200.0, 1118847249300.0, 1118847249400.0, 1118847249500.0, 1118847249600.0, 1118847249700.0, 1118847249800.0, 1118847249900.0, 1118847250000.0, 1118847250100.0, 1118847250200.0, 1118847250300.0, 1118847250400.0, 1118847250500.0, 1118847250600.0, 1118847250700.0, 1118847250800.0, 1118847250900.0, 1118847251000.0, 1118847251100.0, 1118847251200.0, 1118847251300.0, 1118847251400.0, 1118847251500.0, 1118847251600.0, 1118847251700.0, 1118847251800.0, 1118847251900.0, 1118847252000.0, 1118847252100.0, 1118847252200.0, 1118847252300.0, 1118847252400.0, 1118847252500.0, 1118847252600.0, 1118847252700.0, 1118847252800.0, 1118847252900.0, 1118847253000.0, 1118847253100.0, 1118847253200.0, 1118847253300.0, 1118847253400.0, 1118847253500.0, 1118847253600.0, 1118847253700.0, 1118847253800.0, 1118847253900.0, 1118847254000.0, 1118847254100.0, 1118847254200.0, 1118847254300.0, 1118847254400.0, 1118847254500.0, 1118847254600.0, 1118847254700.0, 1118847254800.0, 1118847254900.0, 1118847255000.0, 1118847255100.0, 1118847255200.0, 1118847255300.0, 1118847255400.0, 1118847255500.0, 1118847255600.0, 1118847255700.0, 1118847255800.0, 1118847255900.0, 1118847256000.0, 1118847256100.0, 1118847256200.0, 1118847256300.0, 1118847256400.0, 1118847256500.0, 1118847256600.0, 1118847256700.0, 1118847256800.0, 1118847256900.0, 1118847257000.0, 1118847257100.0, 1118847257200.0, 1118847257300.0, 1118847257400.0, 1118847257500.0, 1118847257600.0, 1118847257700.0, 1118847257800.0, 1118847257900.0, 1118847258000.0, 1118847258100.0, 1118847258200.0, 1118847258300.0, 1118847258400.0, 1118847258500.0, 1118847258600.0, 1118847258700.0, 1118847258800.0, 1118847258900.0, 1118847259000.0, 1118847259100.0, 1118847259200.0, 1118847259300.0, 1118847259400.0, 1118847259500.0, 1118847259600.0, 1118847259700.0, 1118847259800.0, 1118847259900.0, 1118847260000.0, 1118847260100.0, 1118847260200.0, 1118847260300.0, 1118847260400.0, 1118847260500.0, 1118847260600.0, 1118847260700.0, 1118847260800.0, 1118847260900.0, 1118847261000.0, 1118847261100.0, 1118847261200.0, 1118847261300.0, 1118847261400.0, 1118847261500.0, 1118847261600.0, 1118847261700.0, 1118847261800.0, 1118847261900.0, 1118847262000.0, 1118847262100.0, 1118847262200.0, 1118847262300.0, 1118847262400.0, 1118847262500.0, 1118847262600.0, 1118847262700.0, 1118847262800.0, 1118847262900.0, 1118847263000.0, 1118847263100.0, 1118847263200.0, 1118847263300.0, 1118847263400.0, 1118847263500.0, 1118847263600.0, 1118847263700.0, 1118847263800.0, 1118847263900.0, 1118847264000.0, 1118847264100.0, 1118847264200.0, 1118847264300.0, 1118847264400.0, 1118847264500.0, 1118847264600.0, 1118847264700.0, 1118847264800.0, 1118847264900.0, 1118847265000.0, 1118847265100.0, 1118847265200.0, 1118847265300.0, 1118847265400.0, 1118847265500.0, 1118847265600.0, 1118847265700.0, 1118847265800.0, 1118847265900.0, 1118847266000.0, 1118847266100.0, 1118847266200.0, 1118847266300.0, 1118847266400.0, 1118847266500.0, 1118847266600.0, 1118847266700.0, 1118847266800.0, 1118847266900.0, 1118847267000.0, 1118847267100.0, 1118847267200.0, 1118847267300.0, 1118847267400.0, 1118847267500.0, 1118847267600.0, 1118847267700.0, 1118847267800.0, 1118847267900.0, 1118847268000.0, 1118847268100.0, 1118847268200.0, 1118847268300.0, 1118847268400.0, 1118847268500.0, 1118847268600.0, 1118847268700.0, 1118847268800.0, 1118847268900.0, 1118847269000.0, 1118847269100.0, 1118847269200.0, 1118847269300.0, 1118847269400.0, 1118847269500.0, 1118847269600.0, 1118847269700.0, 1118847269800.0, 1118847269900.0, 1118847270000.0, 1118847270100.0, 1118847270200.0, 1118847270300.0, 1118847270400.0, 1118847270500.0, 1118847270600.0, 1118847270700.0, 1118847270800.0, 1118847270900.0, 1118847271000.0, 1118847271100.0, 1118847271200.0, 1118847271300.0, 1118847271400.0, 1118847271500.0, 1118847271600.0, 1118847271700.0, 1118847271800.0, 1118847271900.0, 1118847272000.0, 1118847272100.0, 1118847272200.0, 1118847272300.0, 1118847272400.0, 1118847272500.0, 1118847272600.0, 1118847272700.0, 1118847272800.0, 1118847272900.0, 1118847273000.0, 1118847273100.0, 1118847273200.0, 1118847273300.0, 1118847273400.0, 1118847273500.0, 1118847273600.0, 1118847273700.0, 1118847273800.0, 1118847273900.0, 1118847274000.0, 1118847274100.0, 1118847274200.0, 1118847274300.0, 1118847274400.0, 1118847274500.0, 1118847274600.0, 1118847274700.0, 1118847274800.0, 1118847274900.0, 1118847275000.0, 1118847275100.0, 1118847275200.0, 1118847275300.0, 1118847275400.0, 1118847275500.0, 1118847275600.0, 1118847275700.0, 1118847275800.0, 1118847275900.0, 1118847276000.0, 1118847276100.0, 1118847276200.0, 1118847276300.0, 1118847276400.0, 1118847276500.0, 1118847276600.0, 1118847276700.0, 1118847276800.0, 1118847276900.0, 1118847277000.0, 1118847277100.0, 1118847277200.0, 1118847277300.0, 1118847277400.0, 1118847277500.0, 1118847277600.0, 1118847277700.0, 1118847277800.0, 1118847277900.0, 1118847278000.0, 1118847278100.0, 1118847278200.0, 1118847278300.0, 1118847278400.0, 1118847278500.0, 1118847278600.0, 1118847278700.0, 1118847278800.0, 1118847278900.0, 1118847279000.0, 1118847279100.0, 1118847279200.0, 1118847279300.0, 1118847279400.0, 1118847279500.0, 1118847279600.0, 1118847279700.0, 1118847279800.0, 1118847279900.0, 1118847280000.0, 1118847280100.0, 1118847280200.0, 1118847280300.0, 1118847280400.0, 1118847280500.0, 1118847280600.0, 1118847280700.0, 1118847280800.0, 1118847280900.0, 1118847281000.0, 1118847281100.0, 1118847281200.0, 1118847281300.0, 1118847281400.0, 1118847281500.0, 1118847281600.0, 1118847281700.0, 1118847281800.0, 1118847281900.0, 1118847282000.0, 1118847282100.0, 1118847282200.0, 1118847282300.0, 1118847282400.0, 1118847282500.0, 1118847282600.0, 1118847282700.0, 1118847282800.0, 1118847282900.0, 1118847283000.0, 1118847283100.0, 1118847283200.0, 1118847283300.0, 1118847283400.0, 1118847283500.0, 1118847283600.0, 1118847283700.0, 1118847283800.0, 1118847283900.0, 1118847284000.0, 1118847284100.0, 1118847284200.0, 1118847284300.0, 1118847284400.0, 1118847284500.0, 1118847284600.0, 1118847284700.0, 1118847284800.0, 1118847284900.0, 1118847285000.0, 1118847285100.0, 1118847285200.0, 1118847285300.0, 1118847285400.0, 1118847285500.0, 1118847285600.0, 1118847285700.0, 1118847285800.0, 1118847285900.0, 1118847286000.0, 1118847286100.0, 1118847286200.0, 1118847286300.0, 1118847286400.0, 1118847286500.0, 1118847286600.0, 1118847286700.0, 1118847286800.0, 1118847286900.0, 1118847287000.0, 1118847287100.0, 1118847287200.0, 1118847287300.0, 1118847287400.0, 1118847287500.0, 1118847287600.0, 1118847287700.0, 1118847287800.0, 1118847287900.0, 1118847288000.0, 1118847288100.0, 1118847288200.0, 1118847288300.0, 1118847288400.0, 1118847288500.0, 1118847288600.0, 1118847288700.0, 1118847288800.0, 1118847288900.0, 1118847289000.0, 1118847289100.0, 1118847289200.0, 1118847289300.0, 1118847289400.0, 1118847289500.0, 1118847289600.0, 1118847289700.0, 1118847289800.0, 1118847289900.0, 1118847290000.0, 1118847290100.0, 1118847290200.0, 1118847290300.0, 1118847290400.0, 1118847290500.0, 1118847290600.0, 1118847290700.0, 1118847290800.0, 1118847290900.0, 1118847291000.0, 1118847291100.0, 1118847291200.0, 1118847291300.0, 1118847291400.0, 1118847291500.0, 1118847291600.0, 1118847291700.0, 1118847291800.0, 1118847291900.0, 1118847292000.0, 1118847292100.0, 1118847292200.0, 1118847292300.0, 1118847292400.0, 1118847292500.0, 1118847292600.0, 1118847292700.0, 1118847292800.0, 1118847292900.0, 1118847293000.0, 1118847293100.0, 1118847293200.0, 1118847293300.0, 1118847293400.0, 1118847293500.0, 1118847293600.0, 1118847293700.0, 1118847293800.0, 1118847293900.0, 1118847294000.0, 1118847294100.0, 1118847294200.0, 1118847294300.0, 1118847294400.0, 1118847294500.0, 1118847294600.0, 1118847294700.0, 1118847294800.0, 1118847294900.0, 1118847295000.0, 1118847295100.0, 1118847295200.0, 1118847295300.0, 1118847295400.0, 1118847295500.0, 1118847295600.0, 1118847295700.0, 1118847295800.0, 1118847295900.0, 1118847296000.0, 1118847296100.0, 1118847296200.0, 1118847296300.0, 1118847296400.0, 1118847296500.0, 1118847296600.0, 1118847296700.0, 1118847296800.0, 1118847296900.0, 1118847297000.0, 1118847297100.0, 1118847297200.0, 1118847297300.0, 1118847297400.0, 1118847297500.0, 1118847297600.0, 1118847297700.0, 1118847297800.0, 1118847297900.0, 1118847298000.0, 1118847298100.0, 1118847298200.0, 1118847298300.0, 1118847298400.0, 1118847298500.0, 1118847298600.0, 1118847298700.0, 1118847298800.0, 1118847298900.0, 1118847299000.0, 1118847299100.0, 1118847299200.0, 1118847299300.0, 1118847299400.0, 1118847299500.0, 1118847299600.0, 1118847299700.0, 1118847299800.0, 1118847299900.0, 1118847300000.0, 1118847300100.0, 1118847300200.0, 1118847300300.0, 1118847300400.0, 1118847300500.0, 1118847300600.0, 1118847300700.0, 1118847300800.0, 1118847300900.0, 1118847301000.0, 1118847301100.0, 1118847301200.0, 1118847301300.0, 1118847301400.0, 1118847301500.0, 1118847301600.0, 1118847301700.0, 1118847301800.0, 1118847301900.0, 1118847302000.0, 1118847302100.0, 1118847302200.0, 1118847302300.0, 1118847302400.0, 1118847302500.0, 1118847302600.0, 1118847302700.0, 1118847302800.0, 1118847302900.0, 1118847303000.0, 1118847303100.0, 1118847303200.0, 1118847303300.0, 1118847303400.0, 1118847303500.0, 1118847303600.0, 1118847303700.0, 1118847303800.0, 1118847303900.0, 1118847304000.0, 1118847304100.0, 1118847304200.0, 1118847304300.0, 1118847304400.0, 1118847304500.0, 1118847304600.0, 1118847304700.0, 1118847304800.0, 1118847304900.0, 1118847305000.0, 1118847305100.0, 1118847305200.0, 1118847305300.0, 1118847305400.0, 1118847305500.0, 1118847305600.0, 1118847305700.0, 1118847305800.0, 1118847305900.0, 1118847306000.0, 1118847306100.0, 1118847306200.0, 1118847306300.0, 1118847306400.0, 1118847306500.0, 1118847306600.0, 1118847306700.0, 1118847306800.0, 1118847306900.0, 1118847307000.0, 1118847307100.0, 1118847307200.0, 1118847307300.0, 1118847307400.0, 1118847307500.0, 1118847307600.0, 1118847307700.0, 1118847307800.0, 1118847307900.0, 1118847308000.0, 1118847308100.0, 1118847308200.0, 1118847308300.0, 1118847308400.0, 1118847308500.0, 1118847308600.0, 1118847308700.0, 1118847308800.0, 1118847308900.0, 1118847309000.0, 1118847309100.0, 1118847309200.0, 1118847309300.0, 1118847309400.0, 1118847309500.0, 1118847309600.0, 1118847309700.0, 1118847309800.0, 1118847309900.0, 1118847310000.0, 1118847310100.0, 1118847310200.0, 1118847310300.0, 1118847310400.0, 1118847310500.0, 1118847310600.0, 1118847310700.0, 1118847310800.0, 1118847310900.0, 1118847311000.0, 1118847311100.0, 1118847311200.0, 1118847311300.0, 1118847311400.0, 1118847311500.0, 1118847311600.0, 1118847311700.0, 1118847311800.0, 1118847311900.0, 1118847312000.0, 1118847312100.0, 1118847312200.0, 1118847312300.0, 1118847312400.0, 1118847312500.0, 1118847312600.0, 1118847312700.0, 1118847312800.0, 1118847312900.0, 1118847313000.0, 1118847313100.0, 1118847313200.0, 1118847313300.0, 1118847313400.0, 1118847313500.0, 1118847313600.0, 1118847313700.0, 1118847313800.0, 1118847313900.0, 1118847314000.0, 1118847314100.0, 1118847314200.0, 1118847314300.0, 1118847314400.0, 1118847314500.0, 1118847314600.0, 1118847314700.0, 1118847314800.0, 1118847314900.0, 1118847315000.0, 1118847315100.0, 1118847315200.0, 1118847315300.0, 1118847315400.0, 1118847315500.0, 1118847315600.0, 1118847315700.0, 1118847315800.0, 1118847315900.0, 1118847316000.0, 1118847316100.0, 1118847316200.0, 1118847316300.0, 1118847316400.0, 1118847316500.0, 1118847316600.0, 1118847316700.0, 1118847316800.0, 1118847316900.0, 1118847317000.0, 1118847317100.0, 1118847317200.0, 1118847317300.0, 1118847317400.0, 1118847317500.0, 1118847317600.0, 1118847317700.0, 1118847317800.0, 1118847317900.0, 1118847318000.0, 1118847318100.0, 1118847318200.0, 1118847318300.0, 1118847318400.0, 1118847318500.0, 1118847318600.0, 1118847318700.0, 1118847318800.0, 1118847318900.0, 1118847319000.0, 1118847319100.0, 1118847319200.0, 1118847319300.0, 1118847319400.0, 1118847319500.0, 1118847319600.0, 1118847319700.0, 1118847319800.0, 1118847319900.0, 1118847320000.0, 1118847320100.0, 1118847320200.0, 1118847320300.0, 1118847320400.0, 1118847320500.0, 1118847320600.0, 1118847320700.0, 1118847320800.0, 1118847320900.0, 1118847321000.0, 1118847321100.0, 1118847321200.0, 1118847321300.0, 1118847321400.0, 1118847321500.0, 1118847321600.0, 1118847321700.0, 1118847321800.0, 1118847321900.0, 1118847322000.0, 1118847322100.0, 1118847322200.0, 1118847322300.0, 1118847322400.0, 1118847322500.0, 1118847322600.0, 1118847322700.0, 1118847322800.0, 1118847322900.0, 1118847323000.0, 1118847323100.0, 1118847323200.0, 1118847323300.0, 1118847323400.0, 1118847323500.0, 1118847323600.0, 1118847323700.0, 1118847323800.0, 1118847323900.0, 1118847324000.0, 1118847324100.0, 1118847324200.0, 1118847324300.0, 1118847324400.0, 1118847324500.0, 1118847324600.0, 1118847324700.0, 1118847324800.0, 1118847324900.0, 1118847325000.0, 1118847325100.0, 1118847325200.0, 1118847325300.0, 1118847325400.0, 1118847325500.0, 1118847325600.0, 1118847325700.0, 1118847325800.0, 1118847325900.0, 1118847326000.0, 1118847326100.0, 1118847326200.0, 1118847326300.0, 1118847326400.0, 1118847326500.0, 1118847326600.0, 1118847326700.0, 1118847326800.0, 1118847326900.0, 1118847327000.0, 1118847327100.0, 1118847327200.0, 1118847327300.0, 1118847327400.0, 1118847327500.0, 1118847327600.0, 1118847327700.0, 1118847327800.0, 1118847327900.0, 1118847328000.0, 1118847328100.0, 1118847328200.0, 1118847328300.0, 1118847328400.0, 1118847328500.0, 1118847328600.0, 1118847328700.0, 1118847328800.0, 1118847328900.0, 1118847329000.0, 1118847329100.0, 1118847329200.0, 1118847329300.0, 1118847329400.0, 1118847329500.0, 1118847329600.0, 1118847329700.0, 1118847329800.0, 1118847329900.0, 1118847330000.0, 1118847330100.0, 1118847330200.0, 1118847330300.0, 1118847330400.0, 1118847330500.0, 1118847330600.0, 1118847330700.0, 1118847330800.0, 1118847330900.0, 1118847331000.0, 1118847331100.0, 1118847331200.0, 1118847331300.0, 1118847331400.0, 1118847331500.0, 1118847331600.0, 1118847331700.0, 1118847331800.0, 1118847331900.0, 1118847332000.0, 1118847332100.0, 1118847332200.0, 1118847332300.0, 1118847332400.0, 1118847332500.0, 1118847332600.0, 1118847332700.0, 1118847332800.0, 1118847332900.0, 1118847333000.0, 1118847333100.0, 1118847333200.0, 1118847333300.0, 1118847333400.0, 1118847333500.0, 1118847333600.0, 1118847333700.0, 1118847333800.0, 1118847333900.0, 1118847334000.0, 1118847334100.0, 1118847334200.0, 1118847334300.0, 1118847334400.0, 1118847334500.0, 1118847334600.0, 1118847334700.0, 1118847334800.0, 1118847334900.0, 1118847335000.0, 1118847335100.0, 1118847335200.0, 1118847335300.0, 1118847335400.0, 1118847335500.0, 1118847335600.0, 1118847335700.0, 1118847335800.0, 1118847335900.0, 1118847336000.0, 1118847336100.0, 1118847336200.0, 1118847336300.0, 1118847336400.0, 1118847336500.0, 1118847336600.0, 1118847336700.0, 1118847336800.0, 1118847336900.0, 1118847337000.0, 1118847337100.0, 1118847337200.0, 1118847337300.0, 1118847337400.0, 1118847337500.0, 1118847337600.0, 1118847337700.0, 1118847337800.0, 1118847337900.0, 1118847338000.0, 1118847338100.0, 1118847338200.0, 1118847338300.0, 1118847338400.0, 1118847338500.0, 1118847338600.0, 1118847338700.0, 1118847338800.0, 1118847338900.0, 1118847339000.0, 1118847339100.0, 1118847339200.0, 1118847339300.0, 1118847339400.0, 1118847339500.0, 1118847339600.0, 1118847339700.0, 1118847339800.0, 1118847339900.0, 1118847340000.0, 1118847340100.0, 1118847340200.0, 1118847340300.0, 1118847340400.0, 1118847340500.0, 1118847340600.0, 1118847340700.0, 1118847340800.0, 1118847340900.0, 1118847341000.0, 1118847341100.0, 1118847341200.0, 1118847341300.0, 1118847341400.0, 1118847341500.0, 1118847341600.0, 1118847341700.0, 1118847341800.0, 1118847341900.0, 1118847342000.0, 1118847342100.0, 1118847342200.0, 1118847342300.0, 1118847342400.0, 1118847342500.0, 1118847342600.0, 1118847342700.0, 1118847342800.0, 1118847342900.0, 1118847343000.0, 1118847343100.0, 1118847343200.0, 1118847343300.0, 1118847343400.0, 1118847343500.0, 1118847343600.0, 1118847343700.0, 1118847343800.0, 1118847343900.0, 1118847344000.0, 1118847344100.0, 1118847344200.0, 1118847344300.0, 1118847344400.0, 1118847344500.0, 1118847344600.0, 1118847344700.0, 1118847344800.0, 1118847344900.0, 1118847345000.0, 1118847345100.0, 1118847345200.0, 1118847345300.0, 1118847345400.0, 1118847345500.0, 1118847345600.0, 1118847345700.0, 1118847345800.0, 1118847345900.0, 1118847346000.0, 1118847346100.0, 1118847346200.0, 1118847346300.0, 1118847346400.0, 1118847346500.0, 1118847346600.0, 1118847346700.0, 1118847346800.0, 1118847346900.0, 1118847347000.0, 1118847347100.0, 1118847347200.0, 1118847347300.0, 1118847347400.0, 1118847347500.0, 1118847347600.0, 1118847347700.0, 1118847347800.0, 1118847347900.0, 1118847348000.0, 1118847348100.0, 1118847348200.0, 1118847348300.0, 1118847348400.0, 1118847348500.0, 1118847348600.0, 1118847348700.0, 1118847348800.0, 1118847348900.0, 1118847349000.0, 1118847349100.0, 1118847349200.0, 1118847349300.0, 1118847349400.0, 1118847349500.0, 1118847349600.0, 1118847349700.0, 1118847349800.0, 1118847349900.0, 1118847350000.0, 1118847350100.0, 1118847350200.0, 1118847350300.0, 1118847350400.0, 1118847350500.0, 1118847350600.0, 1118847350700.0, 1118847350800.0, 1118847350900.0, 1118847351000.0, 1118847351100.0, 1118847351200.0, 1118847351300.0, 1118847351400.0, 1118847351500.0, 1118847351600.0, 1118847351700.0, 1118847351800.0, 1118847351900.0, 1118847352000.0, 1118847352100.0, 1118847352200.0, 1118847352300.0, 1118847352400.0, 1118847352500.0, 1118847352600.0, 1118847352700.0, 1118847352800.0, 1118847352900.0, 1118847353000.0, 1118847353100.0, 1118847353200.0, 1118847353300.0, 1118847353400.0, 1118847353500.0, 1118847353600.0, 1118847353700.0, 1118847353800.0, 1118847353900.0, 1118847354000.0, 1118847354100.0, 1118847354200.0, 1118847354300.0, 1118847354400.0, 1118847354500.0, 1118847354600.0, 1118847354700.0, 1118847354800.0, 1118847354900.0, 1118847355000.0, 1118847355100.0, 1118847355200.0, 1118847355300.0, 1118847355400.0, 1118847355500.0, 1118847355600.0, 1118847355700.0, 1118847355800.0, 1118847355900.0, 1118847356000.0, 1118847356100.0, 1118847356200.0, 1118847356300.0, 1118847356400.0, 1118847356500.0, 1118847356600.0, 1118847356700.0, 1118847356800.0, 1118847356900.0, 1118847357000.0, 1118847357100.0, 1118847357200.0, 1118847357300.0, 1118847357400.0, 1118847357500.0, 1118847357600.0, 1118847357700.0, 1118847357800.0, 1118847357900.0, 1118847358000.0, 1118847358100.0, 1118847358200.0, 1118847358300.0, 1118847358400.0, 1118847358500.0, 1118847358600.0, 1118847358700.0, 1118847358800.0, 1118847358900.0, 1118847359000.0, 1118847359100.0, 1118847359200.0, 1118847359300.0, 1118847359400.0, 1118847359500.0, 1118847359600.0, 1118847359700.0, 1118847359800.0, 1118847359900.0, 1118847360000.0, 1118847360100.0, 1118847360200.0, 1118847360300.0, 1118847360400.0, 1118847360500.0, 1118847360600.0, 1118847360700.0, 1118847360800.0, 1118847360900.0, 1118847361000.0, 1118847361100.0, 1118847361200.0, 1118847361300.0, 1118847361400.0, 1118847361500.0, 1118847361600.0, 1118847361700.0, 1118847361800.0, 1118847361900.0, 1118847362000.0, 1118847362100.0, 1118847362200.0, 1118847362300.0, 1118847362400.0, 1118847362500.0, 1118847362600.0, 1118847362700.0, 1118847362800.0, 1118847362900.0, 1118847363000.0, 1118847363100.0, 1118847363200.0, 1118847363300.0, 1118847363400.0, 1118847363500.0, 1118847363600.0, 1118847363700.0, 1118847363800.0, 1118847363900.0, 1118847364000.0, 1118847364100.0, 1118847364200.0, 1118847364300.0, 1118847364400.0, 1118847364500.0, 1118847364600.0, 1118847364700.0, 1118847364800.0, 1118847364900.0, 1118847365000.0, 1118847365100.0, 1118847365200.0, 1118847365300.0, 1118847365400.0, 1118847365500.0, 1118847365600.0, 1118847365700.0, 1118847365800.0, 1118847365900.0, 1118847366000.0, 1118847366100.0, 1118847366200.0, 1118847366300.0, 1118847366400.0, 1118847366500.0, 1118847366600.0, 1118847366700.0, 1118847366800.0, 1118847366900.0, 1118847367000.0, 1118847367100.0, 1118847367200.0, 1118847367300.0, 1118847367400.0, 1118847367500.0, 1118847367600.0, 1118847367700.0, 1118847367800.0, 1118847367900.0, 1118847368000.0, 1118847368100.0, 1118847368200.0, 1118847368300.0, 1118847368400.0, 1118847368500.0, 1118847368600.0, 1118847368700.0, 1118847368800.0, 1118847368900.0, 1118847369000.0, 1118847369100.0, 1118847369200.0, 1118847369300.0, 1118847369400.0, 1118847369500.0, 1118847369600.0, 1118847369700.0, 1118847369800.0, 1118847369900.0, 1118847370000.0, 1118847370100.0, 1118847370200.0, 1118847370300.0, 1118847370400.0, 1118847370500.0, 1118847370600.0, 1118847370700.0, 1118847370800.0, 1118847370900.0, 1118847371000.0, 1118847371100.0, 1118847371200.0, 1118847371300.0, 1118847371400.0, 1118847371500.0, 1118847371600.0, 1118847371700.0, 1118847371800.0, 1118847371900.0, 1118847372000.0, 1118847372100.0, 1118847372200.0, 1118847372300.0, 1118847372400.0, 1118847372500.0, 1118847372600.0, 1118847372700.0, 1118847372800.0, 1118847372900.0, 1118847373000.0, 1118847373100.0, 1118847373200.0, 1118847373300.0, 1118847373400.0, 1118847373500.0, 1118847373600.0, 1118847373700.0, 1118847373800.0, 1118847373900.0, 1118847374000.0, 1118847374100.0, 1118847374200.0, 1118847374300.0, 1118847374400.0, 1118847374500.0, 1118847374600.0, 1118847374700.0, 1118847374800.0, 1118847374900.0, 1118847375000.0, 1118847375100.0, 1118847375200.0, 1118847375300.0, 1118847375400.0, 1118847375500.0, 1118847375600.0, 1118847375700.0, 1118847375800.0, 1118847375900.0, 1118847376000.0, 1118847376100.0, 1118847376200.0, 1118847376300.0, 1118847376400.0, 1118847376500.0, 1118847376600.0, 1118847376700.0, 1118847376800.0, 1118847376900.0, 1118847377000.0, 1118847377100.0, 1118847377200.0, 1118847377300.0, 1118847377400.0, 1118847377500.0, 1118847377600.0, 1118847377700.0, 1118847377800.0, 1118847377900.0, 1118847378000.0, 1118847378100.0, 1118847378200.0, 1118847378300.0, 1118847378400.0, 1118847378500.0, 1118847378600.0, 1118847378700.0, 1118847378800.0, 1118847378900.0, 1118847379000.0, 1118847379100.0, 1118847379200.0, 1118847379300.0, 1118847379400.0, 1118847379500.0, 1118847379600.0, 1118847379700.0, 1118847379800.0, 1118847379900.0, 1118847380000.0, 1118847380100.0, 1118847380200.0, 1118847380300.0, 1118847380400.0, 1118847380500.0, 1118847380600.0, 1118847380700.0, 1118847380800.0, 1118847380900.0, 1118847381000.0, 1118847381100.0, 1118847381200.0, 1118847381300.0, 1118847381400.0, 1118847381500.0, 1118847381600.0, 1118847381700.0, 1118847381800.0, 1118847381900.0, 1118847382000.0, 1118847382100.0, 1118847382200.0, 1118847382300.0, 1118847382400.0, 1118847382500.0, 1118847382600.0, 1118847382700.0, 1118847382800.0, 1118847382900.0, 1118847383000.0, 1118847383100.0, 1118847383200.0, 1118847383300.0, 1118847383400.0, 1118847383500.0, 1118847383600.0, 1118847383700.0, 1118847383800.0, 1118847383900.0, 1118847384000.0, 1118847384100.0, 1118847384200.0, 1118847384300.0, 1118847384400.0, 1118847384500.0, 1118847384600.0, 1118847384700.0, 1118847384800.0, 1118847384900.0, 1118847385000.0, 1118847385100.0, 1118847385200.0, 1118847385300.0, 1118847385400.0, 1118847385500.0, 1118847385600.0, 1118847385700.0, 1118847385800.0, 1118847385900.0, 1118847386000.0, 1118847386100.0, 1118847386200.0, 1118847386300.0, 1118847386400.0, 1118847386500.0, 1118847386600.0, 1118847386700.0, 1118847386800.0, 1118847386900.0, 1118847387000.0, 1118847387100.0, 1118847387200.0, 1118847387300.0, 1118847387400.0, 1118847387500.0, 1118847387600.0, 1118847387700.0, 1118847387800.0, 1118847387900.0, 1118847388000.0, 1118847388100.0, 1118847388200.0, 1118847388300.0, 1118847388400.0, 1118847388500.0, 1118847388600.0, 1118847388700.0, 1118847388800.0, 1118847388900.0, 1118847389000.0, 1118847389100.0, 1118847389200.0, 1118847389300.0, 1118847389400.0, 1118847389500.0, 1118847389600.0, 1118847389700.0, 1118847389800.0, 1118847389900.0, 1118847390000.0, 1118847390100.0, 1118847390200.0, 1118847390300.0, 1118847390400.0, 1118847390500.0, 1118847390600.0, 1118847390700.0, 1118847390800.0, 1118847390900.0, 1118847391000.0, 1118847391100.0, 1118847391200.0, 1118847391300.0, 1118847391400.0, 1118847391500.0, 1118847391600.0, 1118847391700.0, 1118847391800.0, 1118847391900.0, 1118847392000.0, 1118847392100.0, 1118847392200.0, 1118847392300.0, 1118847392400.0, 1118847392500.0, 1118847392600.0, 1118847392700.0, 1118847392800.0, 1118847392900.0, 1118847393000.0, 1118847393100.0, 1118847393200.0, 1118847393300.0, 1118847393400.0, 1118847393500.0, 1118847393600.0, 1118847393700.0, 1118847393800.0, 1118847393900.0, 1118847394000.0, 1118847394100.0, 1118847394200.0, 1118847394300.0, 1118847394400.0, 1118847394500.0, 1118847394600.0, 1118847394700.0, 1118847394800.0, 1118847394900.0, 1118847395000.0, 1118847395100.0, 1118847395200.0, 1118847395300.0, 1118847395400.0, 1118847395500.0, 1118847395600.0, 1118847395700.0, 1118847395800.0, 1118847395900.0, 1118847396000.0, 1118847396100.0, 1118847396200.0, 1118847396300.0, 1118847396400.0, 1118847396500.0, 1118847396600.0, 1118847396700.0, 1118847396800.0, 1118847396900.0, 1118847397000.0, 1118847397100.0, 1118847397200.0, 1118847397300.0, 1118847397400.0, 1118847397500.0, 1118847397600.0, 1118847397700.0, 1118847397800.0, 1118847397900.0, 1118847398000.0, 1118847398100.0, 1118847398200.0, 1118847398300.0, 1118847398400.0, 1118847398500.0, 1118847398600.0, 1118847398700.0, 1118847398800.0, 1118847398900.0, 1118847399000.0, 1118847399100.0, 1118847399200.0, 1118847399300.0, 1118847399400.0, 1118847399500.0, 1118847399600.0, 1118847399700.0, 1118847399800.0, 1118847399900.0, 1118847400000.0, 1118847400100.0, 1118847400200.0, 1118847400300.0, 1118847400400.0, 1118847400500.0, 1118847400600.0, 1118847400700.0, 1118847400800.0, 1118847400900.0, 1118847401000.0, 1118847401100.0, 1118847401200.0, 1118847401300.0, 1118847401400.0, 1118847401500.0, 1118847401600.0, 1118847401700.0, 1118847401800.0, 1118847401900.0, 1118847402000.0, 1118847402100.0, 1118847402200.0, 1118847402300.0, 1118847402400.0, 1118847402500.0, 1118847402600.0, 1118847402700.0, 1118847402800.0, 1118847402900.0, 1118847403000.0, 1118847403100.0, 1118847403200.0, 1118847403300.0, 1118847403400.0, 1118847403500.0, 1118847403600.0, 1118847403700.0, 1118847403800.0, 1118847403900.0, 1118847404000.0, 1118847404100.0, 1118847404200.0, 1118847404300.0, 1118847404400.0, 1118847404500.0, 1118847404600.0, 1118847404700.0, 1118847404800.0, 1118847404900.0, 1118847405000.0, 1118847405100.0, 1118847405200.0, 1118847405300.0, 1118847405400.0, 1118847405500.0, 1118847405600.0, 1118847405700.0, 1118847405800.0, 1118847405900.0, 1118847406000.0, 1118847406100.0, 1118847406200.0, 1118847406300.0, 1118847406400.0, 1118847406500.0, 1118847406600.0, 1118847406700.0, 1118847406800.0, 1118847406900.0, 1118847407000.0, 1118847407100.0, 1118847407200.0, 1118847407300.0, 1118847407400.0, 1118847407500.0, 1118847407600.0, 1118847407700.0, 1118847407800.0, 1118847407900.0, 1118847408000.0, 1118847408100.0, 1118847408200.0, 1118847408300.0, 1118847408400.0, 1118847408500.0, 1118847408600.0, 1118847408700.0, 1118847408800.0, 1118847408900.0, 1118847409000.0, 1118847409100.0, 1118847409200.0, 1118847409300.0, 1118847409400.0, 1118847409500.0, 1118847409600.0, 1118847409700.0, 1118847409800.0, 1118847409900.0, 1118847410000.0, 1118847410100.0, 1118847410200.0, 1118847410300.0, 1118847410400.0, 1118847410500.0, 1118847410600.0, 1118847410700.0, 1118847410800.0, 1118847410900.0, 1118847411000.0, 1118847411100.0, 1118847411200.0, 1118847411300.0, 1118847411400.0, 1118847411500.0, 1118847411600.0, 1118847411700.0, 1118847411800.0, 1118847411900.0, 1118847412000.0, 1118847412100.0, 1118847412200.0, 1118847412300.0, 1118847412400.0, 1118847412500.0, 1118847412600.0, 1118847412700.0, 1118847412800.0, 1118847412900.0, 1118847413000.0, 1118847413100.0, 1118847413200.0, 1118847413300.0, 1118847413400.0, 1118847413500.0, 1118847413600.0, 1118847413700.0, 1118847413800.0, 1118847413900.0, 1118847414000.0, 1118847414100.0, 1118847414200.0, 1118847414300.0, 1118847414400.0, 1118847414500.0, 1118847414600.0, 1118847414700.0, 1118847414800.0, 1118847414900.0, 1118847415000.0, 1118847415100.0, 1118847415200.0, 1118847415300.0, 1118847415400.0, 1118847415500.0, 1118847415600.0, 1118847415700.0, 1118847415800.0, 1118847415900.0, 1118847416000.0, 1118847416100.0, 1118847416200.0, 1118847416300.0, 1118847416400.0, 1118847416500.0, 1118847416600.0, 1118847416700.0, 1118847416800.0, 1118847416900.0, 1118847417000.0, 1118847417100.0, 1118847417200.0, 1118847417300.0, 1118847417400.0, 1118847417500.0, 1118847417600.0, 1118847417700.0, 1118847417800.0, 1118847417900.0, 1118847418000.0, 1118847418100.0, 1118847418200.0, 1118847418300.0, 1118847418400.0, 1118847418500.0, 1118847418600.0, 1118847418700.0, 1118847418800.0, 1118847418900.0, 1118847419000.0, 1118847419100.0, 1118847419200.0, 1118847419300.0, 1118847419400.0, 1118847419500.0, 1118847419600.0, 1118847419700.0, 1118847419800.0, 1118847419900.0, 1118847420000.0, 1118847420100.0, 1118847420200.0, 1118847420300.0, 1118847420400.0, 1118847420500.0, 1118847420600.0, 1118847420700.0, 1118847420800.0, 1118847420900.0, 1118847421000.0, 1118847421100.0, 1118847421200.0, 1118847421300.0, 1118847421400.0, 1118847421500.0, 1118847421600.0, 1118847421700.0, 1118847421800.0, 1118847421900.0, 1118847422000.0, 1118847422100.0, 1118847422200.0, 1118847422300.0, 1118847422400.0, 1118847422500.0, 1118847422600.0, 1118847422700.0, 1118847422800.0, 1118847422900.0, 1118847423000.0, 1118847423100.0, 1118847423200.0, 1118847423300.0, 1118847423400.0, 1118847423500.0, 1118847423600.0, 1118847423700.0, 1118847423800.0, 1118847423900.0, 1118847424000.0, 1118847424100.0, 1118847424200.0, 1118847424300.0, 1118847424400.0, 1118847424500.0, 1118847424600.0, 1118847424700.0, 1118847424800.0, 1118847424900.0, 1118847425000.0, 1118847425100.0, 1118847425200.0, 1118847425300.0, 1118847425400.0, 1118847425500.0, 1118847425600.0, 1118847425700.0, 1118847425800.0, 1118847425900.0, 1118847426000.0, 1118847426100.0, 1118847426200.0, 1118847426300.0, 1118847426400.0, 1118847426500.0, 1118847426600.0, 1118847426700.0, 1118847426800.0, 1118847426900.0, 1118847427000.0, 1118847427100.0, 1118847427200.0, 1118847427300.0, 1118847427400.0, 1118847427500.0, 1118847427600.0, 1118847427700.0, 1118847427800.0, 1118847427900.0, 1118847428000.0, 1118847428100.0, 1118847428200.0, 1118847428300.0, 1118847428400.0, 1118847428500.0, 1118847428600.0, 1118847428700.0, 1118847428800.0, 1118847428900.0, 1118847429000.0, 1118847429100.0, 1118847429200.0, 1118847429300.0, 1118847429400.0, 1118847429500.0, 1118847429600.0, 1118847429700.0, 1118847429800.0, 1118847429900.0, 1118847430000.0, 1118847430100.0, 1118847430200.0, 1118847430300.0, 1118847430400.0, 1118847430500.0, 1118847430600.0, 1118847430700.0, 1118847430800.0, 1118847430900.0, 1118847431000.0, 1118847431100.0, 1118847431200.0, 1118847431300.0, 1118847431400.0, 1118847431500.0, 1118847431600.0, 1118847431700.0, 1118847431800.0, 1118847431900.0, 1118847432000.0, 1118847432100.0, 1118847432200.0, 1118847432300.0, 1118847432400.0, 1118847432500.0, 1118847432600.0, 1118847432700.0, 1118847432800.0, 1118847432900.0, 1118847433000.0, 1118847433100.0, 1118847433200.0, 1118847433300.0, 1118847433400.0, 1118847433500.0, 1118847433600.0, 1118847433700.0, 1118847433800.0, 1118847433900.0, 1118847434000.0, 1118847434100.0, 1118847434200.0, 1118847434300.0, 1118847434400.0, 1118847434500.0, 1118847434600.0, 1118847434700.0, 1118847434800.0, 1118847434900.0, 1118847435000.0, 1118847435100.0, 1118847435200.0, 1118847435300.0, 1118847435400.0, 1118847435500.0, 1118847435600.0, 1118847435700.0, 1118847435800.0, 1118847435900.0, 1118847436000.0, 1118847436100.0, 1118847436200.0, 1118847436300.0, 1118847436400.0, 1118847436500.0, 1118847436600.0, 1118847436700.0, 1118847436800.0, 1118847436900.0, 1118847437000.0, 1118847437100.0, 1118847437200.0, 1118847437300.0, 1118847437400.0, 1118847437500.0, 1118847437600.0, 1118847437700.0, 1118847437800.0, 1118847437900.0, 1118847438000.0, 1118847438100.0, 1118847438200.0, 1118847438300.0, 1118847438400.0, 1118847438500.0, 1118847438600.0, 1118847438700.0, 1118847438800.0, 1118847438900.0, 1118847439000.0, 1118847439100.0, 1118847439200.0, 1118847439300.0, 1118847439400.0, 1118847439500.0, 1118847439600.0, 1118847439700.0, 1118847439800.0, 1118847439900.0, 1118847440000.0, 1118847440100.0, 1118847440200.0, 1118847440300.0, 1118847440400.0, 1118847440500.0, 1118847440600.0, 1118847440700.0, 1118847440800.0, 1118847440900.0, 1118847441000.0, 1118847441100.0, 1118847441200.0, 1118847441300.0, 1118847441400.0, 1118847441500.0, 1118847441600.0, 1118847441700.0, 1118847441800.0, 1118847441900.0, 1118847442000.0, 1118847442100.0, 1118847442200.0, 1118847442300.0, 1118847442400.0, 1118847442500.0, 1118847442600.0, 1118847442700.0, 1118847442800.0, 1118847442900.0, 1118847443000.0, 1118847443100.0, 1118847443200.0, 1118847443300.0, 1118847443400.0, 1118847443500.0, 1118847443600.0, 1118847443700.0, 1118847443800.0, 1118847443900.0, 1118847444000.0, 1118847444100.0, 1118847444200.0, 1118847444300.0, 1118847444400.0, 1118847444500.0, 1118847444600.0, 1118847444700.0, 1118847444800.0, 1118847444900.0, 1118847445000.0, 1118847445100.0, 1118847445200.0, 1118847445300.0, 1118847445400.0, 1118847445500.0, 1118847445600.0, 1118847445700.0, 1118847445800.0, 1118847445900.0, 1118847446000.0, 1118847446100.0, 1118847446200.0, 1118847446300.0, 1118847446400.0, 1118847446500.0, 1118847446600.0, 1118847446700.0, 1118847446800.0, 1118847446900.0, 1118847447000.0, 1118847447100.0, 1118847447200.0, 1118847447300.0, 1118847447400.0, 1118847447500.0, 1118847447600.0, 1118847447700.0, 1118847447800.0, 1118847447900.0, 1118847448000.0, 1118847448100.0, 1118847448200.0, 1118847448300.0, 1118847448400.0, 1118847448500.0, 1118847448600.0, 1118847448700.0, 1118847448800.0, 1118847448900.0, 1118847449000.0, 1118847449100.0, 1118847449200.0, 1118847449300.0, 1118847449400.0, 1118847449500.0, 1118847449600.0, 1118847449700.0, 1118847449800.0, 1118847449900.0, 1118847450000.0, 1118847450100.0, 1118847450200.0, 1118847450300.0, 1118847450400.0, 1118847450500.0, 1118847450600.0, 1118847450700.0, 1118847450800.0, 1118847450900.0, 1118847451000.0, 1118847451100.0, 1118847451200.0, 1118847451300.0, 1118847451400.0, 1118847451500.0, 1118847451600.0, 1118847451700.0, 1118847451800.0, 1118847451900.0, 1118847452000.0, 1118847452100.0, 1118847452200.0, 1118847452300.0, 1118847452400.0, 1118847452500.0, 1118847452600.0, 1118847452700.0, 1118847452800.0, 1118847452900.0, 1118847453000.0, 1118847453100.0, 1118847453200.0, 1118847453300.0, 1118847453400.0, 1118847453500.0, 1118847453600.0, 1118847453700.0, 1118847453800.0, 1118847453900.0, 1118847454000.0, 1118847454100.0, 1118847454200.0, 1118847454300.0, 1118847454400.0, 1118847454500.0, 1118847454600.0, 1118847454700.0, 1118847454800.0, 1118847454900.0, 1118847455000.0, 1118847455100.0, 1118847455200.0, 1118847455300.0, 1118847455400.0, 1118847455500.0, 1118847455600.0, 1118847455700.0, 1118847455800.0, 1118847455900.0, 1118847456000.0, 1118847456100.0, 1118847456200.0, 1118847456300.0, 1118847456400.0, 1118847456500.0, 1118847456600.0, 1118847456700.0, 1118847456800.0, 1118847456900.0, 1118847457000.0, 1118847457100.0, 1118847457200.0, 1118847457300.0, 1118847457400.0, 1118847457500.0, 1118847457600.0, 1118847457700.0, 1118847457800.0, 1118847457900.0, 1118847458000.0, 1118847458100.0, 1118847458200.0, 1118847458300.0, 1118847458400.0, 1118847458500.0, 1118847458600.0, 1118847458700.0, 1118847458800.0, 1118847458900.0, 1118847459000.0, 1118847459100.0, 1118847459200.0, 1118847459300.0, 1118847459400.0, 1118847459500.0, 1118847459600.0, 1118847459700.0, 1118847459800.0, 1118847459900.0, 1118847460000.0, 1118847460100.0, 1118847460200.0, 1118847460300.0, 1118847460400.0, 1118847460500.0, 1118847460600.0, 1118847460700.0, 1118847460800.0, 1118847460900.0, 1118847461000.0, 1118847461100.0, 1118847461200.0, 1118847461300.0, 1118847461400.0, 1118847461500.0, 1118847461600.0, 1118847461700.0, 1118847461800.0, 1118847461900.0, 1118847462000.0, 1118847462100.0, 1118847462200.0, 1118847462300.0, 1118847462400.0, 1118847462500.0, 1118847462600.0, 1118847462700.0, 1118847462800.0, 1118847462900.0, 1118847463000.0, 1118847463100.0, 1118847463200.0, 1118847463300.0, 1118847463400.0, 1118847463500.0, 1118847463600.0, 1118847463700.0, 1118847463800.0, 1118847463900.0, 1118847464000.0, 1118847464100.0, 1118847464200.0, 1118847464300.0, 1118847464400.0, 1118847464500.0, 1118847464600.0, 1118847464700.0, 1118847464800.0, 1118847464900.0, 1118847465000.0, 1118847465100.0, 1118847465200.0, 1118847465300.0, 1118847465400.0, 1118847465500.0, 1118847465600.0, 1118847465700.0, 1118847465800.0, 1118847465900.0, 1118847466000.0, 1118847466100.0, 1118847466200.0, 1118847466300.0, 1118847466400.0, 1118847466500.0, 1118847466600.0, 1118847466700.0, 1118847466800.0, 1118847466900.0, 1118847467000.0, 1118847467100.0, 1118847467200.0, 1118847467300.0, 1118847467400.0, 1118847467500.0, 1118847467600.0, 1118847467700.0, 1118847467800.0, 1118847467900.0, 1118847468000.0, 1118847468100.0, 1118847468200.0, 1118847468300.0, 1118847468400.0, 1118847468500.0, 1118847468600.0, 1118847468700.0, 1118847468800.0, 1118847468900.0, 1118847469000.0, 1118847469100.0, 1118847469200.0, 1118847469300.0, 1118847469400.0, 1118847469500.0, 1118847469600.0, 1118847469700.0, 1118847469800.0, 1118847469900.0, 1118847470000.0, 1118847470100.0, 1118847470200.0, 1118847470300.0, 1118847470400.0, 1118847470500.0, 1118847470600.0, 1118847470700.0, 1118847470800.0, 1118847470900.0, 1118847471000.0, 1118847471100.0, 1118847471200.0, 1118847471300.0, 1118847471400.0, 1118847471500.0, 1118847471600.0, 1118847471700.0, 1118847471800.0, 1118847471900.0, 1118847472000.0, 1118847472100.0, 1118847472200.0, 1118847472300.0, 1118847472400.0, 1118847472500.0, 1118847472600.0, 1118847472700.0, 1118847472800.0, 1118847472900.0, 1118847473000.0, 1118847473100.0, 1118847473200.0, 1118847473300.0, 1118847473400.0, 1118847473500.0, 1118847473600.0, 1118847473700.0, 1118847473800.0, 1118847473900.0, 1118847474000.0, 1118847474100.0, 1118847474200.0, 1118847474300.0, 1118847474400.0, 1118847474500.0, 1118847474600.0, 1118847474700.0, 1118847474800.0, 1118847474900.0, 1118847475000.0, 1118847475100.0, 1118847475200.0, 1118847475300.0, 1118847475400.0, 1118847475500.0, 1118847475600.0, 1118847475700.0, 1118847475800.0, 1118847475900.0, 1118847476000.0, 1118847476100.0, 1118847476200.0, 1118847476300.0, 1118847476400.0, 1118847476500.0, 1118847476600.0, 1118847476700.0, 1118847476800.0, 1118847476900.0, 1118847477000.0, 1118847477100.0, 1118847477200.0, 1118847477300.0, 1118847477400.0, 1118847477500.0, 1118847477600.0, 1118847477700.0, 1118847477800.0, 1118847477900.0, 1118847478000.0, 1118847478100.0, 1118847478200.0, 1118847478300.0, 1118847478400.0, 1118847478500.0, 1118847478600.0, 1118847478700.0, 1118847478800.0, 1118847478900.0, 1118847479000.0, 1118847479100.0, 1118847479200.0, 1118847479300.0, 1118847479400.0, 1118847479500.0, 1118847479600.0, 1118847479700.0, 1118847479800.0, 1118847479900.0, 1118847480000.0, 1118847480100.0, 1118847480200.0, 1118847480300.0, 1118847480400.0, 1118847480500.0, 1118847480600.0, 1118847480700.0, 1118847480800.0, 1118847480900.0, 1118847481000.0, 1118847481100.0, 1118847481200.0, 1118847481300.0, 1118847481400.0, 1118847481500.0, 1118847481600.0, 1118847481700.0, 1118847481800.0, 1118847481900.0, 1118847482000.0, 1118847482100.0, 1118847482200.0, 1118847482300.0, 1118847482400.0, 1118847482500.0, 1118847482600.0, 1118847482700.0, 1118847482800.0, 1118847482900.0, 1118847483000.0, 1118847483100.0, 1118847483200.0, 1118847483300.0, 1118847483400.0, 1118847483500.0, 1118847483600.0, 1118847483700.0, 1118847483800.0, 1118847483900.0, 1118847484000.0, 1118847484100.0, 1118847484200.0, 1118847484300.0, 1118847484400.0, 1118847484500.0, 1118847484600.0, 1118847484700.0, 1118847484800.0, 1118847484900.0, 1118847485000.0, 1118847485100.0, 1118847485200.0, 1118847485300.0, 1118847485400.0, 1118847485500.0, 1118847485600.0, 1118847485700.0, 1118847485800.0, 1118847485900.0, 1118847486000.0, 1118847486100.0, 1118847486200.0, 1118847486300.0, 1118847486400.0, 1118847486500.0, 1118847486600.0, 1118847486700.0, 1118847486800.0, 1118847486900.0, 1118847487000.0, 1118847487100.0, 1118847487200.0, 1118847487300.0, 1118847487400.0, 1118847487500.0, 1118847487600.0, 1118847487700.0, 1118847487800.0, 1118847487900.0, 1118847488000.0, 1118847488100.0, 1118847488200.0, 1118847488300.0, 1118847488400.0, 1118847488500.0, 1118847488600.0, 1118847488700.0, 1118847488800.0, 1118847488900.0, 1118847489000.0, 1118847489100.0, 1118847489200.0, 1118847489300.0, 1118847489400.0, 1118847489500.0, 1118847489600.0, 1118847489700.0, 1118847489800.0, 1118847489900.0, 1118847490000.0, 1118847490100.0, 1118847490200.0, 1118847490300.0, 1118847490400.0, 1118847490500.0, 1118847490600.0, 1118847490700.0, 1118847490800.0, 1118847490900.0, 1118847491000.0, 1118847491100.0, 1118847491200.0, 1118847491300.0, 1118847491400.0, 1118847491500.0, 1118847491600.0, 1118847491700.0, 1118847491800.0, 1118847491900.0, 1118847492000.0, 1118847492100.0, 1118847492200.0, 1118847492300.0, 1118847492400.0, 1118847492500.0, 1118847492600.0, 1118847492700.0, 1118847492800.0, 1118847492900.0, 1118847493000.0, 1118847493100.0, 1118847493200.0, 1118847493300.0, 1118847493400.0, 1118847493500.0, 1118847493600.0, 1118847493700.0, 1118847493800.0, 1118847493900.0, 1118847494000.0, 1118847494100.0, 1118847494200.0, 1118847494300.0, 1118847494400.0, 1118847494500.0, 1118847494600.0, 1118847494700.0, 1118847494800.0, 1118847494900.0, 1118847495000.0, 1118847495100.0, 1118847495200.0, 1118847495300.0, 1118847495400.0, 1118847495500.0, 1118847495600.0, 1118847495700.0, 1118847495800.0, 1118847495900.0, 1118847496000.0, 1118847496100.0, 1118847496200.0, 1118847496300.0, 1118847496400.0, 1118847496500.0, 1118847496600.0, 1118847496700.0, 1118847496800.0, 1118847496900.0, 1118847497000.0, 1118847497100.0, 1118847497200.0, 1118847497300.0, 1118847497400.0, 1118847497500.0, 1118847497600.0, 1118847497700.0, 1118847497800.0, 1118847497900.0, 1118847498000.0, 1118847498100.0, 1118847498200.0, 1118847498300.0, 1118847498400.0, 1118847498500.0, 1118847498600.0, 1118847498700.0, 1118847498800.0, 1118847498900.0, 1118847499000.0, 1118847499100.0, 1118847499200.0, 1118847499300.0, 1118847499400.0, 1118847499500.0, 1118847499600.0, 1118847499700.0, 1118847499800.0, 1118847499900.0, 1118847500000.0, 1118847500100.0, 1118847500200.0, 1118847500300.0, 1118847500400.0, 1118847500500.0, 1118847500600.0, 1118847500700.0, 1118847500800.0, 1118847500900.0, 1118847501000.0, 1118847501100.0, 1118847501200.0, 1118847501300.0, 1118847501400.0, 1118847501500.0, 1118847501600.0, 1118847501700.0, 1118847501800.0, 1118847501900.0, 1118847502000.0, 1118847502100.0, 1118847502200.0, 1118847502300.0, 1118847502400.0, 1118847502500.0, 1118847502600.0, 1118847502700.0, 1118847502800.0, 1118847502900.0, 1118847503000.0, 1118847503100.0, 1118847503200.0, 1118847503300.0, 1118847503400.0, 1118847503500.0, 1118847503600.0, 1118847503700.0, 1118847503800.0, 1118847503900.0, 1118847504000.0, 1118847504100.0, 1118847504200.0, 1118847504300.0, 1118847504400.0, 1118847504500.0, 1118847504600.0, 1118847504700.0, 1118847504800.0, 1118847504900.0, 1118847505000.0, 1118847505100.0, 1118847505200.0, 1118847505300.0, 1118847505400.0, 1118847505500.0, 1118847505600.0, 1118847505700.0, 1118847505800.0, 1118847505900.0, 1118847506000.0, 1118847506100.0, 1118847506200.0, 1118847506300.0, 1118847506400.0, 1118847506500.0, 1118847506600.0, 1118847506700.0, 1118847506800.0, 1118847506900.0, 1118847507000.0, 1118847507100.0, 1118847507200.0, 1118847507300.0, 1118847507400.0, 1118847507500.0, 1118847507600.0, 1118847507700.0, 1118847507800.0, 1118847507900.0, 1118847508000.0, 1118847508100.0, 1118847508200.0, 1118847508300.0, 1118847508400.0, 1118847508500.0, 1118847508600.0, 1118847508700.0, 1118847508800.0, 1118847508900.0, 1118847509000.0, 1118847509100.0, 1118847509200.0, 1118847509300.0, 1118847509400.0, 1118847509500.0, 1118847509600.0, 1118847509700.0, 1118847509800.0, 1118847509900.0, 1118847510000.0, 1118847510100.0, 1118847510200.0, 1118847510300.0, 1118847510400.0, 1118847510500.0, 1118847510600.0, 1118847510700.0, 1118847510800.0, 1118847510900.0, 1118847511000.0, 1118847511100.0, 1118847511200.0, 1118847511300.0, 1118847511400.0, 1118847511500.0, 1118847511600.0, 1118847511700.0, 1118847511800.0, 1118847511900.0, 1118847512000.0, 1118847512100.0, 1118847512200.0, 1118847512300.0, 1118847512400.0, 1118847512500.0, 1118847512600.0, 1118847512700.0, 1118847512800.0, 1118847512900.0, 1118847513000.0, 1118847513100.0, 1118847513200.0, 1118847513300.0, 1118847513400.0, 1118847513500.0, 1118847513600.0, 1118847513700.0, 1118847513800.0, 1118847513900.0, 1118847514000.0, 1118847514100.0, 1118847514200.0, 1118847514300.0, 1118847514400.0, 1118847514500.0, 1118847514600.0, 1118847514700.0, 1118847514800.0, 1118847514900.0, 1118847515000.0, 1118847515100.0, 1118847515200.0, 1118847515300.0, 1118847515400.0, 1118847515500.0, 1118847515600.0, 1118847515700.0, 1118847515800.0, 1118847515900.0, 1118847516000.0, 1118847516100.0, 1118847516200.0, 1118847516300.0, 1118847516400.0, 1118847516500.0, 1118847516600.0, 1118847516700.0, 1118847516800.0, 1118847516900.0, 1118847517000.0, 1118847517100.0, 1118847517200.0, 1118847517300.0, 1118847517400.0, 1118847517500.0, 1118847517600.0, 1118847517700.0, 1118847517800.0, 1118847517900.0, 1118847518000.0, 1118847518100.0, 1118847518200.0, 1118847518300.0, 1118847518400.0, 1118847518500.0, 1118847518600.0, 1118847518700.0, 1118847518800.0, 1118847518900.0, 1118847519000.0, 1118847519100.0, 1118847519200.0, 1118847519300.0, 1118847519400.0, 1118847519500.0, 1118847519600.0, 1118847519700.0, 1118847519800.0, 1118847519900.0, 1118847520000.0, 1118847520100.0, 1118847520200.0, 1118847520300.0, 1118847520400.0, 1118847520500.0, 1118847520600.0, 1118847520700.0, 1118847520800.0, 1118847520900.0, 1118847521000.0, 1118847521100.0, 1118847521200.0, 1118847521300.0, 1118847521400.0, 1118847521500.0, 1118847521600.0, 1118847521700.0, 1118847521800.0, 1118847521900.0, 1118847522000.0, 1118847522100.0, 1118847522200.0, 1118847522300.0, 1118847522400.0, 1118847522500.0, 1118847522600.0, 1118847522700.0, 1118847522800.0, 1118847522900.0, 1118847523000.0, 1118847523100.0, 1118847523200.0, 1118847523300.0, 1118847523400.0, 1118847523500.0, 1118847523600.0, 1118847523700.0, 1118847523800.0, 1118847523900.0, 1118847524000.0, 1118847524100.0, 1118847524200.0, 1118847524300.0, 1118847524400.0, 1118847524500.0, 1118847524600.0, 1118847524700.0, 1118847524800.0, 1118847524900.0, 1118847525000.0, 1118847525100.0, 1118847525200.0, 1118847525300.0, 1118847525400.0, 1118847525500.0, 1118847525600.0, 1118847525700.0, 1118847525800.0, 1118847525900.0, 1118847526000.0, 1118847526100.0, 1118847526200.0, 1118847526300.0, 1118847526400.0, 1118847526500.0, 1118847526600.0, 1118847526700.0, 1118847526800.0, 1118847526900.0, 1118847527000.0, 1118847527100.0, 1118847527200.0, 1118847527300.0, 1118847527400.0, 1118847527500.0, 1118847527600.0, 1118847527700.0, 1118847527800.0, 1118847527900.0, 1118847528000.0, 1118847528100.0, 1118847528200.0, 1118847528300.0, 1118847528400.0, 1118847528500.0, 1118847528600.0, 1118847528700.0, 1118847528800.0, 1118847528900.0, 1118847529000.0, 1118847529100.0, 1118847529200.0, 1118847529300.0, 1118847529400.0, 1118847529500.0, 1118847529600.0, 1118847529700.0, 1118847529800.0, 1118847529900.0, 1118847530000.0, 1118847530100.0, 1118847530200.0, 1118847530300.0, 1118847530400.0, 1118847530500.0, 1118847530600.0, 1118847530700.0, 1118847530800.0, 1118847530900.0, 1118847531000.0, 1118847531100.0, 1118847531200.0, 1118847531300.0, 1118847531400.0, 1118847531500.0, 1118847531600.0, 1118847531700.0, 1118847531800.0, 1118847531900.0, 1118847532000.0, 1118847532100.0, 1118847532200.0, 1118847532300.0, 1118847532400.0, 1118847532500.0, 1118847532600.0, 1118847532700.0, 1118847532800.0, 1118847532900.0, 1118847533000.0, 1118847533100.0, 1118847533200.0, 1118847533300.0, 1118847533400.0, 1118847533500.0, 1118847533600.0, 1118847533700.0, 1118847533800.0, 1118847533900.0, 1118847534000.0, 1118847534100.0, 1118847534200.0, 1118847534300.0, 1118847534400.0, 1118847534500.0, 1118847534600.0, 1118847534700.0, 1118847534800.0, 1118847534900.0, 1118847535000.0, 1118847535100.0, 1118847535200.0, 1118847535300.0, 1118847535400.0, 1118847535500.0, 1118847535600.0, 1118847535700.0, 1118847535800.0, 1118847535900.0, 1118847536000.0, 1118847536100.0, 1118847536200.0, 1118847536300.0, 1118847536400.0, 1118847536500.0, 1118847536600.0, 1118847536700.0, 1118847536800.0, 1118847536900.0, 1118847537000.0, 1118847537100.0, 1118847537200.0, 1118847537300.0, 1118847537400.0, 1118847537500.0, 1118847537600.0, 1118847537700.0, 1118847537800.0, 1118847537900.0, 1118847538000.0, 1118847538100.0, 1118847538200.0, 1118847538300.0, 1118847538400.0, 1118847538500.0, 1118847538600.0, 1118847538700.0, 1118847538800.0, 1118847538900.0, 1118847539000.0, 1118847539100.0, 1118847539200.0, 1118847539300.0, 1118847539400.0, 1118847539500.0, 1118847539600.0, 1118847539700.0, 1118847539800.0, 1118847539900.0, 1118847540000.0, 1118847540100.0, 1118847540200.0, 1118847540300.0, 1118847540400.0, 1118847540500.0, 1118847540600.0, 1118847540700.0, 1118847540800.0, 1118847540900.0, 1118847541000.0, 1118847541100.0, 1118847541200.0, 1118847541300.0, 1118847541400.0, 1118847541500.0, 1118847541600.0, 1118847541700.0, 1118847541800.0, 1118847541900.0, 1118847542000.0, 1118847542100.0, 1118847542200.0, 1118847542300.0, 1118847542400.0, 1118847542500.0, 1118847542600.0, 1118847542700.0, 1118847542800.0, 1118847542900.0, 1118847543000.0, 1118847543100.0, 1118847543200.0, 1118847543300.0, 1118847543400.0, 1118847543500.0, 1118847543600.0, 1118847543700.0, 1118847543800.0, 1118847543900.0, 1118847544000.0, 1118847544100.0, 1118847544200.0, 1118847544300.0, 1118847544400.0, 1118847544500.0, 1118847544600.0, 1118847544700.0, 1118847544800.0, 1118847544900.0, 1118847545000.0, 1118847545100.0, 1118847545200.0, 1118847545300.0, 1118847545400.0, 1118847545500.0, 1118847545600.0, 1118847545700.0, 1118847545800.0, 1118847545900.0, 1118847546000.0, 1118847546100.0, 1118847546200.0, 1118847546300.0, 1118847546400.0, 1118847546500.0, 1118847546600.0, 1118847546700.0, 1118847546800.0, 1118847546900.0, 1118847547000.0, 1118847547100.0, 1118847547200.0, 1118847547300.0, 1118847547400.0, 1118847547500.0, 1118847547600.0, 1118847547700.0, 1118847547800.0, 1118847547900.0, 1118847548000.0, 1118847548100.0, 1118847548200.0, 1118847548300.0, 1118847548400.0, 1118847548500.0, 1118847548600.0, 1118847548700.0, 1118847548800.0, 1118847548900.0, 1118847549000.0, 1118847549100.0, 1118847549200.0, 1118847549300.0, 1118847549400.0, 1118847549500.0, 1118847549600.0, 1118847549700.0, 1118847549800.0, 1118847549900.0, 1118847550000.0, 1118847550100.0, 1118847550200.0, 1118847550300.0, 1118847550400.0, 1118847550500.0, 1118847550600.0, 1118847550700.0, 1118847550800.0, 1118847550900.0, 1118847551000.0, 1118847551100.0, 1118847551200.0, 1118847551300.0, 1118847551400.0, 1118847551500.0, 1118847551600.0, 1118847551700.0, 1118847551800.0, 1118847551900.0, 1118847552000.0, 1118847552100.0, 1118847552200.0, 1118847552300.0, 1118847552400.0, 1118847552500.0, 1118847552600.0, 1118847552700.0, 1118847552800.0, 1118847552900.0, 1118847553000.0, 1118847553100.0, 1118847553200.0, 1118847553300.0, 1118847553400.0, 1118847553500.0, 1118847553600.0, 1118847553700.0, 1118847553800.0, 1118847553900.0, 1118847554000.0, 1118847554100.0, 1118847554200.0, 1118847554300.0, 1118847554400.0, 1118847554500.0, 1118847554600.0, 1118847554700.0, 1118847554800.0, 1118847554900.0, 1118847555000.0, 1118847555100.0, 1118847555200.0, 1118847555300.0, 1118847555400.0, 1118847555500.0, 1118847555600.0, 1118847555700.0, 1118847555800.0, 1118847555900.0, 1118847556000.0, 1118847556100.0, 1118847556200.0, 1118847556300.0, 1118847556400.0, 1118847556500.0, 1118847556600.0, 1118847556700.0, 1118847556800.0, 1118847556900.0, 1118847557000.0, 1118847557100.0, 1118847557200.0, 1118847557300.0, 1118847557400.0, 1118847557500.0, 1118847557600.0, 1118847557700.0, 1118847557800.0, 1118847557900.0, 1118847558000.0, 1118847558100.0, 1118847558200.0, 1118847558300.0, 1118847558400.0, 1118847558500.0, 1118847558600.0, 1118847558700.0, 1118847558800.0, 1118847558900.0, 1118847559000.0, 1118847559100.0, 1118847559200.0, 1118847559300.0, 1118847559400.0, 1118847559500.0, 1118847559600.0, 1118847559700.0, 1118847559800.0, 1118847559900.0, 1118847560000.0, 1118847560100.0, 1118847560200.0, 1118847560300.0, 1118847560400.0, 1118847560500.0, 1118847560600.0, 1118847560700.0, 1118847560800.0, 1118847560900.0, 1118847561000.0, 1118847561100.0, 1118847561200.0, 1118847561300.0, 1118847561400.0, 1118847561500.0, 1118847561600.0, 1118847561700.0, 1118847561800.0, 1118847561900.0, 1118847562000.0, 1118847562100.0, 1118847562200.0, 1118847562300.0, 1118847562400.0, 1118847562500.0, 1118847562600.0, 1118847562700.0, 1118847562800.0, 1118847562900.0, 1118847563000.0, 1118847563100.0, 1118847563200.0, 1118847563300.0, 1118847563400.0, 1118847563500.0, 1118847563600.0, 1118847563700.0, 1118847563800.0, 1118847563900.0, 1118847564000.0, 1118847564100.0, 1118847564200.0, 1118847564300.0, 1118847564400.0, 1118847564500.0, 1118847564600.0, 1118847564700.0, 1118847564800.0, 1118847564900.0, 1118847565000.0, 1118847565100.0, 1118847565200.0, 1118847565300.0, 1118847565400.0, 1118847565500.0, 1118847565600.0, 1118847565700.0, 1118847565800.0, 1118847565900.0, 1118847566000.0, 1118847566100.0, 1118847566200.0, 1118847566300.0, 1118847566400.0, 1118847566500.0, 1118847566600.0, 1118847566700.0, 1118847566800.0, 1118847566900.0, 1118847567000.0, 1118847567100.0, 1118847567200.0, 1118847567300.0, 1118847567400.0, 1118847567500.0, 1118847567600.0, 1118847567700.0, 1118847567800.0, 1118847567900.0, 1118847568000.0, 1118847568100.0, 1118847568200.0, 1118847568300.0, 1118847568400.0, 1118847568500.0, 1118847568600.0, 1118847568700.0, 1118847568800.0, 1118847568900.0, 1118847569000.0, 1118847569100.0, 1118847569200.0, 1118847569300.0, 1118847569400.0, 1118847569500.0, 1118847569600.0, 1118847569700.0, 1118847569800.0, 1118847569900.0, 1118847570000.0, 1118847570100.0, 1118847570200.0, 1118847570300.0, 1118847570400.0, 1118847570500.0, 1118847570600.0, 1118847570700.0, 1118847570800.0, 1118847570900.0, 1118847571000.0, 1118847571100.0, 1118847571200.0, 1118847571300.0, 1118847571400.0, 1118847571500.0, 1118847571600.0, 1118847571700.0, 1118847571800.0, 1118847571900.0, 1118847572000.0, 1118847572100.0, 1118847572200.0, 1118847572300.0, 1118847572400.0, 1118847572500.0, 1118847572600.0, 1118847572700.0, 1118847572800.0, 1118847572900.0, 1118847573000.0, 1118847573100.0, 1118847573200.0, 1118847573300.0, 1118847573400.0, 1118847573500.0, 1118847573600.0, 1118847573700.0, 1118847573800.0, 1118847573900.0, 1118847574000.0, 1118847574100.0, 1118847574200.0, 1118847574300.0, 1118847574400.0, 1118847574500.0, 1118847574600.0, 1118847574700.0, 1118847574800.0, 1118847574900.0, 1118847575000.0, 1118847575100.0, 1118847575200.0, 1118847575300.0, 1118847575400.0, 1118847575500.0, 1118847575600.0, 1118847575700.0, 1118847575800.0, 1118847575900.0, 1118847576000.0, 1118847576100.0, 1118847576200.0, 1118847576300.0, 1118847576400.0, 1118847576500.0, 1118847576600.0, 1118847576700.0, 1118847576800.0, 1118847576900.0, 1118847577000.0, 1118847577100.0, 1118847577200.0, 1118847577300.0, 1118847577400.0, 1118847577500.0, 1118847577600.0, 1118847577700.0, 1118847577800.0, 1118847577900.0, 1118847578000.0, 1118847578100.0, 1118847578200.0, 1118847578300.0, 1118847578400.0, 1118847578500.0, 1118847578600.0, 1118847578700.0, 1118847578800.0, 1118847578900.0, 1118847579000.0, 1118847579100.0, 1118847579200.0, 1118847579300.0, 1118847579400.0, 1118847579500.0, 1118847579600.0, 1118847579700.0, 1118847579800.0, 1118847579900.0, 1118847580000.0, 1118847580100.0, 1118847580200.0, 1118847580300.0, 1118847580400.0, 1118847580500.0, 1118847580600.0, 1118847580700.0, 1118847580800.0, 1118847580900.0, 1118847581000.0, 1118847581100.0, 1118847581200.0, 1118847581300.0, 1118847581400.0, 1118847581500.0, 1118847581600.0, 1118847581700.0, 1118847581800.0, 1118847581900.0, 1118847582000.0, 1118847582100.0, 1118847582200.0, 1118847582300.0, 1118847582400.0, 1118847582500.0, 1118847582600.0, 1118847582700.0, 1118847582800.0, 1118847582900.0, 1118847583000.0, 1118847583100.0, 1118847583200.0, 1118847583300.0, 1118847583400.0, 1118847583500.0, 1118847583600.0, 1118847583700.0, 1118847583800.0, 1118847583900.0, 1118847584000.0, 1118847584100.0, 1118847584200.0, 1118847584300.0, 1118847584400.0, 1118847584500.0, 1118847584600.0, 1118847584700.0, 1118847584800.0, 1118847584900.0, 1118847585000.0, 1118847585100.0, 1118847585200.0, 1118847585300.0, 1118847585400.0, 1118847585500.0, 1118847585600.0, 1118847585700.0, 1118847585800.0, 1118847585900.0, 1118847586000.0, 1118847586100.0, 1118847586200.0, 1118847586300.0, 1118847586400.0, 1118847586500.0, 1118847586600.0, 1118847586700.0, 1118847586800.0, 1118847586900.0, 1118847587000.0, 1118847587100.0, 1118847587200.0, 1118847587300.0, 1118847587400.0, 1118847587500.0, 1118847587600.0, 1118847587700.0, 1118847587800.0, 1118847587900.0, 1118847588000.0, 1118847588100.0, 1118847588200.0, 1118847588300.0, 1118847588400.0, 1118847588500.0, 1118847588600.0, 1118847588700.0, 1118847588800.0, 1118847588900.0, 1118847589000.0, 1118847589100.0, 1118847589200.0, 1118847589300.0, 1118847589400.0, 1118847589500.0, 1118847589600.0, 1118847589700.0, 1118847589800.0, 1118847589900.0, 1118847590000.0, 1118847590100.0, 1118847590200.0, 1118847590300.0, 1118847590400.0, 1118847590500.0, 1118847590600.0, 1118847590700.0, 1118847590800.0, 1118847590900.0, 1118847591000.0, 1118847591100.0, 1118847591200.0, 1118847591300.0, 1118847591400.0, 1118847591500.0, 1118847591600.0, 1118847591700.0, 1118847591800.0, 1118847591900.0, 1118847592000.0, 1118847592100.0, 1118847592200.0, 1118847592300.0, 1118847592400.0, 1118847592500.0, 1118847592600.0, 1118847592700.0, 1118847592800.0, 1118847592900.0, 1118847593000.0, 1118847593100.0, 1118847593200.0, 1118847593300.0, 1118847593400.0, 1118847593500.0, 1118847593600.0, 1118847593700.0, 1118847593800.0, 1118847593900.0, 1118847594000.0, 1118847594100.0, 1118847594200.0, 1118847594300.0, 1118847594400.0, 1118847594500.0, 1118847594600.0, 1118847594700.0, 1118847594800.0, 1118847594900.0, 1118847595000.0, 1118847595100.0, 1118847595200.0, 1118847595300.0, 1118847595400.0, 1118847595500.0, 1118847595600.0, 1118847595700.0, 1118847595800.0, 1118847595900.0, 1118847596000.0, 1118847596100.0, 1118847596200.0, 1118847596300.0, 1118847596400.0, 1118847596500.0, 1118847596600.0, 1118847596700.0, 1118847596800.0, 1118847596900.0, 1118847597000.0, 1118847597100.0, 1118847597200.0, 1118847597300.0, 1118847597400.0, 1118847597500.0, 1118847597600.0, 1118847597700.0, 1118847597800.0, 1118847597900.0, 1118847598000.0, 1118847598100.0, 1118847598200.0, 1118847598300.0, 1118847598400.0, 1118847598500.0, 1118847598600.0, 1118847598700.0, 1118847598800.0, 1118847598900.0, 1118847599000.0, 1118847599100.0, 1118847599200.0, 1118847599300.0, 1118847599400.0, 1118847599500.0, 1118847599600.0, 1118847599700.0, 1118847599800.0, 1118847599900.0, 1118847600000.0, 1118847600100.0, 1118847600200.0, 1118847600300.0, 1118847600400.0, 1118847600500.0, 1118847600600.0, 1118847600700.0, 1118847600800.0, 1118847600900.0, 1118847601000.0, 1118847601100.0, 1118847601200.0, 1118847601300.0, 1118847601400.0, 1118847601500.0, 1118847601600.0, 1118847601700.0, 1118847601800.0, 1118847601900.0, 1118847602000.0, 1118847602100.0, 1118847602200.0, 1118847602300.0, 1118847602400.0, 1118847602500.0, 1118847602600.0, 1118847602700.0, 1118847602800.0, 1118847602900.0, 1118847603000.0, 1118847603100.0, 1118847603200.0, 1118847603300.0, 1118847603400.0, 1118847603500.0, 1118847603600.0, 1118847603700.0, 1118847603800.0, 1118847603900.0, 1118847604000.0, 1118847604100.0, 1118847604200.0, 1118847604300.0, 1118847604400.0, 1118847604500.0, 1118847604600.0, 1118847604700.0, 1118847604800.0, 1118847604900.0, 1118847605000.0, 1118847605100.0, 1118847605200.0, 1118847605300.0, 1118847605400.0, 1118847605500.0, 1118847605600.0, 1118847605700.0, 1118847605800.0, 1118847605900.0, 1118847606000.0, 1118847606100.0, 1118847606200.0, 1118847606300.0, 1118847606400.0, 1118847606500.0, 1118847606600.0, 1118847606700.0, 1118847606800.0, 1118847606900.0, 1118847607000.0, 1118847607100.0, 1118847607200.0, 1118847607300.0, 1118847607400.0, 1118847607500.0, 1118847607600.0, 1118847607700.0, 1118847607800.0, 1118847607900.0, 1118847608000.0, 1118847608100.0, 1118847608200.0, 1118847608300.0, 1118847608400.0, 1118847608500.0, 1118847608600.0, 1118847608700.0, 1118847608800.0, 1118847608900.0, 1118847609000.0, 1118847609100.0, 1118847609200.0, 1118847609300.0, 1118847609400.0, 1118847609500.0, 1118847609600.0, 1118847609700.0, 1118847609800.0, 1118847609900.0, 1118847610000.0, 1118847610100.0, 1118847610200.0, 1118847610300.0, 1118847610400.0, 1118847610500.0, 1118847610600.0, 1118847610700.0, 1118847610800.0, 1118847610900.0, 1118847611000.0, 1118847611100.0, 1118847611200.0, 1118847611300.0, 1118847611400.0, 1118847611500.0, 1118847611600.0, 1118847611700.0, 1118847611800.0, 1118847611900.0, 1118847612000.0, 1118847612100.0, 1118847612200.0, 1118847612300.0, 1118847612400.0, 1118847612500.0, 1118847612600.0, 1118847612700.0, 1118847612800.0, 1118847612900.0, 1118847613000.0, 1118847613100.0, 1118847613200.0, 1118847613300.0, 1118847613400.0, 1118847613500.0, 1118847613600.0, 1118847613700.0, 1118847613800.0, 1118847613900.0, 1118847614000.0, 1118847614100.0, 1118847614200.0, 1118847614300.0, 1118847614400.0, 1118847614500.0, 1118847614600.0, 1118847614700.0, 1118847614800.0, 1118847614900.0, 1118847615000.0, 1118847615100.0, 1118847615200.0, 1118847615300.0, 1118847615400.0, 1118847615500.0, 1118847615600.0, 1118847615700.0, 1118847615800.0, 1118847615900.0, 1118847616000.0, 1118847616100.0, 1118847616200.0, 1118847616300.0, 1118847616400.0, 1118847616500.0, 1118847616600.0, 1118847616700.0, 1118847616800.0, 1118847616900.0, 1118847617000.0, 1118847617100.0, 1118847617200.0, 1118847617300.0, 1118847617400.0, 1118847617500.0, 1118847617600.0, 1118847617700.0, 1118847617800.0, 1118847617900.0, 1118847618000.0, 1118847618100.0, 1118847618200.0, 1118847618300.0, 1118847618400.0, 1118847618500.0, 1118847618600.0, 1118847618700.0, 1118847618800.0, 1118847618900.0, 1118847619000.0, 1118847619100.0, 1118847619200.0, 1118847619300.0, 1118847619400.0, 1118847619500.0, 1118847619600.0, 1118847619700.0, 1118847619800.0, 1118847619900.0, 1118847620000.0, 1118847620100.0, 1118847620200.0, 1118847620300.0, 1118847620400.0, 1118847620500.0, 1118847620600.0, 1118847620700.0, 1118847620800.0, 1118847620900.0, 1118847621000.0, 1118847621100.0, 1118847621200.0, 1118847621300.0, 1118847621400.0, 1118847621500.0, 1118847621600.0, 1118847621700.0, 1118847621800.0, 1118847621900.0, 1118847622000.0, 1118847622100.0, 1118847622200.0, 1118847622300.0, 1118847622400.0, 1118847622500.0, 1118847622600.0, 1118847622700.0, 1118847622800.0, 1118847622900.0, 1118847623000.0, 1118847623100.0, 1118847623200.0, 1118847623300.0, 1118847623400.0, 1118847623500.0, 1118847623600.0, 1118847623700.0, 1118847623800.0, 1118847623900.0, 1118847624000.0, 1118847624100.0, 1118847624200.0, 1118847624300.0, 1118847624400.0, 1118847624500.0, 1118847624600.0, 1118847624700.0, 1118847624800.0, 1118847624900.0, 1118847625000.0, 1118847625100.0, 1118847625200.0, 1118847625300.0, 1118847625400.0, 1118847625500.0, 1118847625600.0, 1118847625700.0, 1118847625800.0, 1118847625900.0, 1118847626000.0, 1118847626100.0, 1118847626200.0, 1118847626300.0, 1118847626400.0, 1118847626500.0, 1118847626600.0, 1118847626700.0, 1118847626800.0, 1118847626900.0, 1118847627000.0, 1118847627100.0, 1118847627200.0, 1118847627300.0, 1118847627400.0, 1118847627500.0, 1118847627600.0, 1118847627700.0, 1118847627800.0, 1118847627900.0, 1118847628000.0, 1118847628100.0, 1118847628200.0, 1118847628300.0, 1118847628400.0, 1118847628500.0, 1118847628600.0, 1118847628700.0, 1118847628800.0, 1118847628900.0, 1118847629000.0, 1118847629100.0, 1118847629200.0, 1118847629300.0, 1118847629400.0, 1118847629500.0, 1118847629600.0, 1118847629700.0, 1118847629800.0, 1118847629900.0, 1118847630000.0, 1118847630100.0, 1118847630200.0, 1118847630300.0, 1118847630400.0, 1118847630500.0, 1118847630600.0, 1118847630700.0, 1118847630800.0, 1118847630900.0, 1118847631000.0, 1118847631100.0, 1118847631200.0, 1118847631300.0, 1118847631400.0, 1118847631500.0, 1118847631600.0, 1118847631700.0, 1118847631800.0, 1118847631900.0, 1118847632000.0, 1118847632100.0, 1118847632200.0, 1118847632300.0, 1118847632400.0, 1118847632500.0, 1118847632600.0, 1118847632700.0, 1118847632800.0, 1118847632900.0, 1118847633000.0, 1118847633100.0, 1118847633200.0, 1118847633300.0, 1118847633400.0, 1118847633500.0, 1118847633600.0, 1118847633700.0, 1118847633800.0, 1118847633900.0, 1118847634000.0, 1118847634100.0, 1118847634200.0, 1118847634300.0, 1118847634400.0, 1118847634500.0, 1118847634600.0, 1118847634700.0, 1118847634800.0, 1118847634900.0, 1118847635000.0, 1118847635100.0, 1118847635200.0, 1118847635300.0, 1118847635400.0, 1118847635500.0, 1118847635600.0, 1118847635700.0, 1118847635800.0, 1118847635900.0, 1118847636000.0, 1118847636100.0, 1118847636200.0, 1118847636300.0, 1118847636400.0, 1118847636500.0, 1118847636600.0, 1118847636700.0, 1118847636800.0, 1118847636900.0, 1118847637000.0, 1118847637100.0, 1118847637200.0, 1118847637300.0, 1118847637400.0, 1118847637500.0, 1118847637600.0, 1118847637700.0, 1118847637800.0, 1118847637900.0, 1118847638000.0, 1118847638100.0, 1118847638200.0, 1118847638300.0, 1118847638400.0, 1118847638500.0, 1118847638600.0, 1118847638700.0, 1118847638800.0, 1118847638900.0, 1118847639000.0, 1118847639100.0, 1118847639200.0, 1118847639300.0, 1118847639400.0, 1118847639500.0, 1118847639600.0, 1118847639700.0, 1118847639800.0, 1118847639900.0, 1118847640000.0, 1118847640100.0, 1118847640200.0, 1118847640300.0, 1118847640400.0, 1118847640500.0, 1118847640600.0, 1118847640700.0, 1118847640800.0, 1118847640900.0, 1118847641000.0, 1118847641100.0, 1118847641200.0, 1118847641300.0, 1118847641400.0, 1118847641500.0, 1118847641600.0, 1118847641700.0, 1118847641800.0, 1118847641900.0, 1118847642000.0, 1118847642100.0, 1118847642200.0, 1118847642300.0, 1118847642400.0, 1118847642500.0, 1118847642600.0, 1118847642700.0, 1118847642800.0, 1118847642900.0, 1118847643000.0, 1118847643100.0, 1118847643200.0, 1118847643300.0, 1118847643400.0, 1118847643500.0, 1118847643600.0, 1118847643700.0, 1118847643800.0, 1118847643900.0, 1118847644000.0, 1118847644100.0, 1118847644200.0, 1118847644300.0, 1118847644400.0, 1118847644500.0, 1118847644600.0, 1118847644700.0, 1118847644800.0, 1118847644900.0, 1118847645000.0, 1118847645100.0, 1118847645200.0, 1118847645300.0, 1118847645400.0, 1118847645500.0, 1118847645600.0, 1118847645700.0, 1118847645800.0, 1118847645900.0, 1118847646000.0, 1118847646100.0, 1118847646200.0, 1118847646300.0, 1118847646400.0, 1118847646500.0, 1118847646600.0, 1118847646700.0, 1118847646800.0, 1118847646900.0, 1118847647000.0, 1118847647100.0, 1118847647200.0, 1118847647300.0, 1118847647400.0, 1118847647500.0, 1118847647600.0, 1118847647700.0, 1118847647800.0, 1118847647900.0, 1118847648000.0, 1118847648100.0, 1118847648200.0, 1118847648300.0, 1118847648400.0, 1118847648500.0, 1118847648600.0, 1118847648700.0, 1118847648800.0, 1118847648900.0, 1118847649000.0, 1118847649100.0, 1118847649200.0, 1118847649300.0, 1118847649400.0, 1118847649500.0, 1118847649600.0, 1118847649700.0, 1118847649800.0, 1118847649900.0, 1118847650000.0, 1118847650100.0, 1118847650200.0, 1118847650300.0, 1118847650400.0, 1118847650500.0, 1118847650600.0, 1118847650700.0, 1118847650800.0, 1118847650900.0, 1118847651000.0, 1118847651100.0, 1118847651200.0, 1118847651300.0, 1118847651400.0, 1118847651500.0, 1118847651600.0, 1118847651700.0, 1118847651800.0, 1118847651900.0, 1118847652000.0, 1118847652100.0, 1118847652200.0, 1118847652300.0, 1118847652400.0, 1118847652500.0, 1118847652600.0, 1118847652700.0, 1118847652800.0, 1118847652900.0, 1118847653000.0, 1118847653100.0, 1118847653200.0, 1118847653300.0, 1118847653400.0, 1118847653500.0, 1118847653600.0, 1118847653700.0, 1118847653800.0, 1118847653900.0, 1118847654000.0, 1118847654100.0, 1118847654200.0, 1118847654300.0, 1118847654400.0, 1118847654500.0, 1118847654600.0, 1118847654700.0, 1118847654800.0, 1118847654900.0, 1118847655000.0, 1118847655100.0, 1118847655200.0, 1118847655300.0, 1118847655400.0, 1118847655500.0, 1118847655600.0, 1118847655700.0, 1118847655800.0, 1118847655900.0, 1118847656000.0, 1118847656100.0, 1118847656200.0, 1118847656300.0, 1118847656400.0, 1118847656500.0, 1118847656600.0, 1118847656700.0, 1118847656800.0, 1118847656900.0, 1118847657000.0, 1118847657100.0, 1118847657200.0, 1118847657300.0, 1118847657400.0, 1118847657500.0, 1118847657600.0, 1118847657700.0, 1118847657800.0, 1118847657900.0, 1118847658000.0, 1118847658100.0, 1118847658200.0, 1118847658300.0, 1118847658400.0, 1118847658500.0, 1118847658600.0, 1118847658700.0, 1118847658800.0, 1118847658900.0, 1118847659000.0, 1118847659100.0, 1118847659200.0, 1118847659300.0, 1118847659400.0, 1118847659500.0, 1118847659600.0, 1118847659700.0, 1118847659800.0, 1118847659900.0, 1118847660000.0, 1118847660100.0, 1118847660200.0, 1118847660300.0, 1118847660400.0, 1118847660500.0, 1118847660600.0, 1118847660700.0, 1118847660800.0, 1118847660900.0, 1118847661000.0, 1118847661100.0, 1118847661200.0, 1118847661300.0, 1118847661400.0, 1118847661500.0, 1118847661600.0, 1118847661700.0, 1118847661800.0, 1118847661900.0, 1118847662000.0, 1118847662100.0, 1118847662200.0, 1118847662300.0, 1118847662400.0, 1118847662500.0, 1118847662600.0, 1118847662700.0, 1118847662800.0, 1118847662900.0, 1118847663000.0, 1118847663100.0, 1118847663200.0, 1118847663300.0, 1118847663400.0, 1118847663500.0, 1118847663600.0, 1118847663700.0, 1118847663800.0, 1118847663900.0, 1118847664000.0, 1118847664100.0, 1118847664200.0, 1118847664300.0, 1118847664400.0, 1118847664500.0, 1118847664600.0, 1118847664700.0, 1118847664800.0, 1118847664900.0, 1118847665000.0, 1118847665100.0, 1118847665200.0, 1118847665300.0, 1118847665400.0, 1118847665500.0, 1118847665600.0, 1118847665700.0, 1118847665800.0, 1118847665900.0, 1118847666000.0, 1118847666100.0, 1118847666200.0, 1118847666300.0, 1118847666400.0, 1118847666500.0, 1118847666600.0, 1118847666700.0, 1118847666800.0, 1118847666900.0, 1118847667000.0, 1118847667100.0, 1118847667200.0, 1118847667300.0, 1118847667400.0, 1118847667500.0, 1118847667600.0, 1118847667700.0, 1118847667800.0, 1118847667900.0, 1118847668000.0, 1118847668100.0, 1118847668200.0, 1118847668300.0, 1118847668400.0, 1118847668500.0, 1118847668600.0, 1118847668700.0, 1118847668800.0, 1118847668900.0, 1118847669000.0, 1118847669100.0, 1118847669200.0, 1118847669300.0, 1118847669400.0, 1118847669500.0, 1118847669600.0, 1118847669700.0, 1118847669800.0, 1118847669900.0, 1118847670000.0, 1118847670100.0, 1118847670200.0, 1118847670300.0, 1118847670400.0, 1118847670500.0, 1118847670600.0, 1118847670700.0, 1118847670800.0, 1118847670900.0, 1118847671000.0, 1118847671100.0, 1118847671200.0, 1118847671300.0, 1118847671400.0, 1118847671500.0, 1118847671600.0, 1118847671700.0, 1118847671800.0, 1118847671900.0, 1118847672000.0, 1118847672100.0, 1118847672200.0, 1118847672300.0, 1118847672400.0, 1118847672500.0, 1118847672600.0, 1118847672700.0, 1118847672800.0, 1118847672900.0, 1118847673000.0, 1118847673100.0, 1118847673200.0, 1118847673300.0, 1118847673400.0, 1118847673500.0, 1118847673600.0, 1118847673700.0, 1118847673800.0, 1118847673900.0, 1118847674000.0, 1118847674100.0, 1118847674200.0, 1118847674300.0, 1118847674400.0, 1118847674500.0, 1118847674600.0, 1118847674700.0, 1118847674800.0, 1118847674900.0, 1118847675000.0, 1118847675100.0, 1118847675200.0, 1118847675300.0, 1118847675400.0, 1118847675500.0, 1118847675600.0, 1118847675700.0, 1118847675800.0, 1118847675900.0, 1118847676000.0, 1118847676100.0, 1118847676200.0, 1118847676300.0, 1118847676400.0, 1118847676500.0, 1118847676600.0, 1118847676700.0, 1118847676800.0, 1118847676900.0, 1118847677000.0, 1118847677100.0, 1118847677200.0, 1118847677300.0, 1118847677400.0, 1118847677500.0, 1118847677600.0, 1118847677700.0, 1118847677800.0, 1118847677900.0, 1118847678000.0, 1118847678100.0, 1118847678200.0, 1118847678300.0, 1118847678400.0, 1118847678500.0, 1118847678600.0, 1118847678700.0, 1118847678800.0, 1118847678900.0, 1118847679000.0, 1118847679100.0, 1118847679200.0, 1118847679300.0, 1118847679400.0, 1118847679500.0, 1118847679600.0, 1118847679700.0, 1118847679800.0, 1118847679900.0, 1118847680000.0, 1118847680100.0, 1118847680200.0, 1118847680300.0, 1118847680400.0, 1118847680500.0, 1118847680600.0, 1118847680700.0, 1118847680800.0, 1118847680900.0, 1118847681000.0, 1118847681100.0, 1118847681200.0, 1118847681300.0, 1118847681400.0, 1118847681500.0, 1118847681600.0, 1118847681700.0, 1118847681800.0, 1118847681900.0, 1118847682000.0, 1118847682100.0, 1118847682200.0, 1118847682300.0, 1118847682400.0, 1118847682500.0, 1118847682600.0, 1118847682700.0, 1118847682800.0, 1118847682900.0, 1118847683000.0, 1118847683100.0, 1118847683200.0, 1118847683300.0, 1118847683400.0, 1118847683500.0, 1118847683600.0, 1118847683700.0, 1118847683800.0, 1118847683900.0, 1118847684000.0, 1118847684100.0, 1118847684200.0, 1118847684300.0, 1118847684400.0, 1118847684500.0, 1118847684600.0, 1118847684700.0, 1118847684800.0, 1118847684900.0, 1118847685000.0, 1118847685100.0, 1118847685200.0, 1118847685300.0, 1118847685400.0, 1118847685500.0, 1118847685600.0, 1118847685700.0, 1118847685800.0, 1118847685900.0, 1118847686000.0, 1118847686100.0, 1118847686200.0, 1118847686300.0, 1118847686400.0, 1118847686500.0, 1118847686600.0, 1118847686700.0, 1118847686800.0, 1118847686900.0, 1118847687000.0, 1118847687100.0, 1118847687200.0, 1118847687300.0, 1118847687400.0, 1118847687500.0, 1118847687600.0, 1118847687700.0, 1118847687800.0, 1118847687900.0, 1118847688000.0, 1118847688100.0, 1118847688200.0, 1118847688300.0, 1118847688400.0, 1118847688500.0, 1118847688600.0, 1118847688700.0, 1118847688800.0, 1118847688900.0, 1118847689000.0, 1118847689100.0, 1118847689200.0, 1118847689300.0, 1118847689400.0, 1118847689500.0, 1118847689600.0, 1118847689700.0, 1118847689800.0, 1118847689900.0, 1118847690000.0, 1118847690100.0, 1118847690200.0, 1118847690300.0, 1118847690400.0, 1118847690500.0, 1118847690600.0, 1118847690700.0, 1118847690800.0, 1118847690900.0, 1118847691000.0, 1118847691100.0, 1118847691200.0, 1118847691300.0, 1118847691400.0, 1118847691500.0, 1118847691600.0, 1118847691700.0, 1118847691800.0, 1118847691900.0, 1118847692000.0, 1118847692100.0, 1118847692200.0, 1118847692300.0, 1118847692400.0, 1118847692500.0, 1118847692600.0, 1118847692700.0, 1118847692800.0, 1118847692900.0, 1118847693000.0, 1118847693100.0, 1118847693200.0, 1118847693300.0, 1118847693400.0, 1118847693500.0, 1118847693600.0, 1118847693700.0, 1118847693800.0, 1118847693900.0, 1118847694000.0, 1118847694100.0, 1118847694200.0, 1118847694300.0, 1118847694400.0, 1118847694500.0, 1118847694600.0, 1118847694700.0, 1118847694800.0, 1118847694900.0, 1118847695000.0, 1118847695100.0, 1118847695200.0, 1118847695300.0, 1118847695400.0, 1118847695500.0, 1118847695600.0, 1118847695700.0, 1118847695800.0, 1118847695900.0, 1118847696000.0, 1118847696100.0, 1118847696200.0, 1118847696300.0, 1118847696400.0, 1118847696500.0, 1118847696600.0, 1118847696700.0, 1118847696800.0, 1118847696900.0, 1118847697000.0, 1118847697100.0, 1118847697200.0, 1118847697300.0, 1118847697400.0, 1118847697500.0, 1118847697600.0, 1118847697700.0, 1118847697800.0, 1118847697900.0, 1118847698000.0, 1118847698100.0, 1118847698200.0, 1118847698300.0, 1118847698400.0, 1118847698500.0, 1118847698600.0, 1118847698700.0, 1118847698800.0, 1118847698900.0, 1118847699000.0, 1118847699100.0, 1118847699200.0, 1118847699300.0, 1118847699400.0, 1118847699500.0, 1118847699600.0, 1118847699700.0, 1118847699800.0, 1118847699900.0, 1118847700000.0, 1118847700100.0, 1118847700200.0, 1118847700300.0, 1118847700400.0, 1118847700500.0, 1118847700600.0, 1118847700700.0, 1118847700800.0, 1118847700900.0, 1118847701000.0, 1118847701100.0, 1118847701200.0, 1118847701300.0, 1118847701400.0, 1118847701500.0, 1118847701600.0, 1118847701700.0, 1118847701800.0, 1118847701900.0, 1118847702000.0, 1118847702100.0, 1118847702200.0, 1118847702300.0, 1118847702400.0, 1118847702500.0, 1118847702600.0, 1118847702700.0, 1118847702800.0, 1118847702900.0, 1118847703000.0, 1118847703100.0, 1118847703200.0, 1118847703300.0, 1118847703400.0, 1118847703500.0, 1118847703600.0, 1118847703700.0, 1118847703800.0, 1118847703900.0, 1118847704000.0, 1118847704100.0, 1118847704200.0, 1118847704300.0, 1118847704400.0, 1118847704500.0, 1118847704600.0, 1118847704700.0, 1118847704800.0, 1118847704900.0, 1118847705000.0, 1118847705100.0, 1118847705200.0, 1118847705300.0, 1118847705400.0, 1118847705500.0, 1118847705600.0, 1118847705700.0, 1118847705800.0, 1118847705900.0, 1118847706000.0, 1118847706100.0, 1118847706200.0, 1118847706300.0, 1118847706400.0, 1118847706500.0, 1118847706600.0, 1118847706700.0, 1118847706800.0, 1118847706900.0, 1118847707000.0, 1118847707100.0, 1118847707200.0, 1118847707300.0, 1118847707400.0, 1118847707500.0, 1118847707600.0, 1118847707700.0, 1118847707800.0, 1118847707900.0, 1118847708000.0, 1118847708100.0, 1118847708200.0, 1118847708300.0, 1118847708400.0, 1118847708500.0, 1118847708600.0, 1118847708700.0, 1118847708800.0, 1118847708900.0, 1118847709000.0, 1118847709100.0, 1118847709200.0, 1118847709300.0, 1118847709400.0, 1118847709500.0, 1118847709600.0, 1118847709700.0, 1118847709800.0, 1118847709900.0, 1118847710000.0, 1118847710100.0, 1118847710200.0, 1118847710300.0, 1118847710400.0, 1118847710500.0, 1118847710600.0, 1118847710700.0, 1118847710800.0, 1118847710900.0, 1118847711000.0, 1118847711100.0, 1118847711200.0, 1118847711300.0, 1118847711400.0, 1118847711500.0, 1118847711600.0, 1118847711700.0, 1118847711800.0, 1118847711900.0, 1118847712000.0, 1118847712100.0, 1118847712200.0, 1118847712300.0, 1118847712400.0, 1118847712500.0, 1118847712600.0, 1118847712700.0, 1118847712800.0, 1118847712900.0, 1118847713000.0, 1118847713100.0, 1118847713200.0, 1118847713300.0, 1118847713400.0, 1118847713500.0, 1118847713600.0, 1118847713700.0, 1118847713800.0, 1118847713900.0, 1118847714000.0, 1118847714100.0, 1118847714200.0, 1118847714300.0, 1118847714400.0, 1118847714500.0, 1118847714600.0, 1118847714700.0, 1118847714800.0, 1118847714900.0, 1118847715000.0, 1118847715100.0, 1118847715200.0, 1118847715300.0, 1118847715400.0, 1118847715500.0, 1118847715600.0, 1118847715700.0, 1118847715800.0, 1118847715900.0, 1118847716000.0, 1118847716100.0, 1118847716200.0, 1118847716300.0, 1118847716400.0, 1118847716500.0, 1118847716600.0, 1118847716700.0, 1118847716800.0, 1118847716900.0, 1118847717000.0, 1118847717100.0, 1118847717200.0, 1118847717300.0, 1118847717400.0, 1118847717500.0, 1118847717600.0, 1118847717700.0, 1118847717800.0, 1118847717900.0, 1118847718000.0, 1118847718100.0, 1118847718200.0, 1118847718300.0, 1118847718400.0, 1118847718500.0, 1118847718600.0, 1118847718700.0, 1118847718800.0, 1118847718900.0, 1118847719000.0, 1118847719100.0, 1118847719200.0, 1118847719300.0, 1118847719400.0, 1118847719500.0, 1118847719600.0, 1118847719700.0, 1118847719800.0, 1118847719900.0, 1118847720000.0, 1118847720100.0, 1118847720200.0, 1118847720300.0, 1118847720400.0, 1118847720500.0, 1118847720600.0, 1118847720700.0, 1118847720800.0, 1118847720900.0, 1118847721000.0, 1118847721100.0, 1118847721200.0, 1118847721300.0, 1118847721400.0, 1118847721500.0, 1118847721600.0, 1118847721700.0, 1118847721800.0, 1118847721900.0, 1118847722000.0, 1118847722100.0, 1118847722200.0, 1118847722300.0, 1118847722400.0, 1118847722500.0, 1118847722600.0, 1118847722700.0, 1118847722800.0, 1118847722900.0, 1118847723000.0, 1118847723100.0, 1118847723200.0, 1118847723300.0, 1118847723400.0, 1118847723500.0, 1118847723600.0, 1118847723700.0, 1118847723800.0, 1118847723900.0, 1118847724000.0, 1118847724100.0, 1118847724200.0, 1118847724300.0, 1118847724400.0, 1118847724500.0, 1118847724600.0, 1118847724700.0, 1118847724800.0, 1118847724900.0, 1118847725000.0, 1118847725100.0, 1118847725200.0, 1118847725300.0, 1118847725400.0, 1118847725500.0, 1118847725600.0, 1118847725700.0, 1118847725800.0, 1118847725900.0, 1118847726000.0, 1118847726100.0, 1118847726200.0, 1118847726300.0, 1118847726400.0, 1118847726500.0, 1118847726600.0, 1118847726700.0, 1118847726800.0, 1118847726900.0, 1118847727000.0, 1118847727100.0, 1118847727200.0, 1118847727300.0, 1118847727400.0, 1118847727500.0, 1118847727600.0, 1118847727700.0, 1118847727800.0, 1118847727900.0, 1118847728000.0, 1118847728100.0, 1118847728200.0, 1118847728300.0, 1118847728400.0, 1118847728500.0, 1118847728600.0, 1118847728700.0, 1118847728800.0, 1118847728900.0, 1118847729000.0, 1118847729100.0, 1118847729200.0, 1118847729300.0, 1118847729400.0, 1118847729500.0, 1118847729600.0, 1118847729700.0, 1118847729800.0, 1118847729900.0, 1118847730000.0, 1118847730100.0, 1118847730200.0, 1118847730300.0, 1118847730400.0, 1118847730500.0, 1118847730600.0, 1118847730700.0, 1118847730800.0, 1118847730900.0, 1118847731000.0, 1118847731100.0, 1118847731200.0, 1118847731300.0, 1118847731400.0, 1118847731500.0, 1118847731600.0, 1118847731700.0, 1118847731800.0, 1118847731900.0, 1118847732000.0, 1118847732100.0, 1118847732200.0, 1118847732300.0, 1118847732400.0, 1118847732500.0, 1118847732600.0, 1118847732700.0, 1118847732800.0, 1118847732900.0, 1118847733000.0, 1118847733100.0, 1118847733200.0, 1118847733300.0, 1118847733400.0, 1118847733500.0, 1118847733600.0, 1118847733700.0, 1118847733800.0, 1118847733900.0, 1118847734000.0, 1118847734100.0, 1118847734200.0, 1118847734300.0, 1118847734400.0, 1118847734500.0, 1118847734600.0, 1118847734700.0, 1118847734800.0, 1118847734900.0, 1118847735000.0, 1118847735100.0, 1118847735200.0, 1118847735300.0, 1118847735400.0, 1118847735500.0, 1118847735600.0, 1118847735700.0, 1118847735800.0, 1118847735900.0, 1118847736000.0, 1118847736100.0, 1118847736200.0, 1118847736300.0, 1118847736400.0, 1118847736500.0, 1118847736600.0, 1118847736700.0, 1118847736800.0, 1118847736900.0, 1118847737000.0, 1118847737100.0, 1118847737200.0, 1118847737300.0, 1118847737400.0, 1118847737500.0, 1118847737600.0, 1118847737700.0, 1118847737800.0, 1118847737900.0, 1118847738000.0, 1118847738100.0, 1118847738200.0, 1118847738300.0, 1118847738400.0, 1118847738500.0, 1118847738600.0, 1118847738700.0, 1118847738800.0, 1118847738900.0, 1118847739000.0, 1118847739100.0, 1118847739200.0, 1118847739300.0, 1118847739400.0, 1118847739500.0, 1118847739600.0, 1118847739700.0, 1118847739800.0, 1118847739900.0, 1118847740000.0, 1118847740100.0, 1118847740200.0, 1118847740300.0, 1118847740400.0, 1118847740500.0, 1118847740600.0, 1118847740700.0, 1118847740800.0, 1118847740900.0, 1118847741000.0, 1118847741100.0, 1118847741200.0, 1118847741300.0, 1118847741400.0, 1118847741500.0, 1118847741600.0, 1118847741700.0, 1118847741800.0, 1118847741900.0, 1118847742000.0, 1118847742100.0, 1118847742200.0, 1118847742300.0, 1118847742400.0, 1118847742500.0, 1118847742600.0, 1118847742700.0, 1118847742800.0, 1118847742900.0, 1118847743000.0, 1118847743100.0, 1118847743200.0, 1118847743300.0, 1118847743400.0, 1118847743500.0, 1118847743600.0, 1118847743700.0, 1118847743800.0, 1118847743900.0, 1118847744000.0, 1118847744100.0, 1118847744200.0, 1118847744300.0, 1118847744400.0, 1118847744500.0, 1118847744600.0, 1118847744700.0, 1118847744800.0, 1118847744900.0, 1118847745000.0, 1118847745100.0, 1118847745200.0, 1118847745300.0, 1118847745400.0, 1118847745500.0, 1118847745600.0, 1118847745700.0, 1118847745800.0, 1118847745900.0, 1118847746000.0, 1118847746100.0, 1118847746200.0, 1118847746300.0, 1118847746400.0, 1118847746500.0, 1118847746600.0, 1118847746700.0, 1118847746800.0, 1118847746900.0, 1118847747000.0, 1118847747100.0, 1118847747200.0, 1118847747300.0, 1118847747400.0, 1118847747500.0, 1118847747600.0, 1118847747700.0, 1118847747800.0, 1118847747900.0, 1118847748000.0, 1118847748100.0, 1118847748200.0, 1118847748300.0, 1118847748400.0, 1118847748500.0, 1118847748600.0, 1118847748700.0, 1118847748800.0, 1118847748900.0, 1118847749000.0, 1118847749100.0, 1118847749200.0, 1118847749300.0, 1118847749400.0, 1118847749500.0, 1118847749600.0, 1118847749700.0, 1118847749800.0, 1118847749900.0, 1118847750000.0, 1118847750100.0, 1118847750200.0, 1118847750300.0, 1118847750400.0, 1118847750500.0, 1118847750600.0, 1118847750700.0, 1118847750800.0, 1118847750900.0, 1118847751000.0, 1118847751100.0, 1118847751200.0, 1118847751300.0, 1118847751400.0, 1118847751500.0, 1118847751600.0, 1118847751700.0, 1118847751800.0, 1118847751900.0, 1118847752000.0, 1118847752100.0, 1118847752200.0, 1118847752300.0, 1118847752400.0, 1118847752500.0, 1118847752600.0, 1118847752700.0, 1118847752800.0, 1118847752900.0, 1118847753000.0, 1118847753100.0, 1118847753200.0, 1118847753300.0, 1118847753400.0, 1118847753500.0, 1118847753600.0, 1118847753700.0, 1118847753800.0, 1118847753900.0, 1118847754000.0, 1118847754100.0, 1118847754200.0, 1118847754300.0, 1118847754400.0, 1118847754500.0, 1118847754600.0, 1118847754700.0, 1118847754800.0, 1118847754900.0, 1118847755000.0, 1118847755100.0, 1118847755200.0, 1118847755300.0, 1118847755400.0, 1118847755500.0, 1118847755600.0, 1118847755700.0, 1118847755800.0, 1118847755900.0, 1118847756000.0, 1118847756100.0, 1118847756200.0, 1118847756300.0, 1118847756400.0, 1118847756500.0, 1118847756600.0, 1118847756700.0, 1118847756800.0, 1118847756900.0, 1118847757000.0, 1118847757100.0, 1118847757200.0, 1118847757300.0, 1118847757400.0, 1118847757500.0, 1118847757600.0, 1118847757700.0, 1118847757800.0, 1118847757900.0, 1118847758000.0, 1118847758100.0, 1118847758200.0, 1118847758300.0, 1118847758400.0, 1118847758500.0, 1118847758600.0, 1118847758700.0, 1118847758800.0, 1118847758900.0, 1118847759000.0, 1118847759100.0, 1118847759200.0, 1118847759300.0, 1118847759400.0, 1118847759500.0, 1118847759600.0, 1118847759700.0, 1118847759800.0, 1118847759900.0, 1118847760000.0, 1118847760100.0, 1118847760200.0, 1118847760300.0, 1118847760400.0, 1118847760500.0, 1118847760600.0, 1118847760700.0, 1118847760800.0, 1118847760900.0, 1118847761000.0, 1118847761100.0, 1118847761200.0, 1118847761300.0, 1118847761400.0, 1118847761500.0, 1118847761600.0, 1118847761700.0, 1118847761800.0, 1118847761900.0, 1118847762000.0, 1118847762100.0, 1118847762200.0, 1118847762300.0, 1118847762400.0, 1118847762500.0, 1118847762600.0, 1118847762700.0, 1118847762800.0, 1118847762900.0, 1118847763000.0, 1118847763100.0, 1118847763200.0, 1118847763300.0, 1118847763400.0, 1118847763500.0, 1118847763600.0, 1118847763700.0, 1118847763800.0, 1118847763900.0, 1118847764000.0, 1118847764100.0, 1118847764200.0, 1118847764300.0, 1118847764400.0, 1118847764500.0, 1118847764600.0, 1118847764700.0, 1118847764800.0, 1118847764900.0, 1118847765000.0, 1118847765100.0, 1118847765200.0, 1118847765300.0, 1118847765400.0, 1118847765500.0, 1118847765600.0, 1118847765700.0, 1118847765800.0, 1118847765900.0, 1118847766000.0, 1118847766100.0, 1118847766200.0, 1118847766300.0, 1118847766400.0, 1118847766500.0, 1118847766600.0, 1118847766700.0, 1118847766800.0, 1118847766900.0, 1118847767000.0, 1118847767100.0, 1118847767200.0, 1118847767300.0, 1118847767400.0, 1118847767500.0, 1118847767600.0, 1118847767700.0, 1118847767800.0, 1118847767900.0, 1118847768000.0, 1118847768100.0, 1118847768200.0, 1118847768300.0, 1118847768400.0, 1118847768500.0, 1118847768600.0, 1118847768700.0, 1118847768800.0, 1118847768900.0, 1118847769000.0, 1118847769100.0, 1118847769200.0, 1118847769300.0, 1118847769400.0, 1118847769500.0, 1118847769600.0, 1118847769700.0, 1118847769800.0, 1118847769900.0, 1118847770000.0, 1118847770100.0, 1118847770200.0, 1118847770300.0, 1118847770400.0, 1118847770500.0, 1118847770600.0, 1118847770700.0, 1118847770800.0, 1118847770900.0, 1118847771000.0, 1118847771100.0, 1118847771200.0, 1118847771300.0, 1118847771400.0, 1118847771500.0, 1118847771600.0, 1118847771700.0, 1118847771800.0, 1118847771900.0, 1118847772000.0, 1118847772100.0, 1118847772200.0, 1118847772300.0, 1118847772400.0, 1118847772500.0, 1118847772600.0, 1118847772700.0, 1118847772800.0, 1118847772900.0, 1118847773000.0, 1118847773100.0, 1118847773200.0, 1118847773300.0, 1118847773400.0, 1118847773500.0, 1118847773600.0, 1118847773700.0, 1118847773800.0, 1118847773900.0, 1118847774000.0, 1118847774100.0, 1118847774200.0, 1118847774300.0, 1118847774400.0, 1118847774500.0, 1118847774600.0, 1118847774700.0, 1118847774800.0, 1118847774900.0, 1118847775000.0, 1118847775100.0, 1118847775200.0, 1118847775300.0, 1118847775400.0, 1118847775500.0, 1118847775600.0, 1118847775700.0, 1118847775800.0, 1118847775900.0, 1118847776000.0, 1118847776100.0, 1118847776200.0, 1118847776300.0, 1118847776400.0, 1118847776500.0, 1118847776600.0, 1118847776700.0, 1118847776800.0, 1118847776900.0, 1118847777000.0, 1118847777100.0, 1118847777200.0, 1118847777300.0, 1118847777400.0, 1118847777500.0, 1118847777600.0, 1118847777700.0, 1118847777800.0, 1118847777900.0, 1118847778000.0, 1118847778100.0, 1118847778200.0, 1118847778300.0, 1118847778400.0, 1118847778500.0, 1118847778600.0, 1118847778700.0, 1118847778800.0, 1118847778900.0, 1118847779000.0, 1118847779100.0, 1118847779200.0, 1118847779300.0, 1118847779400.0, 1118847779500.0, 1118847779600.0, 1118847779700.0, 1118847779800.0, 1118847779900.0, 1118847780000.0, 1118847780100.0, 1118847780200.0, 1118847780300.0, 1118847780400.0, 1118847780500.0, 1118847780600.0, 1118847780700.0, 1118847780800.0, 1118847780900.0, 1118847781000.0, 1118847781100.0, 1118847781200.0, 1118847781300.0, 1118847781400.0, 1118847781500.0, 1118847781600.0, 1118847781700.0, 1118847781800.0, 1118847781900.0, 1118847782000.0, 1118847782100.0, 1118847782200.0, 1118847782300.0, 1118847782400.0, 1118847782500.0, 1118847782600.0, 1118847782700.0, 1118847782800.0, 1118847782900.0, 1118847783000.0, 1118847783100.0, 1118847783200.0, 1118847783300.0, 1118847783400.0, 1118847783500.0, 1118847783600.0, 1118847783700.0, 1118847783800.0, 1118847783900.0, 1118847784000.0, 1118847784100.0, 1118847784200.0, 1118847784300.0, 1118847784400.0, 1118847784500.0, 1118847784600.0, 1118847784700.0, 1118847784800.0, 1118847784900.0, 1118847785000.0, 1118847785100.0, 1118847785200.0, 1118847785300.0, 1118847785400.0, 1118847785500.0, 1118847785600.0, 1118847785700.0, 1118847785800.0, 1118847785900.0, 1118847786000.0, 1118847786100.0, 1118847786200.0, 1118847786300.0, 1118847786400.0, 1118847786500.0, 1118847786600.0, 1118847786700.0, 1118847786800.0, 1118847786900.0, 1118847787000.0, 1118847787100.0, 1118847787200.0, 1118847787300.0, 1118847787400.0, 1118847787500.0, 1118847787600.0, 1118847787700.0, 1118847787800.0, 1118847787900.0, 1118847788000.0, 1118847788100.0, 1118847788200.0, 1118847788300.0, 1118847788400.0, 1118847788500.0, 1118847788600.0, 1118847788700.0, 1118847788800.0, 1118847788900.0, 1118847789000.0, 1118847789100.0, 1118847789200.0, 1118847789300.0, 1118847789400.0, 1118847789500.0, 1118847789600.0, 1118847789700.0, 1118847789800.0, 1118847789900.0, 1118847790000.0, 1118847790100.0, 1118847790200.0, 1118847790300.0, 1118847790400.0, 1118847790500.0, 1118847790600.0, 1118847790700.0, 1118847790800.0, 1118847790900.0, 1118847791000.0, 1118847791100.0, 1118847791200.0, 1118847791300.0, 1118847791400.0, 1118847791500.0, 1118847791600.0, 1118847791700.0, 1118847791800.0, 1118847791900.0, 1118847792000.0, 1118847792100.0, 1118847792200.0, 1118847792300.0, 1118847792400.0, 1118847792500.0, 1118847792600.0, 1118847792700.0, 1118847792800.0, 1118847792900.0, 1118847793000.0, 1118847793100.0, 1118847793200.0, 1118847793300.0, 1118847793400.0, 1118847793500.0, 1118847793600.0, 1118847793700.0, 1118847793800.0, 1118847793900.0, 1118847794000.0, 1118847794100.0, 1118847794200.0, 1118847794300.0, 1118847794400.0, 1118847794500.0, 1118847794600.0, 1118847794700.0, 1118847794800.0, 1118847794900.0, 1118847795000.0, 1118847795100.0, 1118847795200.0, 1118847795300.0, 1118847795400.0, 1118847795500.0, 1118847795600.0, 1118847795700.0, 1118847795800.0, 1118847795900.0, 1118847796000.0, 1118847796100.0, 1118847796200.0, 1118847796300.0, 1118847796400.0, 1118847796500.0, 1118847796600.0, 1118847796700.0, 1118847796800.0, 1118847796900.0, 1118847797000.0, 1118847797100.0, 1118847797200.0, 1118847797300.0, 1118847797400.0, 1118847797500.0, 1118847797600.0, 1118847797700.0, 1118847797800.0, 1118847797900.0, 1118847798000.0, 1118847798100.0, 1118847798200.0, 1118847798300.0, 1118847798400.0, 1118847798500.0, 1118847798600.0, 1118847798700.0, 1118847798800.0, 1118847798900.0, 1118847799000.0, 1118847799100.0, 1118847799200.0, 1118847799300.0, 1118847799400.0, 1118847799500.0, 1118847799600.0, 1118847799700.0, 1118847799800.0, 1118847799900.0, 1118847800000.0, 1118847800100.0, 1118847800200.0, 1118847800300.0, 1118847800400.0, 1118847800500.0, 1118847800600.0, 1118847800700.0, 1118847800800.0, 1118847800900.0, 1118847801000.0, 1118847801100.0, 1118847801200.0, 1118847801300.0, 1118847801400.0, 1118847801500.0, 1118847801600.0, 1118847801700.0, 1118847801800.0, 1118847801900.0, 1118847802000.0, 1118847802100.0, 1118847802200.0, 1118847802300.0, 1118847802400.0, 1118847802500.0, 1118847802600.0, 1118847802700.0, 1118847802800.0, 1118847802900.0, 1118847803000.0, 1118847803100.0, 1118847803200.0, 1118847803300.0, 1118847803400.0, 1118847803500.0, 1118847803600.0, 1118847803700.0, 1118847803800.0, 1118847803900.0, 1118847804000.0, 1118847804100.0, 1118847804200.0, 1118847804300.0, 1118847804400.0, 1118847804500.0, 1118847804600.0, 1118847804700.0, 1118847804800.0, 1118847804900.0, 1118847805000.0, 1118847805100.0, 1118847805200.0, 1118847805300.0, 1118847805400.0, 1118847805500.0, 1118847805600.0, 1118847805700.0, 1118847805800.0, 1118847805900.0, 1118847806000.0, 1118847806100.0, 1118847806200.0, 1118847806300.0, 1118847806400.0, 1118847806500.0, 1118847806600.0, 1118847806700.0, 1118847806800.0, 1118847806900.0, 1118847807000.0, 1118847807100.0, 1118847807200.0, 1118847807300.0, 1118847807400.0, 1118847807500.0, 1118847807600.0, 1118847807700.0, 1118847807800.0, 1118847807900.0, 1118847808000.0, 1118847808100.0, 1118847808200.0, 1118847808300.0, 1118847808400.0, 1118847808500.0, 1118847808600.0, 1118847808700.0, 1118847808800.0, 1118847808900.0, 1118847809000.0, 1118847809100.0, 1118847809200.0, 1118847809300.0, 1118847809400.0, 1118847809500.0, 1118847809600.0, 1118847809700.0, 1118847809800.0, 1118847809900.0, 1118847810000.0, 1118847810100.0, 1118847810200.0, 1118847810300.0, 1118847810400.0, 1118847810500.0, 1118847810600.0, 1118847810700.0, 1118847810800.0, 1118847810900.0, 1118847811000.0, 1118847811100.0, 1118847811200.0, 1118847811300.0, 1118847811400.0, 1118847811500.0, 1118847811600.0, 1118847811700.0, 1118847811800.0, 1118847811900.0, 1118847812000.0, 1118847812100.0, 1118847812200.0, 1118847812300.0, 1118847812400.0, 1118847812500.0, 1118847812600.0, 1118847812700.0, 1118847812800.0, 1118847812900.0, 1118847813000.0, 1118847813100.0, 1118847813200.0, 1118847813300.0, 1118847813400.0, 1118847813500.0, 1118847813600.0, 1118847813700.0, 1118847813800.0, 1118847813900.0, 1118847814000.0, 1118847814100.0, 1118847814200.0, 1118847814300.0, 1118847814400.0, 1118847814500.0, 1118847814600.0, 1118847814700.0, 1118847814800.0, 1118847814900.0, 1118847815000.0, 1118847815100.0, 1118847815200.0, 1118847815300.0, 1118847815400.0, 1118847815500.0, 1118847815600.0, 1118847815700.0, 1118847815800.0, 1118847815900.0, 1118847816000.0, 1118847816100.0, 1118847816200.0, 1118847816300.0, 1118847816400.0, 1118847816500.0, 1118847816600.0, 1118847816700.0, 1118847816800.0, 1118847816900.0, 1118847817000.0, 1118847817100.0, 1118847817200.0, 1118847817300.0, 1118847817400.0, 1118847817500.0, 1118847817600.0, 1118847817700.0, 1118847817800.0, 1118847817900.0, 1118847818000.0, 1118847818100.0, 1118847818200.0, 1118847818300.0, 1118847818400.0, 1118847818500.0, 1118847818600.0, 1118847818700.0, 1118847818800.0, 1118847818900.0, 1118847819000.0, 1118847819100.0, 1118847819200.0, 1118847819300.0, 1118847819400.0, 1118847819500.0, 1118847819600.0, 1118847819700.0, 1118847819800.0, 1118847819900.0, 1118847820000.0, 1118847820100.0, 1118847820200.0, 1118847820300.0, 1118847820400.0, 1118847820500.0, 1118847820600.0, 1118847820700.0, 1118847820800.0, 1118847820900.0, 1118847821000.0, 1118847821100.0, 1118847821200.0, 1118847821300.0, 1118847821400.0, 1118847821500.0, 1118847821600.0, 1118847821700.0, 1118847821800.0, 1118847821900.0, 1118847822000.0, 1118847822100.0, 1118847822200.0, 1118847822300.0, 1118847822400.0, 1118847822500.0, 1118847822600.0, 1118847822700.0, 1118847822800.0, 1118847822900.0, 1118847823000.0, 1118847823100.0, 1118847823200.0, 1118847823300.0, 1118847823400.0, 1118847823500.0, 1118847823600.0, 1118847823700.0, 1118847823800.0, 1118847823900.0, 1118847824000.0, 1118847824100.0, 1118847824200.0, 1118847824300.0, 1118847824400.0, 1118847824500.0, 1118847824600.0, 1118847824700.0, 1118847824800.0, 1118847824900.0, 1118847825000.0, 1118847825100.0, 1118847825200.0, 1118847825300.0, 1118847825400.0, 1118847825500.0, 1118847825600.0, 1118847825700.0, 1118847825800.0, 1118847825900.0, 1118847826000.0, 1118847826100.0, 1118847826200.0, 1118847826300.0, 1118847826400.0, 1118847826500.0, 1118847826600.0, 1118847826700.0, 1118847826800.0, 1118847826900.0, 1118847827000.0, 1118847827100.0, 1118847827200.0, 1118847827300.0, 1118847827400.0, 1118847827500.0, 1118847827600.0, 1118847827700.0, 1118847827800.0, 1118847827900.0, 1118847828000.0, 1118847828100.0, 1118847828200.0, 1118847828300.0, 1118847828400.0, 1118847828500.0, 1118847828600.0, 1118847828700.0, 1118847828800.0, 1118847828900.0, 1118847829000.0, 1118847829100.0, 1118847829200.0, 1118847829300.0, 1118847829400.0, 1118847829500.0, 1118847829600.0, 1118847829700.0, 1118847829800.0, 1118847829900.0, 1118847830000.0, 1118847830100.0, 1118847830200.0, 1118847830300.0, 1118847830400.0, 1118847830500.0, 1118847830600.0, 1118847830700.0, 1118847830800.0, 1118847830900.0, 1118847831000.0, 1118847831100.0, 1118847831200.0, 1118847831300.0, 1118847831400.0, 1118847831500.0, 1118847831600.0, 1118847831700.0, 1118847831800.0, 1118847831900.0, 1118847832000.0, 1118847832100.0, 1118847832200.0, 1118847832300.0, 1118847832400.0, 1118847832500.0, 1118847832600.0, 1118847832700.0, 1118847832800.0, 1118847832900.0, 1118847833000.0, 1118847833100.0, 1118847833200.0, 1118847833300.0, 1118847833400.0, 1118847833500.0, 1118847833600.0, 1118847833700.0, 1118847833800.0, 1118847833900.0, 1118847834000.0, 1118847834100.0, 1118847834200.0, 1118847834300.0, 1118847834400.0, 1118847834500.0, 1118847834600.0, 1118847834700.0, 1118847834800.0, 1118847834900.0, 1118847835000.0, 1118847835100.0, 1118847835200.0, 1118847835300.0, 1118847835400.0, 1118847835500.0, 1118847835600.0, 1118847835700.0, 1118847835800.0, 1118847835900.0, 1118847836000.0, 1118847836100.0, 1118847836200.0, 1118847836300.0, 1118847836400.0, 1118847836500.0, 1118847836600.0, 1118847836700.0, 1118847836800.0, 1118847836900.0, 1118847837000.0, 1118847837100.0, 1118847837200.0, 1118847837300.0, 1118847837400.0, 1118847837500.0, 1118847837600.0, 1118847837700.0, 1118847837800.0, 1118847837900.0, 1118847838000.0, 1118847838100.0, 1118847838200.0, 1118847838300.0, 1118847838400.0, 1118847838500.0, 1118847838600.0, 1118847838700.0, 1118847838800.0, 1118847838900.0, 1118847839000.0, 1118847839100.0, 1118847839200.0, 1118847839300.0, 1118847839400.0, 1118847839500.0, 1118847839600.0, 1118847839700.0, 1118847839800.0, 1118847839900.0, 1118847840000.0, 1118847840100.0, 1118847840200.0, 1118847840300.0, 1118847840400.0, 1118847840500.0, 1118847840600.0, 1118847840700.0, 1118847840800.0, 1118847840900.0, 1118847841000.0, 1118847841100.0, 1118847841200.0, 1118847841300.0, 1118847841400.0, 1118847841500.0, 1118847841600.0, 1118847841700.0, 1118847841800.0, 1118847841900.0, 1118847842000.0, 1118847842100.0, 1118847842200.0, 1118847842300.0, 1118847842400.0, 1118847842500.0, 1118847842600.0, 1118847842700.0, 1118847842800.0, 1118847842900.0, 1118847843000.0, 1118847843100.0, 1118847843200.0, 1118847843300.0, 1118847843400.0, 1118847843500.0, 1118847843600.0, 1118847843700.0, 1118847843800.0, 1118847843900.0, 1118847844000.0, 1118847844100.0, 1118847844200.0, 1118847844300.0, 1118847844400.0, 1118847844500.0, 1118847844600.0, 1118847844700.0, 1118847844800.0, 1118847844900.0, 1118847845000.0, 1118847845100.0, 1118847845200.0, 1118847845300.0, 1118847845400.0, 1118847845500.0, 1118847845600.0, 1118847845700.0, 1118847845800.0, 1118847845900.0, 1118847846000.0, 1118847846100.0, 1118847846200.0, 1118847846300.0, 1118847846400.0, 1118847846500.0, 1118847846600.0, 1118847846700.0, 1118847846800.0, 1118847846900.0, 1118847847000.0, 1118847847100.0, 1118847847200.0, 1118847847300.0, 1118847847400.0, 1118847847500.0, 1118847847600.0, 1118847847700.0, 1118847847800.0, 1118847847900.0, 1118847848000.0, 1118847848100.0, 1118847848200.0, 1118847848300.0, 1118847848400.0, 1118847848500.0, 1118847848600.0, 1118847848700.0, 1118847848800.0, 1118847848900.0, 1118847849000.0, 1118847849100.0, 1118847849200.0, 1118847849300.0, 1118847849400.0, 1118847849500.0, 1118847849600.0, 1118847849700.0, 1118847849800.0, 1118847849900.0, 1118847850000.0, 1118847850100.0, 1118847850200.0, 1118847850300.0, 1118847850400.0, 1118847850500.0, 1118847850600.0, 1118847850700.0, 1118847850800.0, 1118847850900.0, 1118847851000.0, 1118847851100.0, 1118847851200.0, 1118847851300.0, 1118847851400.0, 1118847851500.0, 1118847851600.0, 1118847851700.0, 1118847851800.0, 1118847851900.0, 1118847852000.0, 1118847852100.0, 1118847852200.0, 1118847852300.0, 1118847852400.0, 1118847852500.0, 1118847852600.0, 1118847852700.0, 1118847852800.0, 1118847852900.0, 1118847853000.0, 1118847853100.0, 1118847853200.0, 1118847853300.0, 1118847853400.0, 1118847853500.0, 1118847853600.0, 1118847853700.0, 1118847853800.0, 1118847853900.0, 1118847854000.0, 1118847854100.0, 1118847854200.0, 1118847854300.0, 1118847854400.0, 1118847854500.0, 1118847854600.0, 1118847854700.0, 1118847854800.0, 1118847854900.0, 1118847855000.0, 1118847855100.0, 1118847855200.0, 1118847855300.0, 1118847855400.0, 1118847855500.0, 1118847855600.0, 1118847855700.0, 1118847855800.0, 1118847855900.0, 1118847856000.0, 1118847856100.0, 1118847856200.0, 1118847856300.0, 1118847856400.0, 1118847856500.0, 1118847856600.0, 1118847856700.0, 1118847856800.0, 1118847856900.0, 1118847857000.0, 1118847857100.0, 1118847857200.0, 1118847857300.0, 1118847857400.0, 1118847857500.0, 1118847857600.0, 1118847857700.0, 1118847857800.0, 1118847857900.0, 1118847858000.0, 1118847858100.0, 1118847858200.0, 1118847858300.0, 1118847858400.0, 1118847858500.0, 1118847858600.0, 1118847858700.0, 1118847858800.0, 1118847858900.0, 1118847859000.0, 1118847859100.0, 1118847859200.0, 1118847859300.0, 1118847859400.0, 1118847859500.0, 1118847859600.0, 1118847859700.0, 1118847859800.0, 1118847859900.0, 1118847860000.0, 1118847860100.0, 1118847860200.0, 1118847860300.0, 1118847860400.0, 1118847860500.0, 1118847860600.0, 1118847860700.0, 1118847860800.0, 1118847860900.0, 1118847861000.0, 1118847861100.0, 1118847861200.0, 1118847861300.0, 1118847861400.0, 1118847861500.0, 1118847861600.0, 1118847861700.0, 1118847861800.0, 1118847861900.0, 1118847862000.0, 1118847862100.0, 1118847862200.0, 1118847862300.0, 1118847862400.0, 1118847862500.0, 1118847862600.0, 1118847862700.0, 1118847862800.0, 1118847862900.0, 1118847863000.0, 1118847863100.0, 1118847863200.0, 1118847863300.0, 1118847863400.0, 1118847863500.0, 1118847863600.0, 1118847863700.0, 1118847863800.0, 1118847863900.0, 1118847864000.0, 1118847864100.0, 1118847864200.0, 1118847864300.0, 1118847864400.0, 1118847864500.0, 1118847864600.0, 1118847864700.0, 1118847864800.0, 1118847864900.0, 1118847865000.0, 1118847865100.0, 1118847865200.0, 1118847865300.0, 1118847865400.0, 1118847865500.0, 1118847865600.0, 1118847865700.0, 1118847865800.0, 1118847865900.0, 1118847866000.0, 1118847866100.0, 1118847866200.0, 1118847866300.0, 1118847866400.0, 1118847866500.0, 1118847866600.0, 1118847866700.0, 1118847866800.0, 1118847866900.0, 1118847867000.0, 1118847867100.0, 1118847867200.0, 1118847867300.0, 1118847867400.0, 1118847867500.0, 1118847867600.0, 1118847867700.0, 1118847867800.0, 1118847867900.0, 1118847868000.0, 1118847868100.0, 1118847868200.0, 1118847868300.0, 1118847868400.0, 1118847868500.0, 1118847868600.0, 1118847868700.0, 1118847868800.0, 1118847868900.0, 1118847869000.0, 1118847869100.0, 1118847869200.0, 1118847869300.0, 1118847869400.0, 1118847869500.0, 1118847869600.0, 1118847869700.0, 1118847869800.0, 1118847869900.0, 1118847870000.0, 1118847870100.0, 1118847870200.0, 1118847870300.0, 1118847870400.0, 1118847870500.0, 1118847870600.0, 1118847870700.0, 1118847870800.0, 1118847870900.0, 1118847871000.0, 1118847871100.0, 1118847871200.0, 1118847871300.0, 1118847871400.0, 1118847871500.0, 1118847871600.0, 1118847871700.0, 1118847871800.0, 1118847871900.0, 1118847872000.0, 1118847872100.0, 1118847872200.0, 1118847872300.0, 1118847872400.0, 1118847872500.0, 1118847872600.0, 1118847872700.0, 1118847872800.0, 1118847872900.0, 1118847873000.0, 1118847873100.0, 1118847873200.0, 1118847873300.0, 1118847873400.0, 1118847873500.0, 1118847873600.0, 1118847873700.0, 1118847873800.0, 1118847873900.0, 1118847874000.0, 1118847874100.0, 1118847874200.0, 1118847874300.0, 1118847874400.0, 1118847874500.0, 1118847874600.0, 1118847874700.0, 1118847874800.0, 1118847874900.0, 1118847875000.0, 1118847875100.0, 1118847875200.0, 1118847875300.0, 1118847875400.0, 1118847875500.0, 1118847875600.0, 1118847875700.0, 1118847875800.0, 1118847875900.0, 1118847876000.0, 1118847876100.0, 1118847876200.0, 1118847876300.0, 1118847876400.0, 1118847876500.0, 1118847876600.0, 1118847876700.0, 1118847876800.0, 1118847876900.0, 1118847877000.0, 1118847877100.0, 1118847877200.0, 1118847877300.0, 1118847877400.0, 1118847877500.0, 1118847877600.0, 1118847877700.0, 1118847877800.0, 1118847877900.0, 1118847878000.0, 1118847878100.0, 1118847878200.0, 1118847878300.0, 1118847878400.0, 1118847878500.0, 1118847878600.0, 1118847878700.0, 1118847878800.0, 1118847878900.0, 1118847879000.0, 1118847879100.0, 1118847879200.0, 1118847879300.0, 1118847879400.0, 1118847879500.0, 1118847879600.0, 1118847879700.0, 1118847879800.0, 1118847879900.0, 1118847880000.0, 1118847880100.0, 1118847880200.0, 1118847880300.0, 1118847880400.0, 1118847880500.0, 1118847880600.0, 1118847880700.0, 1118847880800.0, 1118847880900.0, 1118847881000.0, 1118847881100.0, 1118847881200.0, 1118847881300.0, 1118847881400.0, 1118847881500.0, 1118847881600.0, 1118847881700.0, 1118847881800.0, 1118847881900.0, 1118847882000.0, 1118847882100.0, 1118847882200.0, 1118847882300.0, 1118847882400.0, 1118847882500.0, 1118847882600.0, 1118847882700.0, 1118847882800.0, 1118847882900.0, 1118847883000.0, 1118847883100.0, 1118847883200.0, 1118847883300.0, 1118847883400.0, 1118847883500.0, 1118847883600.0, 1118847883700.0, 1118847883800.0, 1118847883900.0, 1118847884000.0, 1118847884100.0, 1118847884200.0, 1118847884300.0, 1118847884400.0, 1118847884500.0, 1118847884600.0, 1118847884700.0, 1118847884800.0, 1118847884900.0, 1118847885000.0, 1118847885100.0, 1118847885200.0, 1118847885300.0, 1118847885400.0, 1118847885500.0, 1118847885600.0, 1118847885700.0, 1118847885800.0, 1118847885900.0, 1118847886000.0, 1118847886100.0, 1118847886200.0, 1118847886300.0, 1118847886400.0, 1118847886500.0, 1118847886600.0, 1118847886700.0, 1118847886800.0, 1118847886900.0, 1118847887000.0, 1118847887100.0, 1118847887200.0, 1118847887300.0, 1118847887400.0, 1118847887500.0, 1118847887600.0, 1118847887700.0, 1118847887800.0, 1118847887900.0, 1118847888000.0, 1118847888100.0, 1118847888200.0, 1118847888300.0, 1118847888400.0, 1118847888500.0, 1118847888600.0, 1118847888700.0, 1118847888800.0, 1118847888900.0, 1118847889000.0, 1118847889100.0, 1118847889200.0, 1118847889300.0, 1118847889400.0, 1118847889500.0, 1118847889600.0, 1118847889700.0, 1118847889800.0, 1118847889900.0, 1118847890000.0, 1118847890100.0, 1118847890200.0, 1118847890300.0, 1118847890400.0, 1118847890500.0, 1118847890600.0, 1118847890700.0, 1118847890800.0, 1118847890900.0, 1118847891000.0, 1118847891100.0, 1118847891200.0, 1118847891300.0, 1118847891400.0, 1118847891500.0, 1118847891600.0, 1118847891700.0, 1118847891800.0, 1118847891900.0, 1118847892000.0, 1118847892100.0, 1118847892200.0, 1118847892300.0, 1118847892400.0, 1118847892500.0, 1118847892600.0, 1118847892700.0, 1118847892800.0, 1118847892900.0, 1118847893000.0, 1118847893100.0, 1118847893200.0, 1118847893300.0, 1118847893400.0, 1118847893500.0, 1118847893600.0, 1118847893700.0, 1118847893800.0, 1118847893900.0, 1118847894000.0, 1118847894100.0, 1118847894200.0, 1118847894300.0, 1118847894400.0, 1118847894500.0, 1118847894600.0, 1118847894700.0, 1118847894800.0, 1118847894900.0, 1118847895000.0, 1118847895100.0, 1118847895200.0, 1118847895300.0, 1118847895400.0, 1118847895500.0, 1118847895600.0, 1118847895700.0, 1118847895800.0, 1118847895900.0, 1118847896000.0, 1118847896100.0, 1118847896200.0, 1118847896300.0, 1118847896400.0, 1118847896500.0, 1118847896600.0, 1118847896700.0, 1118847896800.0, 1118847896900.0, 1118847897000.0, 1118847897100.0, 1118847897200.0, 1118847897300.0, 1118847897400.0, 1118847897500.0, 1118847897600.0, 1118847897700.0, 1118847897800.0, 1118847897900.0, 1118847898000.0, 1118847898100.0, 1118847898200.0, 1118847898300.0, 1118847898400.0, 1118847898500.0, 1118847898600.0, 1118847898700.0, 1118847898800.0, 1118847898900.0, 1118847899000.0, 1118847899100.0, 1118847899200.0, 1118847899300.0, 1118847899400.0, 1118847899500.0, 1118847899600.0, 1118847899700.0, 1118847899800.0, 1118847899900.0, 1118847900000.0, 1118847900100.0, 1118847900200.0, 1118847900300.0, 1118847900400.0, 1118847900500.0, 1118847900600.0, 1118847900700.0, 1118847900800.0, 1118847900900.0, 1118847901000.0, 1118847901100.0, 1118847901200.0, 1118847901300.0, 1118847901400.0, 1118847901500.0, 1118847901600.0, 1118847901700.0, 1118847901800.0, 1118847901900.0, 1118847902000.0, 1118847902100.0, 1118847902200.0, 1118847902300.0, 1118847902400.0, 1118847902500.0, 1118847902600.0, 1118847902700.0, 1118847902800.0, 1118847902900.0, 1118847903000.0, 1118847903100.0, 1118847903200.0, 1118847903300.0, 1118847903400.0, 1118847903500.0, 1118847903600.0, 1118847903700.0, 1118847903800.0, 1118847903900.0, 1118847904000.0, 1118847904100.0, 1118847904200.0, 1118847904300.0, 1118847904400.0, 1118847904500.0, 1118847904600.0, 1118847904700.0, 1118847904800.0, 1118847904900.0, 1118847905000.0, 1118847905100.0, 1118847905200.0, 1118847905300.0, 1118847905400.0, 1118847905500.0, 1118847905600.0, 1118847905700.0, 1118847905800.0, 1118847905900.0, 1118847906000.0, 1118847906100.0, 1118847906200.0, 1118847906300.0, 1118847906400.0, 1118847906500.0, 1118847906600.0, 1118847906700.0, 1118847906800.0, 1118847906900.0, 1118847907000.0, 1118847907100.0, 1118847907200.0, 1118847907300.0, 1118847907400.0, 1118847907500.0, 1118847907600.0, 1118847907700.0, 1118847907800.0, 1118847907900.0, 1118847908000.0, 1118847908100.0, 1118847908200.0, 1118847908300.0, 1118847908400.0, 1118847908500.0, 1118847908600.0, 1118847908700.0, 1118847908800.0, 1118847908900.0, 1118847909000.0, 1118847909100.0, 1118847909200.0, 1118847909300.0, 1118847909400.0, 1118847909500.0, 1118847909600.0, 1118847909700.0, 1118847909800.0, 1118847909900.0, 1118847910000.0, 1118847910100.0, 1118847910200.0, 1118847910300.0, 1118847910400.0, 1118847910500.0, 1118847910600.0, 1118847910700.0, 1118847910800.0, 1118847910900.0, 1118847911000.0, 1118847911100.0, 1118847911200.0, 1118847911300.0, 1118847911400.0, 1118847911500.0, 1118847911600.0, 1118847911700.0, 1118847911800.0, 1118847911900.0, 1118847912000.0, 1118847912100.0, 1118847912200.0, 1118847912300.0, 1118847912400.0, 1118847912500.0, 1118847912600.0, 1118847912700.0, 1118847912800.0, 1118847912900.0, 1118847913000.0, 1118847913100.0, 1118847913200.0, 1118847913300.0, 1118847913400.0, 1118847913500.0, 1118847913600.0, 1118847913700.0, 1118847913800.0, 1118847913900.0, 1118847914000.0, 1118847914100.0, 1118847914200.0, 1118847914300.0, 1118847914400.0, 1118847914500.0, 1118847914600.0, 1118847914700.0, 1118847914800.0, 1118847914900.0, 1118847915000.0, 1118847915100.0, 1118847915200.0, 1118847915300.0, 1118847915400.0, 1118847915500.0, 1118847915600.0, 1118847915700.0, 1118847915800.0, 1118847915900.0, 1118847916000.0, 1118847916100.0, 1118847916200.0, 1118847916300.0, 1118847916400.0, 1118847916500.0, 1118847916600.0, 1118847916700.0, 1118847916800.0, 1118847916900.0, 1118847917000.0, 1118847917100.0, 1118847917200.0, 1118847917300.0, 1118847917400.0, 1118847917500.0, 1118847917600.0, 1118847917700.0, 1118847917800.0, 1118847917900.0, 1118847918000.0, 1118847918100.0, 1118847918200.0, 1118847918300.0, 1118847918400.0, 1118847918500.0, 1118847918600.0, 1118847918700.0, 1118847918800.0, 1118847918900.0, 1118847919000.0, 1118847919100.0, 1118847919200.0, 1118847919300.0, 1118847919400.0, 1118847919500.0, 1118847919600.0, 1118847919700.0, 1118847919800.0, 1118847919900.0, 1118847920000.0, 1118847920100.0, 1118847920200.0, 1118847920300.0, 1118847920400.0, 1118847920500.0, 1118847920600.0, 1118847920700.0, 1118847920800.0, 1118847920900.0, 1118847921000.0, 1118847921100.0, 1118847921200.0, 1118847921300.0, 1118847921400.0, 1118847921500.0, 1118847921600.0, 1118847921700.0, 1118847921800.0, 1118847921900.0, 1118847922000.0, 1118847922100.0, 1118847922200.0, 1118847922300.0, 1118847922400.0, 1118847922500.0, 1118847922600.0, 1118847922700.0, 1118847922800.0, 1118847922900.0, 1118847923000.0, 1118847923100.0, 1118847923200.0, 1118847923300.0, 1118847923400.0, 1118847923500.0, 1118847923600.0, 1118847923700.0, 1118847923800.0, 1118847923900.0, 1118847924000.0, 1118847924100.0, 1118847924200.0, 1118847924300.0, 1118847924400.0, 1118847924500.0, 1118847924600.0, 1118847924700.0, 1118847924800.0, 1118847924900.0, 1118847925000.0, 1118847925100.0, 1118847925200.0, 1118847925300.0, 1118847925400.0, 1118847925500.0, 1118847925600.0, 1118847925700.0, 1118847925800.0, 1118847925900.0, 1118847926000.0, 1118847926100.0, 1118847926200.0, 1118847926300.0, 1118847926400.0, 1118847926500.0, 1118847926600.0, 1118847926700.0, 1118847926800.0, 1118847926900.0, 1118847927000.0, 1118847927100.0, 1118847927200.0, 1118847927300.0, 1118847927400.0, 1118847927500.0, 1118847927600.0, 1118847927700.0, 1118847927800.0, 1118847927900.0, 1118847928000.0, 1118847928100.0, 1118847928200.0, 1118847928300.0, 1118847928400.0, 1118847928500.0, 1118847928600.0, 1118847928700.0, 1118847928800.0, 1118847928900.0, 1118847929000.0, 1118847929100.0, 1118847929200.0, 1118847929300.0, 1118847929400.0, 1118847929500.0, 1118847929600.0, 1118847929700.0, 1118847929800.0, 1118847929900.0, 1118847930000.0, 1118847930100.0, 1118847930200.0, 1118847930300.0, 1118847930400.0, 1118847930500.0, 1118847930600.0, 1118847930700.0, 1118847930800.0, 1118847930900.0, 1118847931000.0, 1118847931100.0, 1118847931200.0, 1118847931300.0, 1118847931400.0, 1118847931500.0, 1118847931600.0, 1118847931700.0, 1118847931800.0, 1118847931900.0, 1118847932000.0, 1118847932100.0, 1118847932200.0, 1118847932300.0, 1118847932400.0, 1118847932500.0])\n"
     ]
    }
   ],
   "source": [
    "def getValueByLable(lableList,valueList):\n",
    "    \"\"\"\n",
    "    For instance, given a lable list ['Local_X','Local_Y'] and a value list [2.0, 24.0, 437.0, 1118846981300.0, 16.254, \n",
    "    79.349, 6451167.199, 1873312.382, 14.5, 4.9, 2.0, 39.14, -5.73, 2.0, 0.0, 13.0, 0.0, 0.0] which values sorted by the \n",
    "    order of allLableList below, the function return a value Dict {'Local_X':16.254, 'Local_Y':79.349}\n",
    "    Args:\n",
    "        lableList: the list of lables you've required, such as['Vehicle_ID', 'Total_Frames','Global_Time']\n",
    "        valueList: the list contains all legally value, sorted by:['Vehicle_ID', 'Frame_ID','Total_Frames','Global_Time','Local_X','Local_Y','Global_X','Global_Y',\\\n",
    "                      'v_Length','v_Width','v_Class','v_Vel','v_Acc','Lane_ID','Preceding','Following','Space_Headway',\\\n",
    "                      'Time_Headway']\n",
    "    Returns: \n",
    "        value list in the as the order of lableList\n",
    "    For instance, given a lable list ['Local_X','Local_Y'] and a value list [2.0, 24.0, 437.0, 1118846981300.0, 16.254, \n",
    "    79.349, 6451167.199, 1873312.382, 14.5, 4.9, 2.0, 39.14, -5.73, 2.0, 0.0, 13.0, 0.0, 0.0] which values sorted by the \n",
    "    order of allLableList above, the function return a value List [16.254, 79.349]\n",
    "\n",
    "    \"\"\"\n",
    "    allLableList=['Vehicle_ID', 'Frame_ID','Total_Frames','Global_Time','Local_X','Local_Y','Global_X','Global_Y',\\\n",
    "                  'v_Length','v_Width','v_Class','v_Vel','v_Acc','Lane_ID','Preceding','Following','Space_Headway',\\\n",
    "                  'Time_Headway']\n",
    "    valueDictReturn={}\n",
    "    for lableItem in lableList:\n",
    "        valueDictReturn[lableItem]=valueList[allLableList.index(lableItem)]\n",
    "    return valueDictReturn\n",
    "\n",
    "def rearrangeDataByGlobalTime(allValueLists):\n",
    "    '''\n",
    "    Args:\n",
    "        allValueLists: all values have been read from a txt file which have already been converted to a list\n",
    "    Returns:\n",
    "        dict have been arranged by global time. One single global time generally contains several value lists.\n",
    "    '''\n",
    "    valueDict={}\n",
    "    for valueList in allValueLists:\n",
    "        dictKey=getValueByLable(['Global_Time'],valueList)['Global_Time']\n",
    "        if dictKey in valueDict:\n",
    "            # if dictKey already there, then add valueList to the list of the key\n",
    "            valueDict[dictKey].append(valueList)\n",
    "        else:\n",
    "            #else, create a list and append valueList on it\n",
    "            valueDict[dictKey]=[valueList]\n",
    "    return valueDict\n",
    "\n",
    "maxX=0\n",
    "minX=0\n",
    "maxY=0\n",
    "minY=0\n",
    "maxV=0\n",
    "minV=0\n",
    "maxA=0\n",
    "minA=99999999999999\n",
    "for eachLine in allLineList:\n",
    "    eachValueDict=getValueByLable(['Local_X','Local_Y','v_Vel','v_Acc'],eachLine)\n",
    "    if eachValueDict['Local_X']>maxX:\n",
    "        maxX=eachValueDict['Local_X']\n",
    "    if eachValueDict['Local_X']<minX:\n",
    "        minX=eachValueDict['Local_X']\n",
    "    if eachValueDict['Local_Y']>maxY:\n",
    "        maxY=eachValueDict['Local_Y']\n",
    "    if eachValueDict['Local_Y']<minY:\n",
    "        minY=eachValueDict['Local_Y']\n",
    "    if eachValueDict['v_Vel']>maxV:\n",
    "        maxV=eachValueDict['v_Vel']\n",
    "    if eachValueDict['v_Vel']<minV:\n",
    "        minV=eachValueDict['v_Vel']\n",
    "    if eachValueDict['v_Acc']>maxA:\n",
    "        maxA=eachValueDict['v_Acc']\n",
    "    if eachValueDict['v_Acc']<minA:\n",
    "        minA=eachValueDict['v_Acc']\n",
    "print(maxX,minX,maxY,minY,maxV,minV,maxA,minA)\n",
    "valueDict=rearrangeDataByGlobalTime(allLineList)\n",
    "print('done')\n",
    "print(valueDict.keys())\n",
    "#keys may not in order, sorting before use keys to index frame\n",
    "#keyslist=list(valueDict.keys()), keyslist.sort()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n",
      "9529\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# the maximum number of vehicles existing at one time\n",
    "theMax=0\n",
    "for i in valueDict.keys():\n",
    "    if theMax<valueDict[i].__len__():\n",
    "        theMax=valueDict[i].__len__()\n",
    "print(theMax)\n",
    "print(valueDict.keys().__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "testTensor=torch.zeros((2,100))\n",
    "print(testTensor[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxMatrixIndex=250\n",
    "\n",
    "#this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "matrixIndexAndVehicleIDRecordDict={}\n",
    "#initialize dict above\n",
    "for i in range(0,maxMatrixIndex):\n",
    "    matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "matrixIndexAndVehicleIDRecordDict['time']=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def readFirstFrame(matrixIndexAndVehicleIDRecordDictParam, valueLists):\n",
    "    \"\"\"\n",
    "    To generate the first set of tensors from the first frame\n",
    "    Args:\n",
    "        matrixIndexAndVehicleIDRecordDictParam: just as its name\n",
    "        valueLists: a list consists of all valuelist at one time\n",
    "    Returns:\n",
    "        several tensors arranged by: positionTensor, speedTensor, accTensor, angleTensor,newVehicleList(type:list)\n",
    "    \n",
    "    \"\"\"\n",
    "    maxMatrixIndex=matrixIndexAndVehicleIDRecordDictParam.keys().__len__()-1\n",
    "    #tensors initialize\n",
    "    positionTensor=torch.zeros(2,maxMatrixIndex)\n",
    "    speedTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    accTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    angleTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    newVehicleIDList=[]\n",
    "    curMatrixIndex=0\n",
    "    matrixIndexAndVehicleIDRecordDictParam['time']=getValueByLable([\"Global_Time\"],valueLists[0])['Global_Time']\n",
    "    #fill out all tensors\n",
    "    for eachValueList in valueLists:\n",
    "        #get values from eachValueList, generate dict\n",
    "        returnedEachValueDict=getValueByLable(['Vehicle_ID','Local_X','Local_Y','v_Vel','v_Acc'],eachValueList)\n",
    "        #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "        #angle Tensor assignment is not neeed for the initial value of each element in it is already zero\n",
    "        positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "        speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "        accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "        #then handle the record matrix\n",
    "        matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['Vehicle_ID']=returnedEachValueDict['Vehicle_ID']\n",
    "        matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['refresh']=0\n",
    "        newVehicleIDList.append(copy.deepcopy(returnedEachValueDict['Vehicle_ID']))\n",
    "        curMatrixIndex=curMatrixIndex+1\n",
    "    return positionTensor,speedTensor,accTensor,angleTensor,newVehicleIDList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test readFirstFrame function\n",
    "positionTensor,speedTensor,accTensor,angleTensor,newVehicleList=readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[1118847019700.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[1118847019700.0])\n",
    "import math\n",
    "\n",
    "def findMatrixIndexByVehicleID(matrixIndexAndVehicleIDRecordDictParam, vehicle_ID):\n",
    "    for i in range(0, len(matrixIndexAndVehicleIDRecordDictParam)-1):\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']==vehicle_ID:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def findEmptyMatrixIndex(matrixIndexAndVehicleIDRecordDictParam):\n",
    "    for i in range(0, len(matrixIndexAndVehicleIDRecordDictParam)-1):\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']==-1:\n",
    "            #Vehicle_ID=-1 when there is no existed vehicle ID bounding to the index\n",
    "            return i\n",
    "    raise Exception(\"NO EMPTY ELEMENT IN MATRIX\")\n",
    "\n",
    "def readGeneralFrame(matrixIndexAndVehicleIDRecordDictParam, valueLists, prePositionTensor):\n",
    "    \"\"\"\n",
    "    To generate the first set of tensors from the general frame that have a preceding one\n",
    "    Args:\n",
    "        matrixIndexAndVehicleIDRecordDictParam: just as its name\n",
    "        valueLists: a list consists of all valuelist at one time\n",
    "        prePositionTensor: positionTensor from the preceding frame, which is used to calculate angle tensor\n",
    "    Returns:\n",
    "        everal tensors arranged by: positionTensor, speedTensor, accTensor, angleTensor,newVehicleList(type:list),\n",
    "        vanishedVehicleList(type:list)\n",
    "    \n",
    "    \"\"\"\n",
    "    #tensors initialize\n",
    "    maxMatrixIndex=matrixIndexAndVehicleIDRecordDictParam.keys().__len__()-1\n",
    "    positionTensor=torch.zeros(2,maxMatrixIndex)\n",
    "    speedTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    accTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    angleTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    newVehicleIDList=[]\n",
    "    vanishedVehicleList=[]\n",
    "    curMatrixIndex=0\n",
    "    matrixIndexAndVehicleIDRecordDictParam['time']=getValueByLable([\"Global_Time\"],valueLists[0])['Global_Time']\n",
    "    #fill out all tensors\n",
    "    for eachValueList in valueLists:\n",
    "        #get values from eachValueList, generate dict\n",
    "        returnedEachValueDict=getValueByLable(['Vehicle_ID','Local_X','Local_Y','v_Vel','v_Acc'],eachValueList)\n",
    "        indexOfVehicle=findMatrixIndexByVehicleID(matrixIndexAndVehicleIDRecordDictParam,returnedEachValueDict['Vehicle_ID'])\n",
    "        if indexOfVehicle!=-1:\n",
    "        #index exist then the vehicle already existed in the preceded frame\n",
    "            matrixIndexAndVehicleIDRecordDictParam[indexOfVehicle]['refresh']=1\n",
    "            curMatrixIndex=indexOfVehicle\n",
    "            #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "            positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "            speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "            accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "            angleTensor[:,curMatrixIndex]=math.atan2(positionTensor[0,curMatrixIndex]-\\\n",
    "                                                     prePositionTensor[0,curMatrixIndex],\\\n",
    "                                                    positionTensor[1,curMatrixIndex]-prePositionTensor[1,curMatrixIndex])\n",
    "        else:\n",
    "        #a new vehicle ID\n",
    "            newVehicleIDList.append(copy.deepcopy(returnedEachValueDict['Vehicle_ID']))\n",
    "            curMatrixIndex=findEmptyMatrixIndex(matrixIndexAndVehicleIDRecordDictParam)\n",
    "            matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['Vehicle_ID']=copy.deepcopy(returnedEachValueDict['Vehicle_ID'])\n",
    "            matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['refresh']=1\n",
    "            #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "            positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "            speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "            accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "            angleTensor[:,curMatrixIndex]=math.atan2(positionTensor[0,curMatrixIndex]-\\\n",
    "                                                     prePositionTensor[0,curMatrixIndex],\\\n",
    "                                                    positionTensor[1,curMatrixIndex]-prePositionTensor[1,curMatrixIndex])\n",
    "    for i in range(0,maxMatrixIndex):\n",
    "    #find vanished vehicle and remove from dict\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['refresh']==0:\n",
    "            #if refresh=0 then the corresponding vehicle ID was not found in this frame\n",
    "            vanishedVehicleList.append(copy.deepcopy(matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']))\n",
    "            matrixIndexAndVehicleIDRecordDictParam[i]['refresh']=-1\n",
    "            matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']=-1\n",
    "    \n",
    "    for i in range(0,maxMatrixIndex):\n",
    "    #set all refrshed which equivalent to 1 to 0 to prepare for the next frame\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['refresh']==1:\n",
    "                #if refresh=0 then the corresponding vehicle ID was not found in this frame\n",
    "                matrixIndexAndVehicleIDRecordDictParam[i]['refresh']=0\n",
    "\n",
    "    return positionTensor,speedTensor,accTensor,angleTensor,newVehicleIDList,vanishedVehicleList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global_Time:  1118846979700.0 new Vehicle ID:  [5.0]\n"
     ]
    }
   ],
   "source": [
    "keys=list(valueDict.keys())\n",
    "keys.sort()\n",
    "\n",
    "#\n",
    "#this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "matrixIndexAndVehicleIDRecordDict={}\n",
    "#initialize dict above\n",
    "for i in range(0,maxMatrixIndex):\n",
    "    matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "positionTensor,speedTensor,accTensor,angleTensor,newVehicleList=readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[keys[0]])\n",
    "print('Global_Time: ',matrixIndexAndVehicleIDRecordDict['time'],'new Vehicle ID: ',newVehicleList)\n",
    "# for i in range(1,keys.__len__()):\n",
    "#     if(i%1000==0):\n",
    "#         print(i)\n",
    "#     positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "#     =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[keys[i]],positionTensor)\n",
    "#     if (newVehicleList.__len__()+vanishedVehicleList.__len__())>=1:\n",
    "#         print('Global_Time: ',matrixIndexAndVehicleIDRecordDict['time'],', new vehicle ID: ',newVehicleList,\\\n",
    "#          ', vanished vehicle ID: ',vanishedVehicleList)\n",
    "    \n",
    "    \n",
    "# #     print(matrixIndexAndVehicleIDRecordDict)\n",
    "\n",
    "\n",
    "# for i in range(1,keys.__len__()):\n",
    "#     distance=keys[i]-keys[i-1]\n",
    "#     if(distance!=100):\n",
    "#         print(keys[i],keys[i-1],distance)\n",
    "# for i in range(0,keys.__len__()):\n",
    "#     print(keys[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as utils\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromDirGenerateDict(trajectoryDir):\n",
    "    trajectoryDataFile=open('/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt')\n",
    "    count=0\n",
    "    allLineList=[]\n",
    "    count=0\n",
    "    for count,line in enumerate(trajectoryDataFile):\n",
    "        #read a single line, remove space and enter\n",
    "        lineList=line.split(' ')\n",
    "        try:\n",
    "            while True:\n",
    "                lineList.remove('')\n",
    "        except:\n",
    "            try:\n",
    "                lineList.remove('\\n')\n",
    "            except:\n",
    "                pass\n",
    "            pass\n",
    "        for i in range(0,lineList.__len__()):\n",
    "            # convert string to float\n",
    "            lineList[i]=float(lineList[i])\n",
    "        allLineList.append(lineList)\n",
    "    valueDict=rearrangeDataByGlobalTime(allLineList)\n",
    "    return valueDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 9, 999]\n",
      "[1, 2, 3, 9, 999]\n"
     ]
    }
   ],
   "source": [
    "list11=[1,2,3,4,5]\n",
    "a=list11\n",
    "b=a\n",
    "list11[4]=999\n",
    "a[3]=9\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensorsDataset(Dataset):\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=100,lableTensorEachBatch=10):\n",
    "        if(numberOfTensorsEachBatch<5):\n",
    "            raise Exception(\"THE NUMBER OF TENSORS IN EACH BATCH IS TOO SMALL\")\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the ture index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                 speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                 accTensor.mul(angleCosTensor)),0)\n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "            if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "            elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "            else:\n",
    "                allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "        return allCombineTensorTrain,allCombineTensorValid\n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n",
    "\n",
    "trajectorDataSet=tensorsDataset(trajectoryFileList)\n",
    "                                 \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.tensorsDataset"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trajectorDataSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLoader=DataLoader(trajectorDataSet,batch_size=4,shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, cell_size, hidden_size, output_last = True):\n",
    "        \"\"\"\n",
    "        cell_size is the size of cell_state.\n",
    "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, input, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((input, Hidden_State), 1)\n",
    "        f = F.sigmoid(self.fl(combined))\n",
    "        i = F.sigmoid(self.il(combined))\n",
    "        o = F.sigmoid(self.ol(combined))\n",
    "        C = F.tanh(self.Cl(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        time_step = inputs.size(1)\n",
    "        Hidden_State, Cell_State = self.initHidden(batch_size)\n",
    "        \n",
    "        if self.output_last:\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:]), Hidden_State, Cell_State)  \n",
    "            return Hidden_State\n",
    "        else:\n",
    "            outputs = None\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:]), Hidden_State, Cell_State)  \n",
    "                if outputs is None:\n",
    "                    outputs = Hidden_State.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "            return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        if use_gpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            return Hidden_State, Cell_State\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            return Hidden_State, Cell_State\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataIter=iter(dataLoader)\n",
    "dataInputs,dataValid=dataIter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100, 6, 250])\n",
      "torch.Size([4, 10, 6, 250])\n",
      "torch.Size([4, 100, 1500])\n",
      "tensor([[ 0.0000e+00,  9.1381e-13, -1.0000e+00,  ...,  0.0000e+00,\n",
      "          1.0000e+00, -1.0000e+00],\n",
      "        [ 0.0000e+00,  9.4221e-03, -2.1361e-33,  ...,  4.2708e-31,\n",
      "          1.0000e+00, -1.0557e-05],\n",
      "        [ 1.7275e-25,  7.6159e-01,  0.0000e+00,  ...,  5.7781e-18,\n",
      "          1.0000e+00, -1.0000e+00],\n",
      "        [ 4.5191e-07,  7.6159e-01, -9.7974e-01,  ...,  0.0000e+00,\n",
      "          1.0000e+00, -6.6325e-28]], device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "print(dataInputs.size())\n",
    "print(dataValid.size())\n",
    "viewedDataInputs=dataInputs.view(4,100,-1)\n",
    "viewedDataValid=dataValid.view(4,10,-1)\n",
    "viewedDataInputs=viewedDataInputs.cuda()\n",
    "viewedDataValid=viewedDataValid.cuda()\n",
    "print(viewedDataInputs.size())\n",
    "net=LSTM(1500,1500,1500).cuda()\n",
    "output=net(viewedDataInputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorNormalization(inputTensor,minValue,maxValue):\n",
    "    inputTensor.add_(-minValue).div_(maxValue-minValue)\n",
    "    \n",
    "def batchNormalizationForCombinedTensor(inputBatchedTensor,minX,maxX,minY,maxY,minV,maxV,minA,maxA):\n",
    "    tensorNormalization(inputBatchedTensor[:,:,0,:],minX,maxX)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,1,:],minY,maxY)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,2:3,:],minV,maxV)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,4:5,:],minA,maxA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15.2570,  7.4360, 51.8760, 36.9590, 14.2130,  5.4760, 52.2140, 27.4030,\n",
      "        44.5940, 51.4550,  5.3920, 55.2170, 40.9150, 16.3500,  8.7230, 37.7770,\n",
      "         9.6940, 28.8070,  8.0820, 53.1010,  0.0000, 41.6660,  4.6010, 30.5350,\n",
      "         8.4100, 19.1890,  7.7820, 41.4860, 51.8520, 22.1460,  9.6230, 19.2270,\n",
      "        30.2700, 52.1180, 29.9330, 18.1110, 40.9620, 20.8610,  7.8260, 30.8780,\n",
      "        28.3950, 41.8530, 51.4160,  7.7410, 18.7370, 51.7570, 33.5210, 20.6160,\n",
      "        21.8860, 30.7940, 38.7930, 41.5850, 20.4510,  6.7560, 54.1350, 32.3670,\n",
      "        32.2170, 16.3010, 55.6490, 53.2210, 28.5790,  5.8780, 43.5210, 20.4000,\n",
      "         7.6550, 51.4370, 40.1800, 19.1770, 31.1100, 41.6670, 41.5560, 20.4800,\n",
      "         8.0180, 31.6290, 17.9900,  8.3490, 64.1420, 51.7240, 41.0080, 20.3900,\n",
      "        29.6120, 51.6420,  6.2090, 17.1700, 42.2960, 53.8060, 32.6920, 64.7850,\n",
      "        19.8000,  8.3300, 40.5000, 19.6990,  8.2190, 30.0230, 55.2750, 20.8430,\n",
      "        53.4690, 42.0270, 29.7820,  8.9550, 20.3210, 40.8630, 29.5800, 18.2430,\n",
      "        17.2700, 40.1300, 54.3500, 28.4870, 18.6830, 29.4870, 40.3360, 50.8690,\n",
      "         7.7630, 28.2850, 52.8730,  7.2640, 53.5550, 30.3920, 18.9050,  4.6490,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000])\n",
      "tensor([0.2076, 0.1012, 0.7060, 0.5030, 0.1934, 0.0745, 0.7106, 0.3730, 0.6069,\n",
      "        0.7003, 0.0734, 0.7515, 0.5569, 0.2225, 0.1187, 0.5141, 0.1319, 0.3921,\n",
      "        0.1100, 0.7227, 0.0000, 0.5671, 0.0626, 0.4156, 0.1145, 0.2612, 0.1059,\n",
      "        0.5646, 0.7057, 0.3014, 0.1310, 0.2617, 0.4120, 0.7093, 0.4074, 0.2465,\n",
      "        0.5575, 0.2839, 0.1065, 0.4203, 0.3865, 0.5696, 0.6998, 0.1054, 0.2550,\n",
      "        0.7044, 0.4562, 0.2806, 0.2979, 0.4191, 0.5280, 0.5660, 0.2783, 0.0919,\n",
      "        0.7368, 0.4405, 0.4385, 0.2219, 0.7574, 0.7243, 0.3890, 0.0800, 0.5923,\n",
      "        0.2776, 0.1042, 0.7001, 0.5469, 0.2610, 0.4234, 0.5671, 0.5656, 0.2787,\n",
      "        0.1091, 0.4305, 0.2448, 0.1136, 0.8730, 0.7040, 0.5581, 0.2775, 0.4030,\n",
      "        0.7029, 0.0845, 0.2337, 0.5757, 0.7323, 0.4449, 0.8817, 0.2695, 0.1134,\n",
      "        0.5512, 0.2681, 0.1119, 0.4086, 0.7523, 0.2837, 0.7277, 0.5720, 0.4053,\n",
      "        0.1219, 0.2766, 0.5561, 0.4026, 0.2483, 0.2350, 0.5462, 0.7397, 0.3877,\n",
      "        0.2543, 0.4013, 0.5490, 0.6923, 0.1057, 0.3850, 0.7196, 0.0989, 0.7289,\n",
      "        0.4136, 0.2573, 0.0633, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "torch.Size([250])\n"
     ]
    }
   ],
   "source": [
    "dataInputs,dataValid=dataIter.next()\n",
    "print(dataInputs[0,70,0,:])\n",
    "batchNormalizationForCombinedTensor(dataInputs,minX,maxX,minY,maxY,minV,maxV,minA,maxA)\n",
    "print(dataInputs[0,70,0,:])\n",
    "print(dataInputs[0][0][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100, 2, 250])\n",
      "batchI 0\n",
      "timeStepI: 0\n",
      "timeStepI: 1\n",
      "timeStepI: 2\n",
      "timeStepI: 3\n",
      "timeStepI: 4\n",
      "timeStepI: 5\n",
      "timeStepI: 6\n",
      "timeStepI: 7\n",
      "timeStepI: 8\n",
      "timeStepI: 9\n",
      "timeStepI: 10\n",
      "timeStepI: 11\n",
      "timeStepI: 12\n",
      "timeStepI: 13\n",
      "timeStepI: 14\n",
      "timeStepI: 15\n",
      "timeStepI: 16\n",
      "timeStepI: 17\n",
      "timeStepI: 18\n",
      "timeStepI: 19\n",
      "timeStepI: 20\n",
      "timeStepI: 21\n",
      "timeStepI: 22\n",
      "timeStepI: 23\n",
      "timeStepI: 24\n",
      "timeStepI: 25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-07cc572df519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mmatrixSequenceInBatchDim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrixSequenceInBatchDim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmatrixSequenceInTimeStepDim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatrixSequenceInBatchDim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mgenerateAdjacencyMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataInputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-07cc572df519>\u001b[0m in \u001b[0;36mgenerateAdjacencyMatrix\u001b[0;34m(batchedPositionTensor, lambdaX, lambdaY, omegaX, omegaY, m)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#                     (omegaY/math.exp(lambdaX*abs(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])))*\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#                     (omegaX/math.exp(lambdaY*abs(batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchedPositionTensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatchI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeStepI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbatchedPositionTensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatchI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeStepI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                         \u001b[0;31m#if i follows j, then multiple m, m<1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                         \u001b[0madjacencyMatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madjacencyMatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def generateAdjacencyMatrix(batchedPositionTensor,lambdaX,lambdaY,omegaX,omegaY,m):\n",
    "    \"\"\"\n",
    "    Using batched position tensor generate batched adjacency matrix\n",
    "    Args:\n",
    "        batchedPositionTensor: a batch of position tensor, which size in (batch, timeSequence,2,vehicles), the \n",
    "        value 2 in dim=2 is the position of x and y. \n",
    "        lambda1,lambda2,omega1,omega2,m are parameters of the function. m<1\n",
    "        see detail in my notebook\n",
    "    Returns:\n",
    "        a batch of adjacency matrix\n",
    "    Example:\n",
    "        if given a batch of combined tensor, named theTensor, which size as below:\n",
    "            (4,100,6,250)\n",
    "        which means 4 batches, 100 time step, 6 dimension which respectively of positonx, positony, velocityx, \n",
    "        velocityy, accx,accy.\n",
    "        then we apply the function in such way:\n",
    "        generateAdjacencyMatrix(theTensor(:,:,0:1,:))\n",
    "    \"\"\"\n",
    "    print(batchedPositionTensor.size())\n",
    "    sizeOfEachMatrix=batchedPositionTensor[0,0,0,:].size()[0]\n",
    "    batchedPositionTensor\n",
    "    for batchI in range(batchedPositionTensor.size()[0]): #revolve each batch\n",
    "        print('batchI',batchI)\n",
    "        for timeStepI in range(batchedPositionTensor.size()[1]):#revolve each time step\n",
    "            print('timeStepI:',timeStepI)\n",
    "            adjacencyMatrix=torch.zeros((sizeOfEachMatrix,sizeOfEachMatrix))\n",
    "            for i in range(sizeOfEachMatrix):\n",
    "                for j in range(sizeOfEachMatrix):\n",
    "#                     adjacencyMatrix[i,j]=1\n",
    "                    #calculate original element with linear function\n",
    "                    adjacencyMatrix[i,j]=abs(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])*\\\n",
    "                        abs(batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])\n",
    "#                     (omegaY/math.exp(lambdaX*abs(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])))*\\\n",
    "#                     (omegaX/math.exp(lambdaY*abs(batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])))\n",
    "                    \n",
    "                    #calculate original element with expenential\n",
    "#                     adjacencyMatrix[i,j]=\n",
    "#                     (omegaY/math.exp(lambdaX*abs(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])))*\\\n",
    "#                     (omegaX/math.exp(lambdaY*abs(batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])))\n",
    "                    if(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,i]<0):\n",
    "                        #if i follows j, then multiple m, m<1\n",
    "                        adjacencyMatrix[i,j]=adjacencyMatrix[i,j]*m\n",
    "            adjacencyMatrix=adjacencyMatrix.unsqueeze(0)\n",
    "            if timeStepI==0:\n",
    "                matrixSequenceInTimeStepDim=adjacencyMatrix\n",
    "            else:\n",
    "                matrixSequenceInTimeStepDim=\\\n",
    "                torch.cat((matrixSequenceInTimeStepDim,adjacencyMatrix),0)\n",
    "        matrixSequenceInTimeStepDim=matrixSequenceInTimeStepDim.unsqueeze(0)\n",
    "        if batchI==0:\n",
    "            matrixSequenceInBatchDim=matrixSequenceInTimeStepDim\n",
    "        else:\n",
    "            matrixSequenceInBatchDim=torch.cat((matrixSequenceInBatchDim,matrixSequenceInTimeStepDim),0)            \n",
    "    return matrixSequenceInBatchDim\n",
    "generateAdjacencyMatrix(dataInputs[:,:,0:2,:],5,5,1,1,2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100, 2, 250])\n",
      "batchI 0\n",
      "timeStepI: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4692,  0.0469,  0.4272,  0.4634, -0.3572,  0.0631, -0.8156, -0.1434,\n",
      "         0.3318, -0.3463], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3866, 0.0365, 0.4255, 0.7478, 0.4504, 0.1000, 0.0457, 0.5955,\n",
      "        0.0349], device='cuda:0')\n",
      "tensor(0.3919, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 1\n",
      "tensor([ 0.0478,  0.0277,  0.0562,  0.0078, -0.0487, -0.0199, -0.1013, -0.0141,\n",
      "         0.0652, -0.0219], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3851, 0.0365, 0.4250, 0.7432, 0.4508, 0.1001, 0.0452, 0.5930,\n",
      "        0.0348], device='cuda:0')\n",
      "tensor(0.3037, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 2\n",
      "tensor([ 0.0488,  0.0274,  0.0547,  0.0105, -0.0465, -0.0163, -0.1088, -0.0175,\n",
      "         0.0592, -0.0211], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.0893e-04, 3.6663e-02, 4.2636e-01, 7.3090e-01, 4.5335e-01,\n",
      "        1.0076e-01, 4.4692e-02, 5.8499e-01, 3.4389e-02], device='cuda:0')\n",
      "tensor(0.3074, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 3\n",
      "tensor([ 0.0477,  0.0302,  0.0485,  0.0173, -0.0437, -0.0091, -0.1250, -0.0199,\n",
      "         0.0518, -0.0231], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 4.5152e-04, 3.6075e-02, 4.1873e-01, 7.0709e-01, 4.5111e-01,\n",
      "        9.9353e-02, 4.3816e-02, 5.6939e-01, 3.3552e-02], device='cuda:0')\n",
      "tensor(0.3043, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 4\n",
      "tensor([ 0.0434,  0.0307,  0.0392,  0.0240, -0.0413,  0.0020, -0.1489, -0.0215,\n",
      "         0.0401, -0.0286], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 4.3590e-04, 3.5598e-02, 4.0979e-01, 6.8232e-01, 4.4884e-01,\n",
      "        9.8865e-02, 4.2176e-02, 5.5406e-01, 3.2807e-02], device='cuda:0')\n",
      "tensor(0.2994, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 5\n",
      "tensor([ 0.0656,  0.0137,  0.0600,  0.0030, -0.0301,  0.0189, -0.1441, -0.0295,\n",
      "         0.0405, -0.0026], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 4.3298e-04, 3.5123e-02, 4.0017e-01, 6.5894e-01, 4.4044e-01,\n",
      "        9.8689e-02, 4.0225e-02, 5.3321e-01, 3.1761e-02], device='cuda:0')\n",
      "tensor(0.2984, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 6\n",
      "tensor([ 0.0679,  0.0082,  0.0644,  0.0149, -0.0272,  0.0328, -0.1748, -0.0428,\n",
      "         0.0422, -0.0017], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 4.1747e-04, 3.4606e-02, 3.9068e-01, 6.4232e-01, 4.3133e-01,\n",
      "        9.9098e-02, 3.8075e-02, 5.1589e-01, 3.0933e-02], device='cuda:0')\n",
      "tensor(0.2975, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 7\n",
      "tensor([ 0.0910,  0.0226,  0.0528,  0.0269, -0.0156,  0.0427, -0.2014, -0.0492,\n",
      "         0.0428,  0.0054], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 4.0316e-04, 3.3884e-02, 3.7966e-01, 6.2633e-01, 4.2335e-01,\n",
      "        9.7902e-02, 3.6329e-02, 4.9889e-01, 3.0094e-02], device='cuda:0')\n",
      "tensor(0.2925, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 8\n",
      "tensor([ 0.0781, -0.0053,  0.0748,  0.0256, -0.0193,  0.0702, -0.2188, -0.0643,\n",
      "         0.0566,  0.0223], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.8709e-04, 3.2635e-02, 3.6493e-01, 6.0685e-01, 4.1461e-01,\n",
      "        9.5306e-02, 3.4626e-02, 4.8287e-01, 2.9225e-02], device='cuda:0')\n",
      "tensor(0.2916, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 9\n",
      "tensor([ 0.0989,  0.0105,  0.0566,  0.0469, -0.0109,  0.0685, -0.2451, -0.0546,\n",
      "         0.0481,  0.0073], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.7093e-04, 3.1335e-02, 3.5064e-01, 5.8779e-01, 4.0595e-01,\n",
      "        9.1328e-02, 3.3269e-02, 4.6660e-01, 2.8235e-02], device='cuda:0')\n",
      "tensor(0.2905, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 10\n",
      "tensor([ 0.0861, -0.0117,  0.0771,  0.0555, -0.0179,  0.0914, -0.2715, -0.0721,\n",
      "         0.0642,  0.0183], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.5484e-04, 3.0111e-02, 3.3667e-01, 5.6945e-01, 3.9282e-01,\n",
      "        8.7684e-02, 3.1868e-02, 4.4917e-01, 2.7151e-02], device='cuda:0')\n",
      "tensor(0.2894, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 11\n",
      "tensor([ 0.1128,  0.0165,  0.0692,  0.0714, -0.0101,  0.0905, -0.2964, -0.0583,\n",
      "         0.0595,  0.0109], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.4137e-04, 2.9047e-02, 3.2487e-01, 5.4966e-01, 3.7746e-01,\n",
      "        8.4614e-02, 3.0813e-02, 4.3486e-01, 2.6253e-02], device='cuda:0')\n",
      "tensor(0.2846, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 12\n",
      "tensor([ 0.1015, -0.0194,  0.0847,  0.0728, -0.0155,  0.1096, -0.3092, -0.0863,\n",
      "         0.0679,  0.0258], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.3179e-04, 1.9775e-04, 3.1657e-01, 5.3402e-01, 3.6575e-01,\n",
      "        8.2445e-02, 3.0184e-02, 4.2542e-01, 2.5644e-02], device='cuda:0')\n",
      "tensor(0.2876, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 13\n",
      "tensor([ 0.1373,  0.0228,  0.0678,  0.0915, -0.0108,  0.0860, -0.3352, -0.0757,\n",
      "         0.0657,  0.0052], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.2487e-04, 1.9232e-04, 3.1055e-01, 5.1745e-01, 3.5672e-01,\n",
      "        8.0884e-02, 2.9524e-02, 4.1864e-01, 2.5237e-02], device='cuda:0')\n",
      "tensor(0.2862, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 14\n",
      "tensor([ 0.1197, -0.0223,  0.1052,  0.0971, -0.0210,  0.1252, -0.3514, -0.0965,\n",
      "         0.0703,  0.0261], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.2135e-04, 1.8896e-04, 3.0754e-01, 5.1229e-01, 3.5186e-01,\n",
      "        8.0097e-02, 2.9201e-02, 4.1584e-01, 2.5002e-02], device='cuda:0')\n",
      "tensor(0.2848, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 15\n",
      "tensor([ 0.1626,  0.0406,  0.0715,  0.1292,  0.0040,  0.0823, -0.3766, -0.0903,\n",
      "         0.0691, -0.0033], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.1961e-04, 1.8667e-04, 3.0613e-01, 5.1148e-01, 3.4848e-01,\n",
      "        7.9731e-02, 2.9094e-02, 4.1417e-01, 2.5089e-02], device='cuda:0')\n",
      "tensor(0.2833, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 16\n",
      "tensor([ 0.1316, -0.0191,  0.1065,  0.1163, -0.0310,  0.1329, -0.4037, -0.1061,\n",
      "         0.0732,  0.0235], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.1727e-04, 6.6504e-03, 3.0414e-01, 5.1510e-01, 3.4646e-01,\n",
      "        7.9214e-02, 2.8938e-02, 4.1229e-01, 2.5255e-02], device='cuda:0')\n",
      "tensor(0.2777, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 17\n",
      "tensor([ 0.1750,  0.0475,  0.0887,  0.1674,  0.0174,  0.0974, -0.4078, -0.1020,\n",
      "         0.0634, -0.0131], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.1495e-04, 6.5954e-03, 1.8148e-04, 5.1835e-01, 3.4488e-01,\n",
      "        1.8148e-04, 2.8781e-02, 4.1182e-01, 2.5583e-02], device='cuda:0')\n",
      "tensor(0.2843, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 18\n",
      "tensor([ 0.1635,  0.0030,  0.1212,  0.1220, -0.0318,  0.1406, -0.4539, -0.1096,\n",
      "         0.0733,  0.0033], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.1265e-04, 6.5403e-03, 1.7893e-04, 5.1944e-01, 3.4275e-01,\n",
      "        1.7893e-04, 2.8623e-02, 4.1194e-01, 2.6077e-02], device='cuda:0')\n",
      "tensor(0.2823, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 19\n",
      "tensor([ 0.1937,  0.0436,  0.0981,  0.1778,  0.0220,  0.1254, -0.4403, -0.1112,\n",
      "         0.0674, -0.0128], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.1031e-04, 6.4856e-03, 1.7641e-04, 5.1899e-01, 3.4101e-01,\n",
      "        1.7641e-04, 2.8464e-02, 4.1095e-01, 2.6835e-02], device='cuda:0')\n",
      "tensor(0.2801, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 20\n",
      "tensor([ 0.2079,  0.0236,  0.1212,  0.1590, -0.0286,  0.1432, -0.5020, -0.1128,\n",
      "         0.0768, -0.0113], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.1006e-04, 6.4530e-03, 1.7468e-04, 5.2172e-01, 3.4013e-01,\n",
      "        1.7468e-04, 2.8427e-02, 4.1177e-01, 2.7410e-02], device='cuda:0')\n",
      "tensor(0.2779, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 21\n",
      "tensor([ 0.2126,  0.0473,  0.1227,  0.1872,  0.0230,  0.1492, -0.4781, -0.1128,\n",
      "         0.0782, -0.0103], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.0909e-04, 6.3958e-03, 1.7228e-04, 5.2233e-01, 3.3599e-01,\n",
      "        1.7228e-04, 2.8279e-02, 4.1092e-01, 2.7928e-02], device='cuda:0')\n",
      "tensor(0.2755, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 22\n",
      "tensor([ 0.2506,  0.0320,  0.1150,  0.1907, -0.0256,  0.1500, -0.5377, -0.1081,\n",
      "         0.0852, -0.0202], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.0768e-04, 6.3270e-03, 1.6963e-04, 5.2205e-01, 3.3063e-01,\n",
      "        1.6963e-04, 2.8083e-02, 4.0938e-01, 2.8276e-02], device='cuda:0')\n",
      "tensor(0.2730, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2325,  0.0615,  0.1408,  0.2165,  0.0164,  0.1559, -0.5232, -0.1301,\n",
      "         0.0756, -0.0173], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.0467e-04, 6.2274e-03, 1.6616e-04, 5.1909e-01, 3.2033e-01,\n",
      "        1.6616e-04, 2.7715e-02, 4.0562e-01, 2.8282e-02], device='cuda:0')\n",
      "tensor(0.2703, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 24\n",
      "tensor([ 0.2898,  0.0474,  0.1205,  0.2208, -0.0261,  0.1623, -0.5701, -0.0972,\n",
      "         0.1117, -0.0407], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.0352e-04, 6.1662e-03, 1.6374e-04, 5.1927e-01, 3.1384e-01,\n",
      "        1.6374e-04, 2.7585e-02, 4.0350e-01, 2.8346e-02], device='cuda:0')\n",
      "tensor(0.2719, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 25\n",
      "tensor([ 0.2787,  0.0510,  0.1550,  0.2283,  0.0446,  0.1723, -0.5576, -0.1395,\n",
      "         0.0746, -0.0105], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.0402e-04, 6.1397e-03, 1.6224e-04, 5.2232e-01, 3.1604e-01,\n",
      "        1.6224e-04, 2.7573e-02, 4.0629e-01, 2.8499e-02], device='cuda:0')\n",
      "tensor(0.2688, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 26\n",
      "tensor([ 0.3372,  0.0699,  0.1318,  0.2467, -0.0361,  0.1712, -0.6057, -0.0926,\n",
      "         0.1437, -0.0500], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.0350e-04, 6.0920e-03, 1.6021e-04, 5.2357e-01, 3.1477e-01,\n",
      "        1.6021e-04, 2.7468e-02, 4.0956e-01, 2.8453e-02], device='cuda:0')\n",
      "tensor(0.2656, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 27\n",
      "tensor([ 0.3088,  0.0507,  0.1594,  0.2537,  0.0480,  0.1780, -0.5886, -0.1403,\n",
      "         0.0690, -0.0238], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.0050e-04, 5.9957e-03, 1.9950e-03, 5.2061e-01, 1.5692e-04,\n",
      "        1.5692e-04, 2.7188e-02, 4.1191e-01, 2.8272e-02], device='cuda:0')\n",
      "tensor(0.2629, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 28\n",
      "tensor([ 0.3768,  0.0767,  0.1434,  0.2884, -0.0276,  0.1778, -0.6463, -0.0638,\n",
      "         0.1794, -0.0640], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 2.9556e-04, 5.8612e-03, 1.9558e-03, 5.1414e-01, 2.4098e-04,\n",
      "        1.5266e-04, 2.6811e-02, 4.1344e-01, 2.7769e-02], device='cuda:0')\n",
      "tensor(0.2571, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 29\n",
      "tensor([ 0.3423,  0.0486,  0.1688,  0.2748,  0.0428,  0.1608, -0.6231, -0.1304,\n",
      "         0.0751, -0.0439], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 2.9486e-04, 5.8123e-03, 1.9470e-03, 5.1507e-01, 2.3966e-04,\n",
      "        1.5066e-04, 2.6825e-02, 4.1394e-01, 2.7725e-02], device='cuda:0')\n",
      "tensor(0.2537, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 30\n",
      "tensor([ 0.4228,  0.0863,  0.1476,  0.3126, -0.0173,  0.1849, -0.6758, -0.0594,\n",
      "         0.1883, -0.0659], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 2.9514e-04, 5.7825e-03, 1.9425e-03, 5.1771e-01, 2.3912e-04,\n",
      "        1.4916e-04, 2.6922e-02, 4.1402e-01, 2.7896e-02], device='cuda:0')\n",
      "tensor(0.2502, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 31\n",
      "tensor([ 0.3820,  0.0631,  0.1777,  0.3001,  0.0380,  0.1408, -0.6602, -0.1172,\n",
      "         0.0944, -0.0623], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 2.9945e-04, 5.8317e-03, 1.9648e-03, 5.2735e-01, 2.4185e-04,\n",
      "        1.4971e-04, 2.7392e-02, 4.1393e-01, 2.8749e-02], device='cuda:0')\n",
      "tensor(0.2466, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 32\n",
      "tensor([ 0.4590,  0.0781,  0.1535,  0.3327, -0.0056,  0.1878, -0.6968, -0.0607,\n",
      "         0.1895, -0.0662], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.0228e-04, 5.8514e-03, 1.9771e-03, 5.3421e-01, 2.4336e-04,\n",
      "        5.7912e-03, 2.7729e-02, 4.0883e-01, 2.9586e-02], device='cuda:0')\n",
      "tensor(0.2402, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 33\n",
      "tensor([ 0.4203,  0.0783,  0.1883,  0.3270,  0.0336,  0.1167, -0.6782, -0.0988,\n",
      "         0.1153, -0.0546], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.0293e-04, 5.8175e-03, 1.9711e-03, 5.3675e-01, 2.4263e-04,\n",
      "        5.7628e-03, 2.7809e-02, 3.9884e-01, 2.9807e-02], device='cuda:0')\n",
      "tensor(0.2366, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 34\n",
      "tensor([ 0.4854,  0.0707,  0.1655,  0.3485, -0.0068,  0.1764, -0.7139, -0.0609,\n",
      "         0.1864, -0.0764], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.0242e-04, 5.7801e-03, 1.9645e-03, 5.4235e-01, 2.4179e-04,\n",
      "        5.7319e-03, 2.7880e-02, 3.8946e-01, 2.9727e-02], device='cuda:0')\n",
      "tensor(0.2330, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 35\n",
      "tensor([ 0.4753,  0.0929,  0.1912,  0.3561,  0.0423,  0.1086, -0.6988, -0.0789,\n",
      "         0.1452, -0.0517], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 2.9922e-04, 5.7406e-03, 1.9576e-03, 5.4809e-01, 2.4078e-04,\n",
      "        5.7008e-03, 2.7944e-02, 3.8029e-01, 2.9666e-02], device='cuda:0')\n",
      "tensor(0.2294, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 36\n",
      "tensor([ 0.5166,  0.0604,  0.1768,  0.3601, -0.0101,  0.1575, -0.7295, -0.0587,\n",
      "         0.1814, -0.0917], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 4.8592e-01, 2.5084e-02, 7.3319e-02, 2.5810e-04, 5.9540e-01,\n",
      "        2.5248e-02, 5.1071e-03, 3.8520e-04, 4.8432e-03], device='cuda:0')\n",
      "tensor(0.2298, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 37\n",
      "tensor([ 0.5121,  0.1278,  0.1791,  0.3840,  0.0362,  0.1308, -0.7170, -0.0471,\n",
      "         0.1533, -0.0433], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.4604e-01, 8.7682e-02, 2.5612e-01, 8.8883e-04, 4.1813e-01,\n",
      "        8.8331e-02, 4.2883e-03, 1.3734e-03, 8.0462e-03], device='cuda:0')\n",
      "tensor(0.2224, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 38\n",
      "tensor([ 0.5562,  0.0893,  0.1976,  0.3863, -0.0268,  0.1640, -0.7384, -0.0586,\n",
      "         0.1664, -0.1065], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.4395e-01, 8.7610e-02, 2.5588e-01, 8.7532e-04, 4.1778e-01,\n",
      "        8.8480e-02, 4.2677e-03, 1.4006e-03, 7.9868e-03], device='cuda:0')\n",
      "tensor(0.2186, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 39\n",
      "tensor([ 0.5642,  0.1585,  0.1843,  0.3980,  0.0322,  0.1461, -0.7348, -0.0356,\n",
      "         0.1592, -0.0562], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.4251e-01, 8.7428e-02, 2.5538e-01, 8.6120e-04, 4.1630e-01,\n",
      "        8.8628e-02, 2.8051e-01, 1.4236e-03, 7.9803e-03], device='cuda:0')\n",
      "tensor(0.2231, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 40\n",
      "tensor([ 0.5892,  0.1242,  0.2147,  0.4277, -0.0591,  0.1731, -0.7644, -0.0262,\n",
      "         0.1636, -0.1095], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.4106e-01, 8.7340e-02, 2.5524e-01, 8.4811e-04, 4.1574e-01,\n",
      "        8.8796e-02, 2.7863e-01, 1.4481e-03, 8.0094e-03], device='cuda:0')\n",
      "tensor(0.2190, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 41\n",
      "tensor([ 0.6009,  0.1861,  0.1911,  0.4127,  0.0167,  0.1601, -0.7536,  0.0058,\n",
      "         0.1600, -0.0763], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.3958e-01, 8.7248e-02, 2.5388e-01, 8.3521e-04, 4.1523e-01,\n",
      "        8.8971e-02, 2.7675e-01, 1.4751e-03, 8.0131e-03], device='cuda:0')\n",
      "tensor(0.2150, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 42\n",
      "tensor([ 0.6264,  0.1539,  0.2214,  0.4556, -0.0693,  0.1819, -0.7748,  0.0014,\n",
      "         0.1557, -0.1117], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.3918e-01, 8.7156e-02, 2.5290e-01, 8.2251e-04, 4.1470e-01,\n",
      "        8.9144e-02, 2.7489e-01, 1.5026e-03, 7.9542e-03], device='cuda:0')\n",
      "tensor(0.2152, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 43\n",
      "tensor([ 0.6519,  0.2076,  0.1923,  0.4216,  0.0026,  0.1776, -0.7693,  0.0309,\n",
      "         0.1571, -0.0859], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.3882e-01, 8.7082e-02, 2.5163e-01, 8.1011e-04, 4.1519e-01,\n",
      "        8.9323e-02, 5.0545e-01, 1.5273e-03, 7.8647e-03], device='cuda:0')\n",
      "tensor(0.2057, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 44\n",
      "tensor([ 0.6680,  0.1702,  0.2224,  0.4708, -0.0648,  0.1871, -0.7779,  0.0220,\n",
      "         0.1492, -0.1109], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.3851e-01, 8.6983e-02, 2.5048e-01, 7.9774e-04, 4.1664e-01,\n",
      "        8.9490e-02, 5.0606e-01, 1.5451e-03, 7.7929e-03], device='cuda:0')\n",
      "tensor(0.2018, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6886,  0.2238,  0.2059,  0.4369, -0.0118,  0.1899, -0.7704,  0.0660,\n",
      "         0.1484, -0.0922], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.3816e-01, 8.6904e-02, 2.4891e-01, 7.8566e-04, 4.1804e-01,\n",
      "        8.9664e-02, 5.0677e-01, 1.5591e-03, 7.7203e-03], device='cuda:0')\n",
      "tensor(0.1980, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 46\n",
      "tensor([ 0.7057,  0.2015,  0.2210,  0.4833, -0.0723,  0.1952, -0.7791,  0.0565,\n",
      "         0.1401, -0.1153], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.3785e-01, 8.6810e-02, 2.4748e-01, 7.7360e-04, 4.1949e-01,\n",
      "        8.9838e-02, 5.0742e-01, 1.5726e-03, 7.6267e-03], device='cuda:0')\n",
      "tensor(0.1954, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 47\n",
      "tensor([ 0.7213,  0.2403,  0.2073,  0.4347, -0.0226,  0.2135, -0.7636,  0.0913,\n",
      "         0.1436, -0.0948], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.3750e-01, 8.6732e-02, 2.4608e-01, 7.6200e-04, 4.2086e-01,\n",
      "        9.0012e-02, 5.0810e-01, 1.5858e-03, 7.5407e-03], device='cuda:0')\n",
      "tensor(0.1916, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 48\n",
      "tensor([ 0.7424,  0.2278,  0.2126,  0.4902, -0.0781,  0.2126, -0.7669,  0.0851,\n",
      "         0.1356, -0.1153], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.3719e-01, 8.6849e-02, 2.4468e-01, 7.5069e-04, 4.2275e-01,\n",
      "        9.0180e-02, 5.0873e-01, 1.5998e-03, 7.4948e-03], device='cuda:0')\n",
      "tensor(0.1879, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 49\n",
      "tensor([ 0.7577,  0.2564,  0.2176,  0.4493, -0.0438,  0.2216, -0.7709,  0.1181,\n",
      "         0.1328, -0.0993], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.3684e-01, 8.6802e-02, 2.4331e-01, 7.3728e-04, 4.2426e-01,\n",
      "        9.0357e-02, 5.0946e-01, 1.6133e-03, 7.4641e-03], device='cuda:0')\n",
      "tensor(0.1843, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 50\n",
      "tensor([ 0.7747,  0.2532,  0.2114,  0.4902, -0.0849,  0.2256, -0.7611,  0.1155,\n",
      "         0.1299, -0.1175], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.3649e-01, 8.6504e-02, 2.4184e-01, 7.2296e-04, 4.2568e-01,\n",
      "        9.0489e-02, 5.0942e-01, 1.6229e-03, 7.4277e-03], device='cuda:0')\n",
      "tensor(0.1808, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 51\n",
      "tensor([ 0.7914,  0.2726,  0.2198,  0.4586, -0.0617,  0.2312, -0.7723,  0.1441,\n",
      "         0.1218, -0.1037], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.3568e-01, 8.6155e-02, 2.4068e-01, 7.1322e-04, 4.2679e-01,\n",
      "        9.0781e-02, 5.1036e-01, 1.6302e-03, 7.3806e-03], device='cuda:0')\n",
      "tensor(0.1773, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 52\n",
      "tensor([ 0.8068,  0.2740,  0.2133,  0.4856, -0.0938,  0.2376, -0.7577,  0.1434,\n",
      "         0.1245, -0.1191], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.3548e-01, 8.6053e-02, 2.3967e-01, 7.0738e-04, 4.2721e-01,\n",
      "        9.1163e-02, 5.1152e-01, 1.6348e-03, 7.3197e-03], device='cuda:0')\n",
      "tensor(0.1710, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 53\n",
      "tensor([ 0.8166,  0.2885,  0.2180,  0.4633, -0.0747,  0.2449, -0.7647,  0.1710,\n",
      "         0.1115, -0.1067], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.3672e-01, 8.5884e-02, 2.3845e-01, 7.0455e-04, 4.2875e-01,\n",
      "        9.0989e-02, 5.1191e-01, 1.6373e-03, 7.2810e-03], device='cuda:0')\n",
      "tensor(0.1677, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 54\n",
      "tensor([ 0.8311,  0.2911,  0.2175,  0.4790, -0.1024,  0.2505, -0.7521,  0.1702,\n",
      "         0.1202, -0.1203], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.3931e-01, 8.5654e-02, 2.3708e-01, 7.0188e-04, 4.3074e-01,\n",
      "        9.0431e-02, 5.0966e-01, 1.6395e-03, 7.2416e-03], device='cuda:0')\n",
      "tensor(0.1645, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 55\n",
      "tensor([ 0.8468,  0.3045,  0.2132,  0.4658, -0.0892,  0.2579, -0.7597,  0.1974,\n",
      "         0.1001, -0.1069], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.4324e-01, 8.5199e-02, 2.3511e-01, 6.9667e-04, 4.3448e-01,\n",
      "        8.9608e-02, 5.0625e-01, 1.6371e-03, 7.2397e-03], device='cuda:0')\n",
      "tensor(0.1614, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 56\n",
      "tensor([ 0.8579,  0.3053,  0.2178,  0.4712, -0.1095,  0.2616, -0.7493,  0.1914,\n",
      "         0.1167, -0.1224], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.4840e-01, 8.4429e-02, 2.3226e-01, 6.8903e-04, 4.3813e-01,\n",
      "        8.8459e-02, 5.0048e-01, 1.6289e-03, 7.2261e-03], device='cuda:0')\n",
      "tensor(0.1583, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 57\n",
      "tensor([ 0.8773,  0.3204,  0.2081,  0.4647, -0.1045,  0.2715, -0.7517,  0.2231,\n",
      "         0.0892, -0.1057], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.5328e-01, 8.3751e-02, 2.2968e-01, 6.8210e-04, 4.4310e-01,\n",
      "        8.7409e-02, 4.9521e-01, 1.6198e-03, 7.2400e-03], device='cuda:0')\n",
      "tensor(0.1552, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 58\n",
      "tensor([ 0.8837,  0.3187,  0.2162,  0.4649, -0.1144,  0.2738, -0.7448,  0.2115,\n",
      "         0.1132, -0.1234], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.5820e-01, 8.3033e-02, 2.2715e-01, 6.7536e-04, 4.4804e-01,\n",
      "        8.6372e-02, 4.9004e-01, 1.6128e-03, 7.0083e-03], device='cuda:0')\n",
      "tensor(0.1522, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 59\n",
      "tensor([ 0.9038,  0.3339,  0.2032,  0.4578, -0.1170,  0.2827, -0.7471,  0.2436,\n",
      "         0.0819, -0.1069], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.6324e-01, 8.2469e-02, 2.2481e-01, 6.6869e-04, 4.5204e-01,\n",
      "        8.5343e-02, 4.8492e-01, 1.6156e-03, 7.0198e-03], device='cuda:0')\n",
      "tensor(0.1493, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 60\n",
      "tensor([ 0.9115,  0.3347,  0.2130,  0.4623, -0.1212,  0.2891, -0.7322,  0.2336,\n",
      "         0.1071, -0.1200], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.6836e-01, 8.1850e-02, 2.2154e-01, 6.6210e-04, 4.5427e-01,\n",
      "        8.4331e-02, 4.7984e-01, 1.6217e-03, 2.5050e-01], device='cuda:0')\n",
      "tensor(0.1470, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 61\n",
      "tensor([ 0.9208,  0.3376,  0.1873,  0.4454, -0.1326,  0.2894, -0.7507,  0.2659,\n",
      "         0.0733, -0.0878], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 3.7644e-01, 8.1291e-02, 2.1859e-01, 6.5562e-04, 4.5556e-01,\n",
      "        8.3327e-02, 4.7484e-01, 1.6230e-03, 2.5018e-01], device='cuda:0')\n",
      "tensor(0.1440, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 62\n",
      "tensor([ 0.9302,  0.3473,  0.2015,  0.4575, -0.1300,  0.3016, -0.7224,  0.2566,\n",
      "         0.0948, -0.0918], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3853, 0.0806, 0.2159, 0.2499, 0.4551, 0.0823, 0.4699, 0.0016,\n",
      "        0.2499], device='cuda:0')\n",
      "tensor(0.1454, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 63\n",
      "tensor([ 0.9447,  0.3477,  0.1834,  0.4498, -0.1319,  0.3051, -0.7441,  0.2824,\n",
      "         0.0658, -0.0690], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3944, 0.0800, 0.2130, 0.2495, 0.4567, 0.0814, 0.4650, 0.0016,\n",
      "        0.2495], device='cuda:0')\n",
      "tensor(0.1467, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 64\n",
      "tensor([ 0.9507,  0.3578,  0.1870,  0.4577, -0.1165,  0.3185, -0.7205,  0.2755,\n",
      "         0.0843, -0.0674], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4036, 0.0792, 0.2102, 0.2492, 0.4583, 0.0803, 0.4601, 0.0016,\n",
      "        0.2492], device='cuda:0')\n",
      "tensor(0.1436, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 65\n",
      "tensor([ 0.9708,  0.3578,  0.1802,  0.4464, -0.1231,  0.3146, -0.7384,  0.2972,\n",
      "         0.0666, -0.0556], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4130, 0.0787, 0.2086, 0.2489, 0.4599, 0.0794, 0.4553, 0.0016,\n",
      "        0.2489], device='cuda:0')\n",
      "tensor(0.1406, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 66\n",
      "tensor([ 0.9716,  0.3740,  0.1812,  0.4498, -0.1124,  0.3324, -0.7113,  0.2928,\n",
      "         0.0747, -0.0543], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4228, 0.0781, 0.2069, 0.2486, 0.4615, 0.0788, 0.4505, 0.0016,\n",
      "        0.2486], device='cuda:0')\n",
      "tensor(0.1421, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 67\n",
      "tensor([ 0.9940,  0.3762,  0.1745,  0.4398, -0.1148,  0.3206, -0.7329,  0.3143,\n",
      "         0.0620, -0.0340], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4327, 0.0776, 0.2053, 0.7112, 0.4631, 0.0782, 0.4459, 0.0015,\n",
      "        0.0685], device='cuda:0')\n",
      "tensor(0.1327, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9889,  0.3967,  0.1826,  0.4411, -0.0919,  0.3445, -0.6909,  0.2986,\n",
      "         0.0697, -0.0531], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4437, 0.0771, 0.2037, 0.7151, 0.4646, 0.0777, 0.4412, 0.0015,\n",
      "        0.0679], device='cuda:0')\n",
      "tensor(0.1300, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 69\n",
      "tensor([ 1.0139,  0.3877,  0.1758,  0.4296, -0.0907,  0.3329, -0.7202,  0.3276,\n",
      "         0.0604, -0.0424], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4521, 0.0770, 0.2021, 0.7190, 0.4662, 0.0775, 0.4366, 0.0015,\n",
      "        0.0674], device='cuda:0')\n",
      "tensor(0.1241, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 70\n",
      "tensor([ 1.0085,  0.4144,  0.1810,  0.4261, -0.0834,  0.3584, -0.6768,  0.3110,\n",
      "         0.0630, -0.0587], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4568, 0.0770, 0.2005, 0.7229, 0.4679, 0.0774, 0.4320, 0.0015,\n",
      "        0.0669], device='cuda:0')\n",
      "tensor(0.1239, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 71\n",
      "tensor([ 1.0274,  0.4049,  0.1694,  0.4157, -0.0785,  0.3456, -0.7070,  0.3339,\n",
      "         0.0529, -0.0516], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4590, 0.0770, 0.1986, 0.7270, 0.4696, 0.0773, 0.4274, 0.0015,\n",
      "        0.0664], device='cuda:0')\n",
      "tensor(0.1213, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 72\n",
      "tensor([ 1.0262,  0.4293,  0.1765,  0.4154, -0.0716,  0.3670, -0.6684,  0.3181,\n",
      "         0.0609, -0.0606], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4604, 0.0769, 0.1964, 0.7315, 0.4713, 0.0772, 0.4229, 0.0015,\n",
      "        0.0658], device='cuda:0')\n",
      "tensor(0.1272, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 73\n",
      "tensor([ 1.0367,  0.3973,  0.1601,  0.3923, -0.0661,  0.3542, -0.6963,  0.3442,\n",
      "         0.0512, -0.0551], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4607, 0.0770, 0.1958, 0.7343, 0.4720, 0.0773, 0.4193, 0.0015,\n",
      "        0.0655], device='cuda:0')\n",
      "tensor(0.1243, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 74\n",
      "tensor([ 1.0445,  0.4378,  0.1684,  0.4043, -0.0606,  0.3759, -0.6610,  0.3295,\n",
      "         0.0596, -0.0583], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4602, 0.0776, 0.1959, 0.7327, 0.4700, 0.0778, 0.4180, 0.0015,\n",
      "        0.0655], device='cuda:0')\n",
      "tensor(0.1216, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 75\n",
      "tensor([ 1.0492,  0.4079,  0.1557,  0.3798, -0.0546,  0.3714, -0.6817,  0.3518,\n",
      "         0.0461, -0.0553], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4554, 0.0786, 0.1948, 0.7277, 0.4662, 0.0787, 0.4176, 0.0015,\n",
      "        0.0657], device='cuda:0')\n",
      "tensor(0.1191, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 76\n",
      "tensor([ 1.0579,  0.4463,  0.1652,  0.3938, -0.0514,  0.3854, -0.6526,  0.3401,\n",
      "         0.0599, -0.0606], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4452, 0.0797, 0.1926, 0.7208, 0.4613, 0.0797, 0.4175, 0.0016,\n",
      "        0.0661], device='cuda:0')\n",
      "tensor(0.1166, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 77\n",
      "tensor([ 1.0618,  0.4138,  0.1494,  0.3668, -0.0440,  0.3882, -0.6685,  0.3556,\n",
      "         0.0430, -0.0551], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4332, 0.0810, 0.1904, 0.7133, 0.4563, 0.0808, 0.4181, 0.0016,\n",
      "        0.0666], device='cuda:0')\n",
      "tensor(0.1142, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 78\n",
      "tensor([ 1.0676,  0.4510,  0.1629,  0.3840, -0.0443,  0.3944, -0.6442,  0.3518,\n",
      "         0.0587, -0.0625], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4239, 0.0821, 0.1880, 0.7071, 0.4535, 0.0819, 0.4176, 0.0016,\n",
      "        0.0670], device='cuda:0')\n",
      "tensor(0.1119, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 79\n",
      "tensor([ 1.0744,  0.4143,  0.1434,  0.3522, -0.0323,  0.4031, -0.6550,  0.3560,\n",
      "         0.0431, -0.0556], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4195, 0.0833, 0.1856, 0.7013, 0.4499, 0.0829, 0.4165, 0.0016,\n",
      "        0.0674], device='cuda:0')\n",
      "tensor(0.1066, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 80\n",
      "tensor([ 1.0777,  0.4570,  0.1590,  0.3850, -0.0392,  0.4038, -0.6387,  0.3600,\n",
      "         0.0566, -0.0614], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4172, 0.0843, 0.1830, 0.6965, 0.4502, 0.0837, 0.4168, 0.0016,\n",
      "        0.0678], device='cuda:0')\n",
      "tensor(0.1018, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 81\n",
      "tensor([ 1.0790,  0.4071,  0.1424,  0.3419, -0.0179,  0.4173, -0.6363,  0.3560,\n",
      "         0.0490, -0.0548], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4133, 0.0854, 0.1806, 0.6911, 0.4521, 0.0845, 0.4202, 0.2195,\n",
      "        0.0682], device='cuda:0')\n",
      "tensor(0.1083, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 82\n",
      "tensor([ 1.0845,  0.4439,  0.1648,  0.3778, -0.0339,  0.4134, -0.6197,  0.3700,\n",
      "         0.0693, -0.0525], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4099, 0.0866, 0.1779, 0.6879, 0.4560, 0.0853, 0.4261, 0.2165,\n",
      "        0.0686], device='cuda:0')\n",
      "tensor(0.1100, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 83\n",
      "tensor([ 1.0883,  0.3990,  0.1390,  0.3327,  0.0028,  0.4281, -0.6226,  0.3515,\n",
      "         0.0682, -0.0509], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4073, 0.0873, 0.1757, 0.6826, 0.4612, 0.0861, 0.4331, 0.2133,\n",
      "        0.0689], device='cuda:0')\n",
      "tensor(0.1074, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 84\n",
      "tensor([ 1.0879,  0.4419,  0.1644,  0.3669, -0.0220,  0.4273, -0.6100,  0.3738,\n",
      "         0.0916, -0.0548], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4046, 0.0878, 0.1747, 0.6774, 0.4649, 0.0870, 0.4398, 0.2100,\n",
      "        0.0692], device='cuda:0')\n",
      "tensor(0.1051, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 85\n",
      "tensor([ 1.0970,  0.3955,  0.1364,  0.3233,  0.0090,  0.4406, -0.6124,  0.3550,\n",
      "         0.0891, -0.0483], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.4018, 0.0878, 0.1743, 0.6712, 0.4682, 0.0879, 0.4465, 0.2067,\n",
      "        0.0694], device='cuda:0')\n",
      "tensor(0.1029, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 86\n",
      "tensor([ 1.0917,  0.4373,  0.1584,  0.3543, -0.0113,  0.4381, -0.5967,  0.3780,\n",
      "         0.1127, -0.0539], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3989, 0.0881, 0.1737, 0.6661, 0.4707, 0.0888, 0.4535, 0.2032,\n",
      "        0.0696], device='cuda:0')\n",
      "tensor(0.1007, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 87\n",
      "tensor([ 1.1028,  0.3945,  0.1363,  0.3136,  0.0123,  0.4526, -0.6019,  0.3597,\n",
      "         0.1070, -0.0489], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3963, 0.0884, 0.1745, 0.6614, 0.4712, 0.0894, 0.4604, 0.2893,\n",
      "        0.0698], device='cuda:0')\n",
      "tensor(0.0954, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 88\n",
      "tensor([ 1.0930,  0.4332,  0.1466,  0.3408,  0.0040,  0.4456, -0.5859,  0.3856,\n",
      "         0.1321, -0.0579], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3937, 0.0887, 0.1770, 0.6568, 0.4708, 0.0896, 0.4673, 0.2914,\n",
      "        0.0699], device='cuda:0')\n",
      "tensor(0.0978, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 89\n",
      "tensor([ 1.1103,  0.3900,  0.1361,  0.3112,  0.0136,  0.4636, -0.5916,  0.3597,\n",
      "         0.1273, -0.0511], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3911, 0.0885, 0.1802, 0.6522, 0.4703, 0.0896, 0.4744, 0.2939,\n",
      "        0.0701], device='cuda:0')\n",
      "tensor(0.0957, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 90\n",
      "tensor([ 1.0976,  0.4216,  0.1382,  0.3244,  0.0124,  0.4566, -0.5751,  0.3865,\n",
      "         0.1526, -0.0565], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3883, 0.0878, 0.1827, 0.6480, 0.4688, 0.0895, 0.4817, 0.2962,\n",
      "        0.0703], device='cuda:0')\n",
      "tensor(0.0937, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 91\n",
      "tensor([ 1.1144,  0.3915,  0.1337,  0.3033,  0.0183,  0.4708, -0.5835,  0.3680,\n",
      "         0.1526, -0.0521], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3809, 0.0878, 0.1852, 0.6380, 0.4633, 0.0903, 0.4939, 0.3013,\n",
      "        0.0712], device='cuda:0')\n",
      "tensor(0.0918, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 92\n",
      "tensor([ 1.0986,  0.4117,  0.1311,  0.3082,  0.0212,  0.4637, -0.5628,  0.3925,\n",
      "         0.1736, -0.0546], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3736, 0.0880, 0.1873, 0.6271, 0.4560, 0.0913, 0.5070, 0.3069,\n",
      "        0.0721], device='cuda:0')\n",
      "tensor(0.0899, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1174,  0.3906,  0.1305,  0.2951,  0.0242,  0.4766, -0.5753,  0.3761,\n",
      "         0.1751, -0.0523], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3682, 0.0882, 0.1901, 0.6166, 0.4475, 0.0924, 0.5199, 0.3124,\n",
      "        0.0731], device='cuda:0')\n",
      "tensor(0.0862, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 94\n",
      "tensor([ 1.0982,  0.4027,  0.1248,  0.2926,  0.0305,  0.4672, -0.5507,  0.4032,\n",
      "         0.1953, -0.0518], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3647, 0.0890, 0.1935, 0.6054, 0.4346, 0.0935, 0.5333, 0.3182,\n",
      "        0.0740], device='cuda:0')\n",
      "tensor(0.0845, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 95\n",
      "tensor([ 1.1175,  0.3883,  0.1270,  0.2856,  0.0318,  0.4787, -0.5655,  0.3872,\n",
      "         0.1968, -0.0522], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3627, 0.0893, 0.1956, 0.5983, 0.4236, 0.0936, 0.5409, 0.3218,\n",
      "        0.0745], device='cuda:0')\n",
      "tensor(0.0803, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 96\n",
      "tensor([ 1.0995,  0.3951,  0.1207,  0.2798,  0.0351,  0.4697, -0.5402,  0.4121,\n",
      "         0.2160, -0.0477], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3609, 0.0898, 0.1982, 0.5911, 0.4148, 0.0940, 0.5458, 0.3252,\n",
      "        0.0750], device='cuda:0')\n",
      "tensor(0.0787, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 97\n",
      "tensor([ 1.1160,  0.3846,  0.1226,  0.2741,  0.0368,  0.4779, -0.5543,  0.3976,\n",
      "         0.2180, -0.0509], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3612, 0.0900, 0.1996, 0.5863, 0.4128, 0.0947, 0.5483, 0.3271,\n",
      "        0.0750], device='cuda:0')\n",
      "tensor(0.0799, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 98\n",
      "tensor([ 1.0994,  0.3988,  0.1160,  0.2732,  0.0500,  0.4689, -0.5192,  0.4315,\n",
      "         0.2257, -0.0384], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3622, 0.0908, 0.2007, 0.5818, 0.4125, 0.0961, 0.5507, 0.3287,\n",
      "        0.0750], device='cuda:0')\n",
      "tensor(0.0744, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 99\n",
      "tensor([ 1.1101,  0.3819,  0.1250,  0.2645,  0.0432,  0.4776, -0.5433,  0.4081,\n",
      "         0.2372, -0.0517], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3647, 0.0919, 0.2013, 0.5815, 0.4129, 0.0970, 0.5485, 0.3276,\n",
      "        0.0744], device='cuda:0')\n",
      "tensor(0.0730, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "batchI 1\n",
      "timeStepI: 0\n",
      "tensor([ 1.1038,  0.3910,  0.1141,  0.2610,  0.0593,  0.4673, -0.5167,  0.4328,\n",
      "         0.2355, -0.0309], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3221, 0.0419, 0.1021, 0.6208, 0.4784, 0.2169, 0.2154, 0.0453,\n",
      "        0.5058], device='cuda:0')\n",
      "tensor(0.0872, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 1\n",
      "tensor([ 1.0982,  0.3944,  0.1254,  0.2746,  0.0509,  0.4843, -0.5216,  0.4115,\n",
      "         0.2364, -0.0284], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3214, 0.0418, 0.1018, 0.6262, 0.4764, 0.2161, 0.2155, 0.0442,\n",
      "        0.5064], device='cuda:0')\n",
      "tensor(0.0852, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 2\n",
      "tensor([ 1.1047,  0.3835,  0.1114,  0.2509,  0.0686,  0.4776, -0.5025,  0.4218,\n",
      "         0.2274,  0.0058], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3215, 0.0417, 0.1021, 0.6296, 0.4819, 0.2161, 0.2151, 0.0435,\n",
      "        0.5060], device='cuda:0')\n",
      "tensor(0.0823, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 3\n",
      "tensor([ 1.0988e+00,  3.7859e-01,  1.1011e-01,  2.5061e-01,  6.5687e-02,\n",
      "         4.8124e-01, -5.1013e-01,  4.0320e-01,  2.2663e-01,  5.3415e-04],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3233, 0.0419, 0.1028, 0.6307, 0.4903, 0.2163, 0.2159, 0.0435,\n",
      "        0.5024], device='cuda:0')\n",
      "tensor(0.0799, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 4\n",
      "tensor([ 1.1024,  0.3748,  0.1038,  0.2352,  0.0759,  0.4824, -0.4922,  0.4064,\n",
      "         0.2192,  0.0267], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3240, 0.0419, 0.1031, 0.6329, 0.4946, 0.2157, 0.2161, 0.0438,\n",
      "        0.5006], device='cuda:0')\n",
      "tensor(0.0777, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 5\n",
      "tensor([ 1.0981,  0.3681,  0.0987,  0.2324,  0.0762,  0.4852, -0.4993,  0.3930,\n",
      "         0.2180,  0.0215], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3213, 0.0416, 0.1010, 0.6403, 0.4901, 0.0066, 0.2142, 0.0441,\n",
      "        0.5056], device='cuda:0')\n",
      "tensor(0.0792, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 6\n",
      "tensor([ 1.0961,  0.3668,  0.1032,  0.2253,  0.0854,  0.4842, -0.4838,  0.3913,\n",
      "         0.2101,  0.0470], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3176, 0.0412, 0.0992, 0.6468, 0.4922, 0.0066, 0.2129, 0.0444,\n",
      "        0.5117], device='cuda:0')\n",
      "tensor(0.0771, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 7\n",
      "tensor([ 1.0955,  0.3601,  0.0926,  0.2195,  0.0889,  0.4873, -0.4924,  0.3829,\n",
      "         0.2090,  0.0417], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3183, 0.0413, 0.0991, 0.6444, 0.5033, 0.0065, 0.2148, 0.0450,\n",
      "        0.5121], device='cuda:0')\n",
      "tensor(0.0752, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 8\n",
      "tensor([ 1.0948,  0.3578,  0.0960,  0.2110,  0.0940,  0.4893, -0.4781,  0.3771,\n",
      "         0.2041,  0.0628], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3213, 0.0416, 0.0998, 0.6375, 0.5133, 0.0063, 0.2168, 0.0454,\n",
      "        0.5084], device='cuda:0')\n",
      "tensor(0.0734, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 9\n",
      "tensor([ 1.0953,  0.3530,  0.0850,  0.2048,  0.0988,  0.4913, -0.4849,  0.3732,\n",
      "         0.2018,  0.0585], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3258, 0.0421, 0.1016, 0.6284, 0.5187, 0.0061, 0.2186, 0.0461,\n",
      "        0.5023], device='cuda:0')\n",
      "tensor(0.0717, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 10\n",
      "tensor([ 1.0925,  0.3520,  0.0906,  0.1990,  0.1022,  0.4960, -0.4729,  0.3650,\n",
      "         0.1994,  0.0766], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3292, 0.0425, 0.1033, 0.6201, 0.5175, 0.0060, 0.2190, 0.0467,\n",
      "        0.4980], device='cuda:0')\n",
      "tensor(0.0701, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 11\n",
      "tensor([ 1.0956,  0.3474,  0.0780,  0.1900,  0.1074,  0.4955, -0.4759,  0.3633,\n",
      "         0.1953,  0.0734], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3296, 0.0424, 0.1039, 0.6183, 0.5073, 0.0097, 0.2152, 0.0474,\n",
      "        0.4985], device='cuda:0')\n",
      "tensor(0.0665, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 12\n",
      "tensor([ 1.0905,  0.3495,  0.0833,  0.1890,  0.1083,  0.5030, -0.4671,  0.3537,\n",
      "         0.1961,  0.0890], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3287, 0.0422, 0.1041, 0.6188, 0.4972, 0.0097, 0.2101, 0.0473,\n",
      "        0.5009], device='cuda:0')\n",
      "tensor(0.0651, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 13\n",
      "tensor([ 1.0971,  0.3418,  0.0703,  0.1752,  0.1154,  0.4972, -0.4660,  0.3537,\n",
      "         0.1887,  0.0867], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3257, 0.0417, 0.1037, 0.6233, 0.4820, 0.0097, 0.2042, 0.0468,\n",
      "        0.5064], device='cuda:0')\n",
      "tensor(0.0637, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 14\n",
      "tensor([ 1.0872,  0.3481,  0.0795,  0.1804,  0.1151,  0.5045, -0.4613,  0.3432,\n",
      "         0.1939,  0.1011], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3238, 0.0414, 0.1035, 0.6250, 0.4678, 0.0096, 0.2007, 0.0462,\n",
      "        0.5105], device='cuda:0')\n",
      "tensor(0.0623, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 15\n",
      "tensor([ 1.0976,  0.3347,  0.0648,  0.1617,  0.1248,  0.4950, -0.4564,  0.3433,\n",
      "         0.1823,  0.0991], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3220, 0.0411, 0.1034, 0.6292, 0.4541, 0.0096, 0.1992, 0.0460,\n",
      "        0.5145], device='cuda:0')\n",
      "tensor(0.0610, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 16\n",
      "tensor([ 1.0845,  0.3471,  0.0746,  0.1716,  0.1225,  0.5014, -0.4546,  0.3327,\n",
      "         0.1902,  0.1142], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.3210, 0.0408, 0.1036, 0.6372, 0.4418, 0.0095, 0.2007, 0.0461,\n",
      "        0.5173], device='cuda:0')\n",
      "tensor(0.0598, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 17\n",
      "tensor([ 1.0974,  0.3280,  0.0619,  0.1495,  0.1337,  0.4889, -0.4481,  0.3338,\n",
      "         0.1792,  0.1098], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.0056, 0.0056, 0.1040, 0.6483, 0.4308, 0.0094, 0.2017, 0.0462,\n",
      "        0.5184], device='cuda:0')\n",
      "tensor(0.0634, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0742,  0.3321,  0.0659,  0.1634,  0.1401,  0.4833, -0.4603,  0.3210,\n",
      "         0.1719,  0.1220], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.0055, 0.0055, 0.1053, 0.6568, 0.4243, 0.0092, 0.2030, 0.0470,\n",
      "        0.5135], device='cuda:0')\n",
      "tensor(0.0618, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 19\n",
      "tensor([ 1.0891,  0.2832,  0.0572,  0.1434,  0.1488,  0.4796, -0.4471,  0.3219,\n",
      "         0.1734,  0.1168], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.0053, 0.0053, 0.1077, 0.6644, 0.4242, 0.0089, 0.2042, 0.0484,\n",
      "        0.5019], device='cuda:0')\n",
      "tensor(0.0604, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 20\n",
      "tensor([ 1.0735,  0.2954,  0.0580,  0.1545,  0.1517,  0.4764, -0.4549,  0.3093,\n",
      "         0.1656,  0.1363], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.0051, 0.0051, 0.1107, 0.6605, 0.4295, 0.0086, 0.2053, 0.0498,\n",
      "        0.4890], device='cuda:0')\n",
      "tensor(0.0591, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 21\n",
      "tensor([ 1.0849,  0.2525,  0.0506,  0.1361,  0.1585,  0.4737, -0.4426,  0.3116,\n",
      "         0.1713,  0.1235], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.0049, 0.0049, 0.1134, 0.6537, 0.4364, 0.0083, 0.2072, 0.0508,\n",
      "        0.4763], device='cuda:0')\n",
      "tensor(0.0600, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 22\n",
      "tensor([ 1.0750,  0.2628,  0.0515,  0.1460,  0.1630,  0.4704, -0.4534,  0.3007,\n",
      "         0.1543,  0.1450], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.0047, 0.0047, 0.1164, 0.6399, 0.4469, 0.0080, 0.2116, 0.0520,\n",
      "        0.4634], device='cuda:0')\n",
      "tensor(0.0587, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 23\n",
      "tensor([ 1.0811,  0.2264,  0.0422,  0.1275,  0.1718,  0.4738, -0.4399,  0.3057,\n",
      "         0.1621,  0.1280], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.0056, 0.0045, 0.1197, 0.6181, 0.4595, 0.0077, 0.2187, 0.0534,\n",
      "        0.4501], device='cuda:0')\n",
      "tensor(0.0554, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 24\n",
      "tensor([ 1.0793,  0.2350,  0.0484,  0.1435,  0.1719,  0.4701, -0.4429,  0.2914,\n",
      "         0.1545,  0.1542], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.0057, 0.0043, 0.1232, 0.5916, 0.4730, 0.0074, 0.2287, 0.0548,\n",
      "        0.4372], device='cuda:0')\n",
      "tensor(0.0543, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 25\n",
      "tensor([ 1.0777,  0.2049,  0.0337,  0.1185,  0.1841,  0.4776, -0.4317,  0.3016,\n",
      "         0.1605,  0.1334], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.0059, 0.0041, 0.1270, 0.5656, 0.4871, 0.0071, 0.2418, 0.0563,\n",
      "        0.4247], device='cuda:0')\n",
      "tensor(0.0532, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 26\n",
      "tensor([ 1.0801,  0.2147,  0.0442,  0.1381,  0.1806,  0.4719, -0.4353,  0.2859,\n",
      "         0.1493,  0.1575], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.0060, 0.0040, 0.1303, 0.5455, 0.4992, 0.0069, 0.2505, 0.0577,\n",
      "        0.4136], device='cuda:0')\n",
      "tensor(0.0521, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 27\n",
      "tensor([ 1.0754,  0.1842,  0.0263,  0.1135,  0.1914,  0.4800, -0.4273,  0.2967,\n",
      "         0.1586,  0.1437], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.0061, 0.0038, 0.1329, 0.5337, 0.5085, 0.0067, 0.2554, 0.0587,\n",
      "        0.4056], device='cuda:0')\n",
      "tensor(0.0511, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 28\n",
      "tensor([ 1.0797,  0.1988,  0.0419,  0.1325,  0.1926,  0.4781, -0.4260,  0.2836,\n",
      "         0.1449,  0.1563], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.0062, 0.0037, 0.1345, 0.5276, 0.5142, 0.0065, 0.2559, 0.0593,\n",
      "        0.4038], device='cuda:0')\n",
      "tensor(0.0501, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 29\n",
      "tensor([ 1.0735,  0.1643,  0.0184,  0.1098,  0.1947,  0.4826, -0.4231,  0.2911,\n",
      "         0.1565,  0.1559], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.0062, 0.0037, 0.1352, 0.5237, 0.5169, 0.0064, 0.2515, 0.0594,\n",
      "        0.4071], device='cuda:0')\n",
      "tensor(0.0491, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 30\n",
      "tensor([ 1.0789,  0.1838,  0.0382,  0.1266,  0.2036,  0.4845, -0.4186,  0.2807,\n",
      "         0.1427,  0.1577], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 6.1195e-03, 5.6380e-04, 3.6229e-03, 5.1774e-01, 5.1575e-01,\n",
      "        6.3003e-03, 2.4642e-01, 5.9245e-02, 4.1549e-01], device='cuda:0')\n",
      "tensor(0.0499, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 31\n",
      "tensor([ 1.0744,  0.1497,  0.0126,  0.1067,  0.1932,  0.4869, -0.4112,  0.2935,\n",
      "         0.1556,  0.1736], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 6.0791e-03, 5.6011e-04, 3.5800e-03, 5.1211e-01, 3.5800e-03,\n",
      "        6.1732e-03, 2.4133e-01, 3.5800e-03, 4.2776e-01], device='cuda:0')\n",
      "tensor(0.0556, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 32\n",
      "tensor([ 1.0819,  0.1598,  0.0315,  0.1086,  0.2038,  0.4633, -0.4110,  0.2741,\n",
      "         0.1488,  0.1738], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 6.0416e-03, 5.5662e-04, 2.5324e-03, 5.1147e-01, 3.5363e-03,\n",
      "        6.0126e-03, 2.3640e-01, 3.5363e-03, 4.3544e-01], device='cuda:0')\n",
      "tensor(0.0507, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 33\n",
      "tensor([ 1.0735,  0.1349,  0.0164,  0.0937,  0.1982,  0.4439, -0.4008,  0.2790,\n",
      "         0.1453,  0.1824], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 6.0038e-03, 5.5313e-04, 2.5167e-03, 5.1255e-01, 3.4935e-03,\n",
      "        5.8653e-03, 2.3157e-01, 3.4935e-03, 4.4194e-01], device='cuda:0')\n",
      "tensor(0.0496, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 34\n",
      "tensor([ 1.0812,  0.1444,  0.0265,  0.0934,  0.2112,  0.4283, -0.4039,  0.2673,\n",
      "         0.1393,  0.1807], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000e+00, 5.9665e-03, 5.4970e-04, 2.5009e-03, 5.1188e-01, 3.4509e-03,\n",
      "        5.7211e-03, 2.2686e-01, 3.4509e-03, 4.4569e-01], device='cuda:0')\n",
      "tensor(0.0486, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 35\n",
      "tensor([ 1.0716,  0.1225,  0.0157,  0.0834,  0.2029,  0.4087, -0.3904,  0.2762,\n",
      "         0.1404,  0.1910], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2510, 0.0245, 0.1124, 0.0067, 1.0000, 0.6109, 0.0012, 1.0000,\n",
      "        0.0077], device='cuda:0')\n",
      "tensor(0.0516, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 36\n",
      "tensor([ 1.0777,  0.1407,  0.0254,  0.0872,  0.2129,  0.4375, -0.4014,  0.2537,\n",
      "         0.1683,  0.1748], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2141, 0.4038, 0.5107, 0.0013, 0.0604, 0.0799, 0.0058, 0.0604,\n",
      "        0.0015], device='cuda:0')\n",
      "tensor(0.0468, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 37\n",
      "tensor([ 1.0704,  0.1312,  0.0412,  0.0980,  0.1955,  0.4110, -0.3678,  0.2577,\n",
      "         0.1685,  0.1845], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2141, 0.4038, 0.5107, 0.0013, 0.0602, 0.0786, 0.0059, 0.0602,\n",
      "        0.0015], device='cuda:0')\n",
      "tensor(0.0459, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 38\n",
      "tensor([ 1.0784,  0.1451,  0.0699,  0.1089,  0.2073,  0.4144, -0.3943,  0.2294,\n",
      "         0.1648,  0.1672], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2141, 0.4038, 0.5107, 0.0013, 0.0599, 0.0777, 0.0059, 0.0599,\n",
      "        0.0015], device='cuda:0')\n",
      "tensor(0.0450, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 39\n",
      "tensor([ 1.0672,  0.1363,  0.0793,  0.1241,  0.1910,  0.3883, -0.3587,  0.2432,\n",
      "         0.1692,  0.1735], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2141, 0.4038, 0.5106, 0.0013, 0.0596, 0.0767, 0.0058, 0.0596,\n",
      "        0.0014], device='cuda:0')\n",
      "tensor(0.0441, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 40\n",
      "tensor([ 1.0779,  0.1492,  0.1013,  0.1284,  0.2018,  0.3929, -0.3856,  0.2100,\n",
      "         0.1633,  0.1595], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2141, 0.4038, 0.5107, 0.0013, 0.0593, 0.0759, 0.0058, 0.0593,\n",
      "        0.0014], device='cuda:0')\n",
      "tensor(0.0433, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 41\n",
      "tensor([ 1.0652,  0.1409,  0.1095,  0.1443,  0.1878,  0.3699, -0.3520,  0.2271,\n",
      "         0.1677,  0.1622], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2141, 0.4038, 0.5106, 0.0013, 0.0590, 0.0757, 0.0058, 0.0590,\n",
      "        0.0014], device='cuda:0')\n",
      "tensor(0.0425, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0774,  0.1534,  0.1262,  0.1451,  0.1972,  0.3733, -0.3765,  0.1934,\n",
      "         0.1627,  0.1520], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2141, 0.4038, 0.5106, 0.0013, 0.1592, 0.0759, 0.0057, 0.0587,\n",
      "        0.0014], device='cuda:0')\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 43\n",
      "tensor([ 1.0620,  0.1483,  0.1365,  0.1637,  0.1844,  0.3555, -0.3439,  0.2145,\n",
      "         0.1632,  0.1490], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2141, 0.4038, 0.5107, 0.0013, 0.1592, 0.0763, 0.0057, 0.0584,\n",
      "        0.0013], device='cuda:0')\n",
      "tensor(0.0388, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 44\n",
      "tensor([ 1.0755,  0.1588,  0.1465,  0.1594,  0.1933,  0.3616, -0.3673,  0.1797,\n",
      "         0.1617,  0.1427], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2140, 0.4038, 0.5106, 0.0013, 0.1592, 0.0765, 0.0056, 0.0582,\n",
      "        0.0013], device='cuda:0')\n",
      "tensor(0.0381, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 45\n",
      "tensor([ 1.0614,  0.1518,  0.1594,  0.1802,  0.1801,  0.3439, -0.3369,  0.1989,\n",
      "         0.1601,  0.1388], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2141, 0.4038, 0.5107, 0.0013, 0.1592, 0.0768, 0.0056, 0.0579,\n",
      "        0.0013], device='cuda:0')\n",
      "tensor(0.0374, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 46\n",
      "tensor([ 1.0727,  0.1635,  0.1650,  0.1734,  0.1894,  0.3510, -0.3594,  0.1678,\n",
      "         0.1608,  0.1343], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2140, 0.4038, 0.5107, 0.0014, 0.1592, 0.0771, 0.0055, 0.0576,\n",
      "        0.0013], device='cuda:0')\n",
      "tensor(0.0367, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 47\n",
      "tensor([ 1.0615,  0.1553,  0.1788,  0.1946,  0.1759,  0.3323, -0.3287,  0.1839,\n",
      "         0.1569,  0.1295], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2141, 0.4038, 0.5107, 0.0014, 0.1592, 0.0774, 0.0055, 0.0573,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0361, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 48\n",
      "tensor([ 1.0693,  0.1673,  0.1829,  0.1866,  0.1872,  0.3407, -0.3535,  0.1560,\n",
      "         0.1583,  0.1262], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2144, 0.4038, 0.5102, 0.0014, 0.1592, 0.0777, 0.0055, 0.0570,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0354, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 49\n",
      "tensor([ 1.0624,  0.1597,  0.1941,  0.2067,  0.1700,  0.3218, -0.3185,  0.1711,\n",
      "         0.1558,  0.1212], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2151, 0.4038, 0.5100, 0.0014, 0.1592, 0.0780, 0.0054, 0.0568,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0386, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 50\n",
      "tensor([ 1.0670,  0.1624,  0.2022,  0.1990,  0.1834,  0.3299, -0.3507,  0.1438,\n",
      "         0.1542,  0.1185], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2158, 0.4038, 0.5097, 0.0015, 0.1592, 0.0782, 0.0054, 0.0565,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0378, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 51\n",
      "tensor([ 1.0648,  0.1608,  0.2051,  0.2169,  0.1623,  0.3125, -0.3070,  0.1618,\n",
      "         0.1565,  0.1153], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2164, 0.4038, 0.5094, 0.0015, 0.1593, 0.0785, 0.0562, 0.0562,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0412, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 52\n",
      "tensor([ 1.0607,  0.1631,  0.2199,  0.2108,  0.1860,  0.3199, -0.3509,  0.1362,\n",
      "         0.1460,  0.1083], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2168, 0.4040, 0.5090, 0.0015, 0.1592, 0.0788, 0.4087, 0.0560,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0365, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 53\n",
      "tensor([ 1.0627,  0.1642,  0.2169,  0.2256,  0.1585,  0.3061, -0.2996,  0.1633,\n",
      "         0.1552,  0.1109], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2176, 0.4042, 0.5094, 0.0015, 0.1592, 0.0791, 0.4087, 0.3705,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0331, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 54\n",
      "tensor([ 1.0567,  0.1701,  0.2316,  0.2237,  0.1759,  0.3057, -0.3377,  0.1577,\n",
      "         0.1579,  0.1032], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2182, 0.4032, 0.5107, 0.0015, 0.1592, 0.0794, 0.4091, 0.3705,\n",
      "        0.0011], device='cuda:0')\n",
      "tensor(0.0324, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 55\n",
      "tensor([ 1.0581,  0.1691,  0.2297,  0.2359,  0.1563,  0.2998, -0.2944,  0.1774,\n",
      "         0.1724,  0.1064], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2188, 0.4024, 0.5119, 0.0015, 0.1592, 0.0797, 0.4091, 0.3710,\n",
      "        0.0011], device='cuda:0')\n",
      "tensor(0.0290, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 56\n",
      "tensor([ 1.0527,  0.1808,  0.2424,  0.2372,  0.1717,  0.2949, -0.3267,  0.1736,\n",
      "         0.1775,  0.0954], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2194, 0.4008, 0.5130, 0.0015, 0.1592, 0.0800, 0.4095, 0.3709,\n",
      "        0.0011], device='cuda:0')\n",
      "tensor(0.0363, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 57\n",
      "tensor([ 1.0537,  0.1717,  0.2389,  0.2464,  0.1525,  0.2894, -0.2973,  0.1852,\n",
      "         0.1859,  0.1027], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2200, 0.3993, 0.5141, 0.0015, 0.1592, 0.0803, 0.4093, 0.3710,\n",
      "        0.0011], device='cuda:0')\n",
      "tensor(0.0354, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 58\n",
      "tensor([ 1.0548,  0.1815,  0.2526,  0.2493,  0.1649,  0.2825, -0.3216,  0.1867,\n",
      "         0.1920,  0.0855], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2204, 0.3978, 0.5150, 0.0015, 0.1593, 0.0805, 0.4101, 0.3715,\n",
      "        0.0011], device='cuda:0')\n",
      "tensor(0.0346, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 59\n",
      "tensor([ 1.0493,  0.1768,  0.2487,  0.2552,  0.1496,  0.2840, -0.2909,  0.1947,\n",
      "         0.2000,  0.0954], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2206, 0.3968, 0.5164, 0.0015, 0.1595, 0.0809, 0.4109, 0.3715,\n",
      "        0.0011], device='cuda:0')\n",
      "tensor(0.0338, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 60\n",
      "tensor([ 1.0549,  0.1844,  0.2623,  0.2602,  0.1598,  0.2708, -0.3150,  0.1985,\n",
      "         0.2081,  0.0798], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2207, 0.3962, 0.5165, 0.0015, 0.1592, 0.0810, 0.4104, 0.3726,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0331, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 61\n",
      "tensor([ 1.0457,  0.1808,  0.2578,  0.2636,  0.1463,  0.2786, -0.2846,  0.2057,\n",
      "         0.2136,  0.0880], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2209, 0.3953, 0.5157, 0.0533, 0.1584, 0.0810, 0.4087, 0.3741,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0363, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 62\n",
      "tensor([ 1.0531,  0.1928,  0.2699,  0.2724,  0.1530,  0.2626, -0.3102,  0.2066,\n",
      "         0.2240,  0.0799], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2203, 0.3962, 0.5143, 0.0527, 0.1573, 0.0809, 0.4062, 0.3763,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0355, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 63\n",
      "tensor([ 1.0424,  0.1863,  0.2668,  0.2746,  0.1450,  0.2741, -0.2812,  0.2161,\n",
      "         0.2272,  0.0826], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2203, 0.3962, 0.5143, 0.0522, 0.1566, 0.0810, 0.4047, 0.3774,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0347, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 64\n",
      "tensor([ 1.0530,  0.1951,  0.2787,  0.2802,  0.1491,  0.2534, -0.3016,  0.2174,\n",
      "         0.2382,  0.0758], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2203, 0.3962, 0.5143, 0.0517, 0.1559, 0.0811, 0.4031, 0.3790,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0339, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 65\n",
      "tensor([ 1.0396,  0.1898,  0.2742,  0.2843,  0.1440,  0.2685, -0.2765,  0.2248,\n",
      "         0.2401,  0.0765], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2203, 0.3962, 0.5144, 0.0512, 0.1552, 0.0813, 0.4016, 0.3799,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0373, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 66\n",
      "tensor([ 1.0480,  0.1949,  0.2878,  0.2903,  0.1457,  0.2446, -0.2957,  0.2280,\n",
      "         0.2540,  0.0679], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2203, 0.3962, 0.5144, 0.0508, 0.1546, 0.0814, 0.3997, 0.3813,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0407, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0376,  0.1888,  0.2824,  0.2987,  0.1405,  0.2657, -0.2616,  0.2357,\n",
      "         0.2562,  0.0675], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2203, 0.3962, 0.5143, 0.0503, 0.1543, 0.0815, 0.3986, 0.3826,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0440, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 68\n",
      "tensor([ 1.0436,  0.1934,  0.2956,  0.2976,  0.1434,  0.2422, -0.2849,  0.2351,\n",
      "         0.2663,  0.0630], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2204, 0.3962, 0.5143, 0.0683, 0.1539, 0.0816, 0.3972, 0.3840,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0410, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 69\n",
      "tensor([ 1.0391,  0.1909,  0.2921,  0.3058,  0.1366,  0.2569, -0.2524,  0.2473,\n",
      "         0.2675,  0.0613], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2203, 0.3962, 0.5141, 0.0680, 0.1539, 0.0817, 0.3958, 0.3854,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0376, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 70\n",
      "tensor([ 1.0392,  0.1947,  0.3035,  0.3056,  0.1406,  0.2393, -0.2712,  0.2471,\n",
      "         0.2796,  0.0578], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([1.0000, 0.2201, 0.3962, 0.5138, 0.0677, 0.1537, 0.0818, 0.3944, 0.3868,\n",
      "        0.0012], device='cuda:0')\n",
      "tensor(0.0341, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "timeStepI: 71\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-49c637b6a68c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatrixSequenceInBatchDim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mgenerateAdjacencyMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataInputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-49c637b6a68c>\u001b[0m in \u001b[0;36mgenerateAdjacencyMatrix\u001b[0;34m(batchedPositionTensor, lambdaX, lambdaY, omegaX, omegaY, m)\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0madjacencyMatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0momegaY\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdaX\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchedPositionTensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatchI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeStepI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbatchedPositionTensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatchI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeStepI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                     \u001b[0;34m(\u001b[0m\u001b[0momegaX\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdaY\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchedPositionTensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatchI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeStepI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbatchedPositionTensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatchI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeStepI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchedPositionTensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatchI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeStepI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbatchedPositionTensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatchI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeStepI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                         \u001b[0;31m#if i follows j, then multiple m, m<1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# #The test of whether CNN could be use to fitting a exponential map function.\n",
    "# #result: it does converge, but meaningless\n",
    "# def generateAdjacencyMatrix(batchedPositionTensor,lambdaX,lambdaY,omegaX,omegaY,m):\n",
    "#     \"\"\"\n",
    "#     Using batched position tensor generate batched adjacency matrix\n",
    "#     Args:\n",
    "#         batchedPositionTensor: a batch of position tensor, which size in (batch, timeSequence,2,vehicles), the \n",
    "#         value 2 in dim=2 is the position of x and y. \n",
    "#         lambda1,lambda2,omega1,omega2,m are parameters of the function. m<1\n",
    "#         see detail in my notebook\n",
    "#     Returns:\n",
    "#         a batch of adjacency matrix\n",
    "#     Example:\n",
    "#         if given a batch of combined tensor, named theTensor, which size as below:\n",
    "#             (4,100,6,250)\n",
    "#         which means 4 batches, 100 time step, 6 dimension which respectively of positonx, positony, velocityx, \n",
    "#         velocityy, accx,accy.\n",
    "#         then we apply the function in such way:\n",
    "#         generateAdjacencyMatrix(theTensor(:,:,0:1,:))\n",
    "#     \"\"\"\n",
    "#     class mapAdjacencyMatrixNet(nn.Module):\n",
    "#         def __init__(self, inputSize, outputSize):\n",
    "#             super(mapAdjacencyMatrixNet,self).__init__()\n",
    "#             self.layer1=nn.Linear(inputSize,outputSize)\n",
    "#             self.layer2=nn.Linear(outputSize,inputSize)\n",
    "#             self.outputLayer=nn.Linear(inputSize,outputSize)\n",
    "#         def forward(self,inputs):\n",
    "#             output=self.layer1(inputs)\n",
    "#             output=F.sigmoid(output)\n",
    "#             output=self.layer2(output)\n",
    "#             output=F.sigmoid(output)\n",
    "#             output=self.outputLayer(output)\n",
    "#             return output\n",
    "#     mapNet=mapAdjacencyMatrixNet(250*2,250*250)\n",
    "#     mapNet.cuda()\n",
    "#     mapNet.train()\n",
    "#     loss_MSE = torch.nn.MSELoss()\n",
    "#     loss_L1 = torch.nn.L1Loss()\n",
    "#     learning_rate = 1e-5\n",
    "#     optimizer = torch.optim.RMSprop(mapNet.parameters(), lr = learning_rate)\n",
    "#     print(batchedPositionTensor.size())\n",
    "#     sizeOfEachMatrix=batchedPositionTensor[0,0,0,:].size()[0]\n",
    "#     batchedPositionTensor\n",
    "#     for batchI in range(batchedPositionTensor.size()[0]): #revolve each batch\n",
    "#         print('batchI',batchI)\n",
    "#         for timeStepI in range(batchedPositionTensor.size()[1]):#revolve each time step\n",
    "#             print('timeStepI:',timeStepI)\n",
    "#             adjacencyMatrix=torch.zeros((sizeOfEachMatrix,sizeOfEachMatrix))\n",
    "#             for i in range(sizeOfEachMatrix):\n",
    "#                 for j in range(sizeOfEachMatrix):\n",
    "# #                     adjacencyMatrix[i,j]=1\n",
    "                    \n",
    "#                     #calculate original element \n",
    "#                     adjacencyMatrix[i,j]=\\\n",
    "#                     (omegaY/math.exp(lambdaX*abs(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])))*\\\n",
    "#                     (omegaX/math.exp(lambdaY*abs(batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])))\n",
    "#                     if(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,i]<0):\n",
    "#                         #if i follows j, then multiple m, m<1\n",
    "#                         adjacencyMatrix[i,j]=adjacencyMatrix[i,j]*m\n",
    "#             adjacencyMatrix=adjacencyMatrix.unsqueeze(0)\n",
    "            \n",
    "#             #try fitting the map function with neural network\n",
    "#             inputs=Variable(batchedPositionTensor[batchI,timeStepI,:,:].view(-1)).cuda()\n",
    "#             lables=Variable(adjacencyMatrix.view(-1)).cuda()\n",
    "#             outputs=mapNet(inputs)\n",
    "#             loss=loss_MSE(outputs,lables)\n",
    "#             print(outputs[0:10])\n",
    "#             print(lables[0:10])\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             print(loss)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "#             if timeStepI==0:\n",
    "#                 matrixSequenceInTimeStepDim=adjacencyMatrix\n",
    "#             else:\n",
    "#                 matrixSequenceInTimeStepDim=\\\n",
    "#                 torch.cat((matrixSequenceInTimeStepDim,adjacencyMatrix),0)\n",
    "#         matrixSequenceInTimeStepDim=matrixSequenceInTimeStepDim.unsqueeze(0)\n",
    "#         if batchI==0:\n",
    "#             matrixSequenceInBatchDim=matrixSequenceInTimeStepDim\n",
    "#         else:\n",
    "#             matrixSequenceInBatchDim=torch.cat((matrixSequenceInBatchDim,matrixSequenceInTimeStepDim),0)            \n",
    "#     return matrixSequenceInBatchDim\n",
    "# torch.cuda.empty_cache()\n",
    "# generateAdjacencyMatrix(dataInputs[:,:,0:2,:],5,5,1,1,2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch] *",
   "language": "python",
   "name": "conda-env-.conda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
