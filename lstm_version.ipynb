{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#global variable\n",
    "doNormalization=True\n",
    "useGpu=False\n",
    "runOnG814=False\n",
    "isTest=True\n",
    "modelPath='/home/wangyuchen/wholeNet_300epoch_50perEpoch.pt'\n",
    "maxMatrixIndex=250\n",
    "runRelationLSTM=False\n",
    "runObjectRelationNet=False\n",
    "runSeq2SeqRelationModel=True\n",
    "runLSTM=False\n",
    "\n",
    "maxRelationsNumberGlobal=20 # the maximum nubmer of relation in the \"relation within the given range\" version \n",
    "\n",
    "if useGpu:\n",
    "    import os\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromAllToStr(*args):\n",
    "    returnedStr=str()\n",
    "    for eachItem in args:\n",
    "        returnedStr=returnedStr+str(eachItem)\n",
    "    return returnedStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-24 23:13:53,266 -2 - <module> - DEBUG - 12345test Tensor:tensor([0., 0., 0., 0., 0.])2.000222\n"
     ]
    }
   ],
   "source": [
    "theStr=fromAllToStr(1,2,3,4,5,'test Tensor:',torch.zeros(5),2.000222)\n",
    "logging.debug(theStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='logging.txt',level=logging.DEBUG, format='%(asctime)s -%(lineno)d - %(funcName)s - %(levelname)s - %(message)s',)\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s -%(lineno)d - %(funcName)s - %(levelname)s - %(message)s')\n",
    "console.setFormatter(formatter)\n",
    "logging.getLogger('').addHandler(console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a tuple in which each element is the index of a vehicle\n",
    "#the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "listForEachVehicle=[]\n",
    "for i in range(maxMatrixIndex):\n",
    "    listForEachVehicle.append(i*(maxMatrixIndex-1))\n",
    "tupleForEachVehicle=tuple(listForEachVehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def getValueByLable(lableList,valueList):\n",
    "    \"\"\"\n",
    "    For instance, given a lable list ['Local_X','Local_Y'] and a value list [2.0, 24.0, 437.0, 1118846981300.0, 16.254, \n",
    "    79.349, 6451167.199, 1873312.382, 14.5, 4.9, 2.0, 39.14, -5.73, 2.0, 0.0, 13.0, 0.0, 0.0] which values sorted by the \n",
    "    order of allLableList below, the function return a value Dict {'Local_X':16.254, 'Local_Y':79.349}\n",
    "    Args:\n",
    "        lableList: the list of lables you've required, such as['Vehicle_ID', 'Total_Frames','Global_Time']\n",
    "        valueList: the list contains all legally value, sorted by:['Vehicle_ID', 'Frame_ID','Total_Frames','Global_Time','Local_X','Local_Y','Global_X','Global_Y',\\\n",
    "                      'v_Length','v_Width','v_Class','v_Vel','v_Acc','Lane_ID','Preceding','Following','Space_Headway',\\\n",
    "                      'Time_Headway']\n",
    "    Returns: \n",
    "        value dict of the input lables\n",
    "    For instance, given a lable list ['Local_X','Local_Y'] and a value list [2.0, 24.0, 437.0, 1118846981300.0, 16.254, \n",
    "    79.349, 6451167.199, 1873312.382, 14.5, 4.9, 2.0, 39.14, -5.73, 2.0, 0.0, 13.0, 0.0, 0.0] which values sorted by the \n",
    "    order of allLableList above, the function return a value List [16.254, 79.349]\n",
    "\n",
    "    \"\"\"\n",
    "    allLableList=['Vehicle_ID', 'Frame_ID','Total_Frames','Global_Time','Local_X','Local_Y','Global_X','Global_Y',\\\n",
    "                  'v_Length','v_Width','v_Class','v_Vel','v_Acc','Lane_ID','Preceding','Following','Space_Headway',\\\n",
    "                  'Time_Headway']\n",
    "    valueDictReturn={}\n",
    "    for lableItem in lableList:\n",
    "        valueDictReturn[lableItem]=valueList[allLableList.index(lableItem)]\n",
    "    return valueDictReturn\n",
    "\n",
    "def rearrangeDataByGlobalTime(allValueLists):\n",
    "    '''\n",
    "    Args:\n",
    "        allValueLists: all values have been read from a txt file which have already been converted to a list\n",
    "    Returns:\n",
    "        dict have been arranged by global time. One single global time generally contains several value lists.\n",
    "    '''\n",
    "    valueDict={}\n",
    "    for valueList in allValueLists:\n",
    "        dictKey=getValueByLable(['Global_Time'],valueList)['Global_Time']\n",
    "        if dictKey in valueDict:\n",
    "            # if dictKey already there, then add valueList to the list of the key\n",
    "            valueDict[dictKey].append(valueList)\n",
    "        else:\n",
    "            #else, create a list and append valueList on it\n",
    "            valueDict[dictKey]=[valueList]\n",
    "    return valueDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def readFirstFrame(matrixIndexAndVehicleIDRecordDictParam, valueLists):\n",
    "    \"\"\"\n",
    "    To generate the first set of tensors from the first frame\n",
    "    Args:\n",
    "        matrixIndexAndVehicleIDRecordDictParam: just as its name\n",
    "        valueLists: a list consists of all valuelist at one time\n",
    "    Returns:\n",
    "        several tensors arranged by: positionTensor, speedTensor, accTensor, angleTensor,newVehicleList(type:list)\n",
    "    \n",
    "    \"\"\"\n",
    "    maxMatrixIndex=matrixIndexAndVehicleIDRecordDictParam.keys().__len__()-1\n",
    "    #tensors initialize\n",
    "    positionTensor=torch.zeros(2,maxMatrixIndex)\n",
    "    speedTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    accTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    angleTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    newVehicleIDList=[]\n",
    "    curMatrixIndex=0\n",
    "    matrixIndexAndVehicleIDRecordDictParam['time']=getValueByLable([\"Global_Time\"],valueLists[0])['Global_Time']\n",
    "    #fill out all tensors\n",
    "    for eachValueList in valueLists:\n",
    "        #get values from eachValueList, generate dict\n",
    "        returnedEachValueDict=getValueByLable(['Vehicle_ID','Local_X','Local_Y','v_Vel','v_Acc'],eachValueList)\n",
    "        #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "        #angle Tensor assignment is not neeed for the initial value of each element in it is already zero\n",
    "        positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "        speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "        accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "        #then handle the record matrix\n",
    "        matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['Vehicle_ID']=returnedEachValueDict['Vehicle_ID']\n",
    "        matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['refresh']=0\n",
    "        newVehicleIDList.append(copy.deepcopy(returnedEachValueDict['Vehicle_ID']))\n",
    "        curMatrixIndex=curMatrixIndex+1\n",
    "    return positionTensor,speedTensor,accTensor,angleTensor,newVehicleIDList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMatrixIndexByVehicleID(matrixIndexAndVehicleIDRecordDictParam, vehicle_ID):\n",
    "    for i in range(0, len(matrixIndexAndVehicleIDRecordDictParam)-1):\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']==vehicle_ID:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def findEmptyMatrixIndex(matrixIndexAndVehicleIDRecordDictParam):\n",
    "    for i in range(0, len(matrixIndexAndVehicleIDRecordDictParam)-1):\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']==-1:\n",
    "            #Vehicle_ID=-1 when there is no existed vehicle ID bounding to the index\n",
    "            return i\n",
    "    raise Exception(\"NO EMPTY ELEMENT IN MATRIX\")\n",
    "\n",
    "def readGeneralFrame(matrixIndexAndVehicleIDRecordDictParam, valueLists, prePositionTensor):\n",
    "    \"\"\"\n",
    "    To generate the first set of tensors from the general frame that have a preceding one.\n",
    "    In this version, we ignore the new vehicle appeared among a serial of frame.\n",
    "    Args:\n",
    "        matrixIndexAndVehicleIDRecordDictParam: just as its name\n",
    "        valueLists: a list consists of all valuelist at one time\n",
    "        prePositionTensor: positionTensor from the preceding frame, which is used to calculate angle tensor\n",
    "    Returns:\n",
    "        everal tensors arranged by: positionTensor, speedTensor, accTensor, angleTensor,newVehicleList(type:list),\n",
    "        vanishedVehicleList(type:list)\n",
    "    \n",
    "    \"\"\"\n",
    "    #tensors initialize\n",
    "    maxMatrixIndex=matrixIndexAndVehicleIDRecordDictParam.keys().__len__()-1\n",
    "    positionTensor=torch.zeros(2,maxMatrixIndex)\n",
    "    speedTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    accTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    angleTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    newVehicleIDList=[]\n",
    "    vanishedVehicleList=[]\n",
    "    curMatrixIndex=0\n",
    "    matrixIndexAndVehicleIDRecordDictParam['time']=getValueByLable([\"Global_Time\"],valueLists[0])['Global_Time']\n",
    "    #fill out all tensors\n",
    "    for eachValueList in valueLists:\n",
    "        #get values from eachValueList, generate dict\n",
    "        returnedEachValueDict=getValueByLable(['Vehicle_ID','Local_X','Local_Y','v_Vel','v_Acc'],eachValueList)\n",
    "        indexOfVehicle=findMatrixIndexByVehicleID(matrixIndexAndVehicleIDRecordDictParam,returnedEachValueDict['Vehicle_ID'])\n",
    "        if indexOfVehicle!=-1:\n",
    "        #if index exist then the vehicle already existed in the preceded frame\n",
    "            matrixIndexAndVehicleIDRecordDictParam[indexOfVehicle]['refresh']=1\n",
    "            curMatrixIndex=indexOfVehicle\n",
    "            #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "            positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "            speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "            accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "            angleTensor[:,curMatrixIndex]=math.atan2(positionTensor[0,curMatrixIndex]-\\\n",
    "                                                     prePositionTensor[0,curMatrixIndex],\\\n",
    "                                                    positionTensor[1,curMatrixIndex]-prePositionTensor[1,curMatrixIndex])\n",
    "        else:\n",
    "            pass #ignore new vehicleID\n",
    "        #a new vehicle ID\n",
    "#             newVehicleIDList.append(copy.deepcopy(returnedEachValueDict['Vehicle_ID']))\n",
    "#             curMatrixIndex=findEmptyMatrixIndex(matrixIndexAndVehicleIDRecordDictParam)\n",
    "#             matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['Vehicle_ID']=copy.deepcopy(returnedEachValueDict['Vehicle_ID'])\n",
    "#             matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['refresh']=1\n",
    "#             #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "#             positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "#             speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "#             accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "#             angleTensor[:,curMatrixIndex]=math.atan2(positionTensor[0,curMatrixIndex]-\\\n",
    "#                                                      prePositionTensor[0,curMatrixIndex],\\\n",
    "#                                                     positionTensor[1,curMatrixIndex]-prePositionTensor[1,curMatrixIndex])\n",
    "    for i in range(0,maxMatrixIndex):\n",
    "    #find vanished vehicle and remove from dict\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['refresh']==0:\n",
    "            #if refresh=0 then the corresponding vehicle ID was not found in this frame\n",
    "            vanishedVehicleList.append(copy.deepcopy(matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']))\n",
    "            matrixIndexAndVehicleIDRecordDictParam[i]['refresh']=-1\n",
    "            matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']=-1\n",
    "    \n",
    "    for i in range(0,maxMatrixIndex):\n",
    "    #set all refrshed which equivalent to 1 to 0 to prepare for the next frame\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['refresh']==1:\n",
    "                #if refresh=0 then the corresponding vehicle ID was not found in this frame\n",
    "                matrixIndexAndVehicleIDRecordDictParam[i]['refresh']=0\n",
    "\n",
    "    return positionTensor,speedTensor,accTensor,angleTensor,newVehicleIDList,vanishedVehicleList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-24 22:26:55,893 -276 - wrapper - DEBUG - $HOME=/home/wangyuchen\n",
      "2020-07-24 22:26:55,895 -276 - wrapper - DEBUG - CONFIGDIR=/home/wangyuchen/.config/matplotlib\n",
      "2020-07-24 22:26:55,896 -276 - wrapper - DEBUG - matplotlib data path: /home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/matplotlib/mpl-data\n",
      "2020-07-24 22:26:55,899 -1007 - rc_params_from_file - DEBUG - loaded rc file /home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/matplotlib/mpl-data/matplotlibrc\n",
      "2020-07-24 22:26:55,902 -1644 - <module> - DEBUG - matplotlib version 3.1.3\n",
      "2020-07-24 22:26:55,903 -1645 - <module> - DEBUG - interactive is False\n",
      "2020-07-24 22:26:55,904 -1646 - <module> - DEBUG - platform is linux\n",
      "2020-07-24 22:26:55,904 -1647 - <module> - DEBUG - loaded modules: ['builtins', 'sys', '_frozen_importlib', '_imp', '_warnings', '_thread', '_weakref', '_frozen_importlib_external', '_io', 'marshal', 'posix', 'zipimport', 'encodings', 'codecs', '_codecs', 'encodings.aliases', 'encodings.utf_8', '_signal', '__main__', 'encodings.latin_1', 'io', 'abc', '_weakrefset', '_bootlocale', '_locale', 'site', 'os', 'errno', 'stat', '_stat', 'posixpath', 'genericpath', 'os.path', '_collections_abc', '_sitebuiltins', 'sysconfig', '_sysconfigdata_m_linux_x86_64-linux-gnu', 'types', 'functools', '_functools', 'collections', 'operator', '_operator', 'keyword', 'heapq', '_heapq', 'itertools', 'reprlib', '_collections', 'weakref', 'collections.abc', 'importlib', 'importlib._bootstrap', 'importlib._bootstrap_external', 'warnings', 'importlib.util', 'importlib.abc', 'importlib.machinery', 'contextlib', 'mpl_toolkits', 'runpy', 'pkgutil', 'ipykernel', 'ipykernel._version', 'ipykernel.connect', '__future__', 'json', 'json.decoder', 're', 'enum', 'sre_compile', '_sre', 'sre_parse', 'sre_constants', 'copyreg', 'json.scanner', '_json', 'json.encoder', 'subprocess', 'time', 'signal', '_posixsubprocess', 'select', 'selectors', 'math', 'threading', 'traceback', 'linecache', 'tokenize', 'token', 'IPython', 'IPython.core', 'IPython.core.getipython', 'IPython.core.release', 'IPython.core.application', 'atexit', 'copy', 'glob', 'fnmatch', 'logging', 'string', '_string', 'shutil', 'zlib', 'bz2', '_compression', '_bz2', 'lzma', '_lzma', 'pwd', 'grp', 'traitlets', 'traitlets.traitlets', 'inspect', 'ast', '_ast', 'dis', 'opcode', '_opcode', 'six', 'struct', '_struct', 'traitlets.utils', 'traitlets.utils.getargspec', 'traitlets.utils.importstring', 'ipython_genutils', 'ipython_genutils._version', 'ipython_genutils.py3compat', 'ipython_genutils.encoding', 'locale', 'platform', 'traitlets.utils.sentinel', 'traitlets.utils.bunch', 'traitlets._version', 'traitlets.config', 'traitlets.config.application', 'decorator', 'traitlets.config.configurable', 'traitlets.config.loader', 'argparse', 'textwrap', 'gettext', 'ipython_genutils.path', 'random', 'hashlib', '_hashlib', '_blake2', '_sha3', 'bisect', '_bisect', '_random', 'ipython_genutils.text', 'ipython_genutils.importstring', 'IPython.core.crashhandler', 'pprint', 'IPython.core.ultratb', 'pydoc', 'urllib', 'urllib.parse', 'IPython.core.debugger', 'bdb', 'IPython.utils', 'IPython.utils.PyColorize', 'IPython.utils.coloransi', 'IPython.utils.ipstruct', 'IPython.utils.colorable', 'pygments', 'pygments.util', 'IPython.utils.py3compat', 'IPython.utils.encoding', 'IPython.core.excolors', 'IPython.testing', 'IPython.testing.skipdoctest', 'pdb', 'cmd', 'code', 'codeop', 'IPython.core.display_trap', 'IPython.utils.path', 'IPython.utils.process', 'IPython.utils._process_posix', 'pexpect', 'pexpect.exceptions', 'pexpect.utils', 'pexpect.expect', 'pexpect.pty_spawn', 'pty', 'tty', 'termios', 'ptyprocess', 'ptyprocess.ptyprocess', 'fcntl', 'resource', 'ptyprocess.util', 'pexpect.spawnbase', 'pexpect.run', 'IPython.utils._process_common', 'shlex', 'IPython.utils.decorators', 'IPython.utils.data', 'IPython.utils.terminal', 'IPython.utils.sysinfo', 'IPython.utils._sysinfo', 'IPython.core.profiledir', 'IPython.paths', 'tempfile', 'IPython.utils.importstring', 'IPython.terminal', 'IPython.terminal.embed', 'IPython.core.compilerop', 'IPython.core.magic_arguments', 'IPython.core.error', 'IPython.utils.text', 'pathlib', 'ntpath', 'IPython.core.magic', 'getopt', 'IPython.core.oinspect', 'typing', 'typing.io', 'typing.re', 'IPython.core.page', 'IPython.core.display', 'binascii', 'mimetypes', 'IPython.lib', 'IPython.lib.security', 'getpass', 'IPython.lib.pretty', 'datetime', '_datetime', 'IPython.utils.openpy', 'IPython.utils.dir2', 'IPython.utils.wildcard', 'pygments.lexers', 'pygments.lexers._mapping', 'pygments.modeline', 'pygments.plugin', 'pygments.lexers.python', 'pygments.lexer', 'pygments.filter', 'pygments.filters', 'pygments.token', 'pygments.regexopt', 'pygments.unistring', 'pygments.formatters', 'pygments.formatters._mapping', 'pygments.formatters.html', 'pygments.formatter', 'pygments.styles', 'IPython.core.inputtransformer2', 'IPython.core.interactiveshell', 'pickleshare', 'pickle', '_compat_pickle', '_pickle', 'IPython.core.prefilter', 'IPython.core.autocall', 'IPython.core.macro', 'IPython.core.splitinput', 'IPython.core.alias', 'IPython.core.builtin_trap', 'IPython.core.events', 'backcall', 'backcall.backcall', 'IPython.core.displayhook', 'IPython.core.displaypub', 'IPython.core.extensions', 'IPython.core.formatters', 'IPython.utils.sentinel', 'IPython.core.history', 'sqlite3', 'sqlite3.dbapi2', '_sqlite3', 'IPython.core.logger', 'IPython.core.payload', 'IPython.core.usage', 'IPython.display', 'IPython.lib.display', 'html', 'html.entities', 'IPython.utils.io', 'IPython.utils.capture', 'IPython.utils.strdispatch', 'IPython.core.hooks', 'IPython.utils.syspathcontext', 'IPython.utils.tempdir', 'IPython.utils.contexts', 'IPython.core.async_helpers', 'IPython.terminal.interactiveshell', 'asyncio', 'asyncio.base_events', 'concurrent', 'concurrent.futures', 'concurrent.futures._base', 'concurrent.futures.process', 'queue', 'multiprocessing', 'multiprocessing.context', 'multiprocessing.process', 'multiprocessing.reduction', 'socket', '_socket', 'array', '__mp_main__', 'multiprocessing.connection', '_multiprocessing', 'multiprocessing.util', 'concurrent.futures.thread', 'asyncio.compat', 'asyncio.coroutines', 'asyncio.constants', 'asyncio.events', 'asyncio.base_futures', 'asyncio.log', 'asyncio.futures', 'asyncio.base_tasks', '_asyncio', 'asyncio.tasks', 'asyncio.locks', 'asyncio.protocols', 'asyncio.queues', 'asyncio.streams', 'asyncio.subprocess', 'asyncio.transports', 'asyncio.unix_events', 'asyncio.base_subprocess', 'asyncio.selector_events', 'ssl', 'ipaddress', '_ssl', 'base64', 'asyncio.sslproto', 'prompt_toolkit', 'prompt_toolkit.application', 'prompt_toolkit.application.application', 'prompt_toolkit.buffer', 'prompt_toolkit.application.current', 'prompt_toolkit.eventloop', 'prompt_toolkit.eventloop.async_generator', 'prompt_toolkit.eventloop.utils', 'prompt_toolkit.eventloop.dummy_contextvars', 'prompt_toolkit.eventloop.inputhook', 'prompt_toolkit.utils', 'wcwidth', 'wcwidth.wcwidth', 'wcwidth.table_wide', 'wcwidth.table_zero', 'prompt_toolkit.application.run_in_terminal', 'prompt_toolkit.eventloop.async_context_manager', 'prompt_toolkit.auto_suggest', 'prompt_toolkit.document', 'prompt_toolkit.clipboard', 'prompt_toolkit.clipboard.base', 'prompt_toolkit.selection', 'prompt_toolkit.clipboard.in_memory', 'prompt_toolkit.filters', 'prompt_toolkit.filters.app', 'prompt_toolkit.cache', 'prompt_toolkit.enums', 'prompt_toolkit.filters.base', 'prompt_toolkit.filters.cli', 'prompt_toolkit.filters.utils', 'prompt_toolkit.completion', 'prompt_toolkit.completion.base', 'prompt_toolkit.formatted_text', 'prompt_toolkit.formatted_text.ansi', 'prompt_toolkit.output', 'prompt_toolkit.output.base', 'prompt_toolkit.data_structures', 'prompt_toolkit.styles', 'prompt_toolkit.styles.base', 'prompt_toolkit.styles.defaults', 'prompt_toolkit.styles.named_colors', 'prompt_toolkit.styles.style', 'prompt_toolkit.styles.pygments', 'prompt_toolkit.styles.style_transformation', 'colorsys', 'prompt_toolkit.output.color_depth', 'prompt_toolkit.output.defaults', 'prompt_toolkit.patch_stdout', 'prompt_toolkit.output.vt100', 'prompt_toolkit.formatted_text.base', 'prompt_toolkit.mouse_events', 'prompt_toolkit.formatted_text.html', 'xml', 'xml.dom', 'xml.dom.domreg', 'xml.dom.minidom', 'xml.dom.minicompat', 'xml.dom.xmlbuilder', 'xml.dom.NodeFilter', 'prompt_toolkit.formatted_text.pygments', 'prompt_toolkit.formatted_text.utils', 'prompt_toolkit.completion.filesystem', 'prompt_toolkit.completion.fuzzy_completer', 'prompt_toolkit.completion.word_completer', 'prompt_toolkit.completion.nested', 'prompt_toolkit.history', 'prompt_toolkit.search', 'prompt_toolkit.key_binding', 'prompt_toolkit.key_binding.key_bindings', 'prompt_toolkit.keys', 'prompt_toolkit.key_binding.key_processor', 'prompt_toolkit.key_binding.vi_state', 'prompt_toolkit.validation', 'prompt_toolkit.input', 'prompt_toolkit.input.base', 'prompt_toolkit.input.defaults', 'prompt_toolkit.input.typeahead', 'prompt_toolkit.key_binding.bindings', 'prompt_toolkit.key_binding.bindings.page_navigation', 'prompt_toolkit.key_binding.bindings.scroll', 'prompt_toolkit.key_binding.defaults', 'prompt_toolkit.key_binding.bindings.basic', 'prompt_toolkit.key_binding.bindings.named_commands', 'prompt_toolkit.layout', 'prompt_toolkit.layout.containers', 'prompt_toolkit.layout.controls', 'prompt_toolkit.lexers', 'prompt_toolkit.lexers.base', 'prompt_toolkit.lexers.pygments', 'prompt_toolkit.layout.processors', 'prompt_toolkit.layout.utils', 'prompt_toolkit.layout.dimension', 'prompt_toolkit.layout.margins', 'prompt_toolkit.layout.mouse_handlers', 'prompt_toolkit.layout.screen', 'prompt_toolkit.layout.layout', 'prompt_toolkit.layout.menus', 'prompt_toolkit.key_binding.bindings.completion', 'prompt_toolkit.key_binding.bindings.cpr', 'prompt_toolkit.key_binding.bindings.emacs', 'prompt_toolkit.key_binding.bindings.mouse', 'prompt_toolkit.key_binding.bindings.vi', 'prompt_toolkit.input.vt100_parser', 'prompt_toolkit.input.ansi_escape_sequences', 'prompt_toolkit.key_binding.digraphs', 'prompt_toolkit.key_binding.emacs_state', 'prompt_toolkit.layout.dummy', 'prompt_toolkit.renderer', 'prompt_toolkit.application.dummy', 'prompt_toolkit.shortcuts', 'prompt_toolkit.shortcuts.dialogs', 'prompt_toolkit.key_binding.bindings.focus', 'prompt_toolkit.widgets', 'prompt_toolkit.widgets.base', 'prompt_toolkit.widgets.toolbars', 'prompt_toolkit.widgets.dialogs', 'prompt_toolkit.widgets.menus', 'prompt_toolkit.shortcuts.progress_bar', 'prompt_toolkit.shortcuts.progress_bar.base', 'prompt_toolkit.shortcuts.progress_bar.formatters', 'prompt_toolkit.shortcuts.prompt', 'prompt_toolkit.key_binding.bindings.auto_suggest', 'prompt_toolkit.key_binding.bindings.open_in_editor', 'prompt_toolkit.shortcuts.utils', 'pygments.style', 'IPython.terminal.debugger', 'IPython.core.completer', 'unicodedata', 'IPython.core.latex_symbols', 'IPython.utils.generics', 'jedi', 'jedi.api', 'parso', 'parso.parser', 'parso.tree', 'parso._compatibility', 'parso.utils', 'parso.pgen2', 'parso.pgen2.generator', 'parso.pgen2.grammar_parser', 'parso.python', 'parso.python.tokenize', 'parso.python.token', 'parso.grammar', 'parso.python.diff', 'difflib', 'parso.python.parser', 'parso.python.tree', 'parso.python.prefix', 'parso.cache', 'gc', 'parso.python.errors', 'parso.normalizer', 'parso.python.pep8', 'parso.file_io', 'jedi._compatibility', 'jedi.file_io', 'jedi.parser_utils', 'jedi.debug', 'jedi.settings', 'jedi.cache', 'jedi.api.classes', 'jedi.inference', 'jedi.inference.imports', 'jedi.inference.sys_path', 'jedi.inference.cache', 'jedi.inference.base_value', 'jedi.common', 'jedi.common.value', 'jedi.inference.helpers', 'jedi.inference.utils', 'jedi.common.utils', 'jedi.inference.compiled', 'jedi.inference.compiled.value', 'jedi.inference.filters', 'jedi.inference.flow_analysis', 'jedi.inference.recursion', 'jedi.inference.names', 'jedi.inference.docstrings', 'jedi.inference.lazy_value', 'jedi.plugins', 'jedi.inference.compiled.access', 'jedi.inference.compiled.getattr_static', 'jedi.inference.signature', 'jedi.inference.context', 'jedi.inference.analysis', 'jedi.inference.gradual', 'jedi.inference.gradual.typeshed', 'jedi.inference.gradual.stub_value', 'jedi.inference.value', 'jedi.inference.value.module', 'jedi.inference.value.klass', 'jedi.inference.arguments', 'jedi.inference.value.iterable', 'jedi.inference.value.dynamic_arrays', 'jedi.inference.value.function', 'jedi.inference.parser_cache', 'jedi.inference.gradual.generics', 'jedi.inference.value.instance', 'jedi.inference.gradual.typing', 'jedi.inference.gradual.base', 'jedi.inference.gradual.type_var', 'jedi.inference.syntax_tree', 'jedi.inference.gradual.annotation', 'jedi.inference.param', 'jedi.inference.value.decorator', 'jedi.inference.gradual.conversion', 'jedi.api.keywords', 'pydoc_data', 'pydoc_data.topics', 'jedi.api.completion_cache', 'jedi.api.helpers', 'jedi.api.interpreter', 'jedi.inference.compiled.mixed', 'jedi.api.completion', 'jedi.api.strings', 'jedi.api.file_name', 'jedi.api.environment', 'filecmp', 'jedi.inference.compiled.subprocess', 'jedi.inference.compiled.subprocess.functions', 'jedi.api.exceptions', 'jedi.api.project', 'jedi.inference.references', 'jedi.inference.gradual.utils', 'jedi.plugins.registry', 'jedi.plugins.stdlib', 'jedi.plugins.flask', 'jedi.plugins.pytest', 'IPython.terminal.ptutils', 'IPython.terminal.shortcuts', 'IPython.terminal.magics', 'IPython.lib.clipboard', 'IPython.terminal.pt_inputhooks', 'IPython.terminal.prompts', 'IPython.terminal.ipapp', 'IPython.core.magics', 'IPython.core.magics.auto', 'IPython.core.magics.basic', 'IPython.core.magics.code', 'urllib.request', 'email', 'http', 'http.client', 'email.parser', 'email.feedparser', 'email.errors', 'email._policybase', 'email.header', 'email.quoprimime', 'email.base64mime', 'email.charset', 'email.encoders', 'quopri', 'email.utils', 'email._parseaddr', 'calendar', 'email.message', 'uu', 'email._encoded_words', 'email.iterators', 'urllib.error', 'urllib.response', 'IPython.core.magics.config', 'IPython.core.magics.display', 'IPython.core.magics.execution', 'timeit', 'cProfile', '_lsprof', 'profile', 'optparse', 'pstats', 'IPython.utils.module_paths', 'IPython.utils.timing', 'IPython.core.magics.extension', 'IPython.core.magics.history', 'IPython.core.magics.logging', 'IPython.core.magics.namespace', 'IPython.core.magics.osm', 'IPython.core.magics.packaging', 'IPython.core.magics.pylab', 'IPython.core.pylabtools', 'IPython.core.magics.script', 'IPython.lib.backgroundjobs', 'IPython.core.shellapp', 'IPython.extensions', 'IPython.extensions.storemagic', 'IPython.utils.frame', 'jupyter_client', 'jupyter_client._version', 'jupyter_client.connect', 'zmq', 'ctypes', '_ctypes', 'ctypes._endian', 'zmq.backend', 'zmq.backend.select', 'zmq.backend.cython', 'zmq.backend.cython.constants', 'cython_runtime', 'zmq.backend.cython.error', '_cython_0_29_14', 'zmq.backend.cython.message', 'zmq.error', 'zmq.backend.cython.context', 'zmq.backend.cython.socket', 'zmq.backend.cython.utils', 'zmq.backend.cython._poll', 'zmq.backend.cython._version', 'zmq.backend.cython._device', 'zmq.backend.cython._proxy_steerable', 'zmq.sugar', 'zmq.sugar.constants', 'zmq.utils', 'zmq.utils.constant_names', 'zmq.sugar.context', 'zmq.sugar.attrsettr', 'zmq.sugar.socket', 'zmq.sugar.poll', 'zmq.utils.jsonapi', 'zmq.utils.strtypes', 'zmq.sugar.frame', 'zmq.sugar.tracker', 'zmq.sugar.version', 'zmq.sugar.stopwatch', 'jupyter_client.localinterfaces', 'jupyter_core', 'jupyter_core.version', 'jupyter_core.paths', 'jupyter_client.launcher', 'traitlets.log', 'jupyter_client.client', 'jupyter_client.channels', 'jupyter_client.channelsabc', 'jupyter_client.clientabc', 'jupyter_client.manager', 'jupyter_client.kernelspec', 'jupyter_client.managerabc', 'jupyter_client.blocking', 'jupyter_client.blocking.client', 'jupyter_client.blocking.channels', 'jupyter_client.multikernelmanager', 'uuid', 'ctypes.util', 'ipykernel.kernelapp', 'tornado', 'tornado.ioloop', 'numbers', 'tornado.concurrent', 'tornado.log', 'logging.handlers', 'tornado.escape', 'tornado.util', 'tornado.speedups', 'curses', '_curses', 'zmq.eventloop', 'zmq.eventloop.ioloop', 'tornado.platform', 'tornado.platform.asyncio', 'tornado.gen', 'zmq.eventloop.zmqstream', 'ipykernel.iostream', 'imp', 'jupyter_client.session', 'hmac', 'jupyter_client.jsonutil', 'dateutil', 'dateutil._version', 'dateutil.parser', 'dateutil.parser._parser', 'decimal', '_decimal', 'dateutil.relativedelta', 'dateutil._common', 'dateutil.tz', 'dateutil.tz.tz', 'six.moves', 'dateutil.tz._common', 'dateutil.tz._factories', 'dateutil.parser.isoparser', '_strptime', 'jupyter_client.adapter', 'ipykernel.heartbeat', 'ipykernel.ipkernel', 'IPython.utils.tokenutil', 'ipykernel.comm', 'ipykernel.comm.manager', 'ipykernel.comm.comm', 'ipykernel.kernelbase', 'tornado.queues', 'tornado.locks', 'ipykernel.jsonutil', 'ipykernel.zmqshell', 'IPython.core.payloadpage', 'ipykernel.displayhook', 'ipykernel.eventloops', 'distutils', 'distutils.version', 'ipykernel.parentpoller', 'faulthandler', 'ipykernel.datapub', 'ipykernel.serialize', 'ipykernel.pickleutil', 'ipykernel.codeutil', 'IPython.core.completerlib', 'storemagic', 'numpy', 'numpy._globals', 'numpy.__config__', 'numpy.version', 'numpy._distributor_init', 'mkl', 'mkl._mklinit', 'mkl._py_mkl_service', 'numpy.core', 'numpy.core.multiarray', 'numpy.core.overrides', 'numpy.core._multiarray_umath', 'numpy.compat', 'numpy.compat._inspect', 'numpy.compat.py3k', 'numpy.core.umath', 'numpy.core.numerictypes', 'numpy.core._string_helpers', 'numpy.core._type_aliases', 'numpy.core._dtype', 'numpy.core.numeric', 'numpy.core.shape_base', 'numpy.core._asarray', 'numpy.core.fromnumeric', 'numpy.core._methods', 'numpy.core._exceptions', 'numpy.core._ufunc_config', 'numpy.core.arrayprint', 'numpy.core.defchararray', 'numpy.core.records', 'numpy.core.memmap', 'numpy.core.function_base', 'numpy.core.machar', 'numpy.core.getlimits', 'numpy.core.einsumfunc', 'numpy.core._add_newdocs', 'numpy.core._multiarray_tests', 'numpy.core._dtype_ctypes', 'numpy.core._internal', 'numpy._pytesttester', 'numpy.lib', 'numpy.lib.mixins', 'numpy.lib.scimath', 'numpy.lib.type_check', 'numpy.lib.ufunclike', 'numpy.lib.index_tricks', 'numpy.matrixlib', 'numpy.matrixlib.defmatrix', 'numpy.linalg', 'numpy.linalg.linalg', 'numpy.lib.twodim_base', 'numpy.linalg.lapack_lite', 'numpy.linalg._umath_linalg', 'numpy.lib.function_base', 'numpy.lib.histograms', 'numpy.lib.stride_tricks', 'numpy.lib.nanfunctions', 'numpy.lib.shape_base', 'numpy.lib.polynomial', 'numpy.lib.utils', 'numpy.lib.arraysetops', 'numpy.lib.npyio', 'numpy.lib.format', 'numpy.lib._datasource', 'numpy.lib._iotools', 'numpy.lib.financial', 'numpy.lib.arrayterator', 'numpy.lib.arraypad', 'numpy.lib._version', 'numpy.fft', 'numpy.fft._pocketfft', 'numpy.fft._pocketfft_internal', 'numpy.fft.helper', 'numpy.polynomial', 'numpy.polynomial.polynomial', 'numpy.polynomial.polyutils', 'numpy.polynomial._polybase', 'numpy.polynomial.chebyshev', 'numpy.polynomial.legendre', 'numpy.polynomial.hermite', 'numpy.polynomial.hermite_e', 'numpy.polynomial.laguerre', 'numpy.random', 'numpy.random._pickle', 'numpy.random.mtrand', 'numpy.random._bit_generator', 'numpy.random._common', 'secrets', 'numpy.random._bounded_integers', 'numpy.random._mt19937', 'numpy.random._philox', 'numpy.random._pcg64', 'numpy.random._sfc64', 'numpy.random._generator', 'numpy.ctypeslib', 'numpy.ma', 'numpy.ma.core', 'numpy.ma.extras', 'numpy.testing', 'unittest', 'unittest.result', 'unittest.util', 'unittest.case', 'unittest.suite', 'unittest.loader', 'unittest.main', 'unittest.runner', 'unittest.signals', 'numpy.testing._private', 'numpy.testing._private.utils', 'numpy.testing._private.decorators', 'numpy.testing._private.nosetester', 'torch', 'torch._utils', 'torch._utils_internal', 'torch.version', 'torch._six', 'torch._C._onnx', 'torch._C._jit_tree_views', 'torch._C.cpp', 'torch._C.cpp.nn', 'torch._C', 'torch.random', 'torch.serialization', 'tarfile', 'torch._tensor_str', 'torch.tensor', 'torch._namedtensor_internals', 'torch.utils', 'torch.utils.throughput_benchmark', 'torch.utils.hooks', 'torch.storage', 'torch.cuda', 'torch.cuda._utils', 'torch.cuda.memory', 'torch.cuda.random', 'torch.cuda.sparse', 'torch.cuda.profiler', 'torch.cuda.nvtx', 'torch.cuda.streams', 'torch.sparse', 'torch.functional', 'torch.nn', 'torch.nn.modules', 'torch.nn.modules.module', 'torch.nn.parameter', 'torch.nn.modules.linear', 'torch.nn.functional', 'torch.nn._reduction', 'torch.nn.modules.utils', 'torch.nn.grad', 'torch.nn._VF', 'torch._jit_internal', 'torch.nn.init', 'torch.nn.modules.conv', 'torch.nn.modules.activation', 'torch.nn.modules.loss', 'torch.nn.modules.container', 'torch.nn.modules.pooling', 'torch.nn.modules.batchnorm', 'torch.nn.modules._functions', 'torch.autograd', 'torch.autograd.variable', 'torch.autograd.function', 'torch.autograd.gradcheck', 'torch.testing', 'torch.autograd.grad_mode', 'torch.autograd.anomaly_mode', 'torch.autograd.profiler', 'torch.nn.modules.instancenorm', 'torch.nn.modules.normalization', 'torch.nn.modules.dropout', 'torch.nn.modules.padding', 'torch.nn.modules.sparse', 'torch.nn.modules.rnn', 'torch.nn.utils', 'torch.nn.utils.rnn', 'torch.nn.utils.clip_grad', 'torch.nn.utils.weight_norm', 'torch.nn.utils.convert_parameters', 'torch.nn.utils.spectral_norm', 'torch.nn.utils.fusion', 'torch.nn.modules.pixelshuffle', 'torch.nn.modules.upsampling', 'torch.nn.modules.distance', 'torch.nn.modules.fold', 'torch.nn.modules.adaptive', 'torch.nn.modules.transformer', 'torch.nn.modules.flatten', 'torch.nn.parallel', 'torch.nn.parallel.parallel_apply', 'torch.nn.parallel.replicate', 'torch.cuda.comm', 'torch.cuda.nccl', 'torch.nn.parallel.data_parallel', 'torch.nn.parallel.scatter_gather', 'torch.nn.parallel._functions', 'torch.nn.parallel.distributed', 'torch.distributed', 'torch.distributed.distributed_c10d', 'torch.distributed.rendezvous', 'torch.nn.intrinsic', 'torch.nn.intrinsic.modules', 'torch.nn.intrinsic.modules.fused', 'torch.nn.quantized', 'torch.nn.quantized.modules', 'torch.nn.quantized.modules.activation', 'torch.nn.quantized.functional', 'torch.nn.quantized.modules.conv', 'torch.nn.intrinsic.qat', 'torch.nn.intrinsic.qat.modules', 'torch.nn.intrinsic.qat.modules.linear_relu', 'torch.nn.qat', 'torch.nn.qat.modules', 'torch.nn.qat.modules.linear', 'torch.nn.qat.modules.conv', 'torch.nn.intrinsic.qat.modules.conv_fused', 'torch._ops', 'torch.jit', 'torch.backends', 'torch.backends.cudnn', 'torch.jit.annotations', 'torch.jit._recursive', 'torch.jit.frontend', 'torch.nn.quantized.modules.utils', 'torch.nn.quantized.modules.linear', 'torch.nn.quantized.modules.functional_modules', 'torch.optim', 'torch.optim.adadelta', 'torch.optim.optimizer', 'torch.optim.adagrad', 'torch.optim.adam', 'torch.optim.adamw', 'torch.optim.sparse_adam', 'torch.optim.adamax', 'torch.optim.asgd', 'torch.optim.sgd', 'torch.optim.rprop', 'torch.optim.rmsprop', 'torch.optim.lbfgs', 'torch.optim.lr_scheduler', 'torch.multiprocessing', 'torch.multiprocessing.reductions', 'multiprocessing.resource_sharer', 'torch.multiprocessing.spawn', 'torch.utils.backcompat', 'torch.onnx', 'torch.hub', 'zipfile', 'torch.distributions', 'torch.distributions.bernoulli', 'torch.distributions.constraints', 'torch.distributions.exp_family', 'torch.distributions.distribution', 'torch.distributions.utils', 'torch.distributions.beta', 'torch.distributions.dirichlet', 'torch.distributions.binomial', 'torch.distributions.categorical', 'torch.distributions.cauchy', 'torch.distributions.chi2', 'torch.distributions.gamma', 'torch.distributions.constraint_registry', 'torch.distributions.transforms', 'torch.distributions.exponential', 'torch.distributions.fishersnedecor', 'torch.distributions.geometric', 'torch.distributions.gumbel', 'torch.distributions.uniform', 'torch.distributions.transformed_distribution', 'torch.distributions.half_cauchy', 'torch.distributions.half_normal', 'torch.distributions.normal', 'torch.distributions.independent', 'torch.distributions.kl', 'torch.distributions.laplace', 'torch.distributions.lowrank_multivariate_normal', 'torch.distributions.multivariate_normal', 'torch.distributions.one_hot_categorical', 'torch.distributions.pareto', 'torch.distributions.poisson', 'torch.distributions.log_normal', 'torch.distributions.logistic_normal', 'torch.distributions.multinomial', 'torch.distributions.negative_binomial', 'torch.distributions.relaxed_bernoulli', 'torch.distributions.relaxed_categorical', 'torch.distributions.studentT', 'torch.distributions.weibull', 'torch.backends.cuda', 'torch.backends.mkl', 'torch.backends.openmp', 'torch.backends.quantized', 'torch.quantization', 'torch.quantization.quantize', 'torch.quantization.default_mappings', 'torch.nn.intrinsic.quantized', 'torch.nn.intrinsic.quantized.modules', 'torch.nn.intrinsic.quantized.modules.linear_relu', 'torch.nn.intrinsic.quantized.modules.conv_relu', 'torch.nn.quantized.dynamic', 'torch.nn.quantized.dynamic.modules', 'torch.nn.quantized.dynamic.modules.linear', 'torch.nn.quantized.dynamic.modules.rnn', 'torch.quantization.stubs', 'torch.quantization.qconfig', 'torch.quantization.observer', 'torch.quantization.fake_quantize', 'torch.quantization.fuse_modules', 'torch.utils.data', 'torch.utils.data.sampler', 'torch.utils.data.distributed', 'torch.utils.data.dataset', 'torch.utils.data.dataloader', 'torch.utils.data._utils', 'torch.utils.data._utils.worker', 'torch.utils.data._utils.signal_handling', 'torch.utils.data._utils.pin_memory', 'torch.utils.data._utils.collate', 'torch.utils.data._utils.fetch', 'torch.__config__', 'torch.__future__', 'torch._torch_docs', 'torch._tensor_docs', 'torch._storage_docs', 'torch._classes', 'torch.quasirandom', 'pandas', 'pytz', 'pytz.exceptions', 'pytz.lazy', 'pytz.tzinfo', 'pytz.tzfile', 'pandas.compat', 'pandas.compat.numpy', 'pandas._libs', 'pandas._libs.tslibs', 'pandas._libs.tslibs.conversion', 'pandas._libs.tslibs.c_timestamp', 'pandas._libs.tslibs.nattype', 'pandas._libs.tslibs.np_datetime', 'pandas._libs.tslibs.timezones', 'pandas._libs.tslibs.tzconversion', 'pandas._libs.tslibs.timedeltas', 'pandas._libs.tslibs.offsets', 'pandas._libs.tslibs.ccalendar', 'pandas._config', 'pandas._config.config', 'pandas._config.dates', 'pandas._config.display', 'pandas._config.localization', 'pandas._libs.tslibs.strptime', 'pandas._libs.tslibs.fields', 'pandas._libs.tslibs.parsing', 'pandas._libs.tslibs.frequencies', 'pandas._libs.tslibs.period', 'pandas._libs.tslibs.timestamps', 'pandas._libs.tslibs.resolution', 'pandas._libs.hashtable', 'pandas._libs.missing', 'pandas._libs.ops_dispatch', 'pandas._libs.lib', 'fractions', 'pandas._libs.tslib', 'pandas.core', 'pandas.core.config_init', 'pandas.core.api', 'pandas.core.dtypes', 'pandas.core.dtypes.dtypes', 'pandas._libs.interval', 'pandas._libs.algos', 'pandas._typing', 'pandas.core.dtypes.base', 'pandas.errors', 'pandas.core.dtypes.generic', 'pandas.core.dtypes.inference', 'pandas.core.dtypes.missing', 'pandas.core.dtypes.common', 'pandas.core.algorithms', 'pandas.util', 'pandas.util._decorators', 'pandas._libs.properties', 'pandas.core.util', 'pandas.core.util.hashing', 'pandas._libs.hashing', 'pandas.core.dtypes.cast', 'pandas.util._validators', 'pandas.core.common', 'pandas.core.construction', 'pandas.core.indexers', 'pandas.core.arrays', 'pandas.core.arrays.base', 'pandas.compat.numpy.function', 'pandas.core.ops', 'pandas.core.ops.array_ops', 'pandas._libs.ops', 'pandas.core.ops.missing', 'pandas.core.ops.roperator', 'pandas.core.ops.dispatch', 'pandas.core.ops.invalid', 'pandas.core.ops.common', 'pandas.core.ops.docstrings', 'pandas.core.ops.mask_ops', 'pandas.core.ops.methods', 'pandas.core.missing', 'pandas.compat._optional', 'pandas.core.sorting', 'pandas.core.arrays.boolean', 'pandas.core.nanops', 'pandas.core.arrays.masked', 'pandas.core.arrays.categorical', 'pandas.core.accessor', 'pandas.core.base', 'pandas.io', 'pandas.io.formats', 'pandas.io.formats.console', 'pandas.core.arrays.datetimes', 'pandas.core.arrays.datetimelike', 'pandas.tseries', 'pandas.tseries.frequencies', 'pandas.tseries.offsets', 'dateutil.easter', 'pandas.core.arrays._ranges', 'pandas.core.arrays.integer', 'pandas.core.tools', 'pandas.core.tools.numeric', 'pandas.core.arrays.interval', 'pandas.core.indexes', 'pandas.core.indexes.base', 'pandas._libs.index', 'pandas._libs.join', 'pandas.core.dtypes.concat', 'pandas.core.indexes.frozen', 'pandas.io.formats.printing', 'pandas.core.strings', 'pandas.core.arrays.numpy_', 'pandas.core.arrays.period', 'pandas.core.arrays.sparse', 'pandas.core.arrays.sparse.accessor', 'pandas.core.arrays.sparse.array', 'pandas._libs.sparse', 'pandas.core.arrays.sparse.dtype', 'pandas.core.arrays.string_', 'pandas.core.arrays.timedeltas', 'pandas.core.groupby', 'pandas.core.groupby.generic', 'pandas.core.frame', 'pandas.core.generic', 'pandas.core.indexes.api', 'pandas.core.indexes.category', 'pandas.core.indexes.extension', 'pandas.core.indexes.datetimes', 'pandas.core.indexes.datetimelike', 'pandas.core.indexes.numeric', 'pandas.core.tools.timedeltas', 'pandas.core.tools.datetimes', 'pandas.arrays', 'pandas.core.indexes.interval', 'pandas.util._exceptions', 'pandas.core.indexes.multi', 'pandas.core.indexes.timedeltas', 'pandas.core.indexes.period', 'pandas.core.indexes.range', 'pandas.core.indexing', 'pandas._libs.indexing', 'pandas.core.internals', 'pandas.core.internals.blocks', 'pandas._libs.writers', 'pandas._libs.internals', 'pandas.core.internals.managers', 'pandas.core.internals.concat', 'pandas.io.formats.format', 'pandas.io.common', 'gzip', 'mmap', 'pandas.core.internals.construction', 'pandas.core.series', 'pandas._libs.reshape', 'pandas.core.indexes.accessors', 'pandas.plotting', 'pandas.plotting._core', 'pandas.plotting._misc', 'pandas.core.window', 'pandas.core.window.ewm', 'pandas._libs.window', 'pandas._libs.window.aggregations', 'pandas.core.window.common', 'pandas.core.groupby.base', 'pandas.core.window.rolling', 'pandas.core.window.indexers', 'pandas._libs.window.indexers', 'pandas.core.window.numba_', 'pandas.core.window.expanding', 'pandas.core.groupby.groupby', 'pandas._libs.groupby', 'pandas.core.groupby.ops', 'pandas._libs.reduction', 'pandas.core.groupby.grouper', 'pandas.core.groupby.categorical', 'pandas.tseries.api', 'pandas.core.computation', 'pandas.core.computation.api', 'pandas.core.computation.eval', 'pandas.core.computation.engines', 'pandas.core.computation.align', 'pandas.core.computation.common', 'pandas.core.computation.ops', 'pandas.core.computation.scope', 'pandas.compat.chainmap', 'pandas.core.computation.expr', 'pandas.core.computation.parsing', 'pandas.core.reshape', 'pandas.core.reshape.api', 'pandas.core.reshape.concat', 'pandas.core.reshape.melt', 'pandas.core.reshape.merge', 'pandas.core.reshape.pivot', 'pandas.core.reshape.util', 'pandas.core.reshape.reshape', 'pandas.core.reshape.tile', 'pandas.api', 'pandas.api.extensions', 'pandas.api.indexers', 'pandas.api.types', 'pandas.core.dtypes.api', 'pandas.util._print_versions', 'pandas.io.api', 'pandas.io.clipboards', 'pandas.io.excel', 'pandas.io.excel._base', 'pandas._libs.parsers', 'csv', '_csv', 'pandas.io.excel._util', 'pandas.io.parsers', 'pandas.io.date_converters', 'pandas.io.excel._odfreader', 'pandas.io.excel._openpyxl', 'pandas.io.excel._xlrd', 'pandas.io.excel._pyxlsb', 'pandas.io.excel._xlsxwriter', 'pandas._libs.json', 'pandas.io.excel._xlwt', 'pandas.io.feather_format', 'pandas.io.gbq', 'pandas.io.html', 'pandas.io.json', 'pandas.io.json._json', 'pandas.io.json._normalize', 'pandas.io.json._table_schema', 'pandas.io.orc', 'pandas.io.parquet', 'pandas.io.pickle', 'pandas.compat.pickle_compat', 'pandas.io.pytables', 'pandas.core.computation.pytables', 'pandas.io.sas', 'pandas.io.sas.sasreader', 'pandas.io.spss', 'pandas.io.sql', 'pandas.io.stata', 'pandas.util._tester', 'pandas.testing', 'pandas._testing', 'pandas._libs.testing', 'pandas._version', 'matplotlib', 'matplotlib.cbook', 'matplotlib.cbook.deprecation', 'matplotlib.rcsetup', 'matplotlib.fontconfig_pattern', 'pyparsing', 'matplotlib.colors', 'matplotlib._color_data', 'cycler', 'matplotlib._version', 'matplotlib.ft2font', 'kiwisolver']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-24 22:26:55,955 -276 - wrapper - DEBUG - CACHEDIR=/home/wangyuchen/.cache/matplotlib\n",
      "2020-07-24 22:26:55,958 -1360 - <module> - DEBUG - Using fontManager instance from /home/wangyuchen/.cache/matplotlib/fontlist-v310.json\n",
      "2020-07-24 22:26:56,128 -225 - switch_backend - DEBUG - Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "2020-07-24 22:26:56,133 -225 - switch_backend - DEBUG - Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "2020-07-24 22:26:56,137 -225 - switch_backend - DEBUG - Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as utils\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "if not runOnG814:\n",
    "    %matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromDirGenerateDict(trajectoryDir):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        valueDict: the key is global time, and the value of each key contain SEVERAL LIST of properties, \n",
    "                   each list consist of all property of a single vehicle at one time.\n",
    "    \"\"\"\n",
    "    trajectoryDataFile=open(trajectoryDir)\n",
    "    count=0\n",
    "    allLineList=[]\n",
    "    count=0\n",
    "    for count,line in enumerate(trajectoryDataFile):\n",
    "        #read a single line, remove space and enter\n",
    "        lineList=line.split(' ')\n",
    "        try:\n",
    "            while True:\n",
    "                lineList.remove('')\n",
    "        except:\n",
    "            try:\n",
    "                lineList.remove('\\n')\n",
    "            except:\n",
    "                pass\n",
    "            pass\n",
    "        for i in range(0,lineList.__len__()):\n",
    "            # convert string to float\n",
    "            lineList[i]=float(lineList[i])\n",
    "        allLineList.append(lineList)\n",
    "    valueDict=rearrangeDataByGlobalTime(allLineList)\n",
    "    return valueDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxAndMinValueFromValueDict(valueDict,lableList):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        valueDict: each key in dict is global time, the value of each key is a list of all value at one time\n",
    "        lableList: lables from which you want to get the max and min value. the type of each value in the list \n",
    "                    is str.\n",
    "    Returns:\n",
    "        a dict, which has keys keys from the input lable list and the value of each key is a dict which formed\n",
    "        as 'max':value, 'min':value\n",
    "    \"\"\"\n",
    "    maxAndMinDict={}\n",
    "    keys=list(valueDict.keys())\n",
    "    for lable in lableList:\n",
    "        max=0\n",
    "        min=0 #speed,  positon are all from 0 to max, accelerate from - to +\n",
    "        for eachKey in keys:\n",
    "            valueLists=valueDict[eachKey]\n",
    "            for valueList in valueLists:\n",
    "                value=getValueByLable([lable],valueList)[lable]\n",
    "                if value>max:\n",
    "                    max=value\n",
    "                if value<min:\n",
    "                    min=value\n",
    "        maxAndMinDict[lable]={'max':max,'min':min}\n",
    "    return maxAndMinDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test function of finding the max and min value: block 1, get valueDict for saving time from file reaidng\n",
    "# valueDict=fromDirGenerateDict(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test function of finding the max and min value: block 1, get valueDict for saving time from file reaidng\n",
    "# getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Acc','v_Vel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valueDict=fromDirGenerateDict(1)\n",
    "# theKey=list(valueDict.keys())[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeTensorData(xTensor,yTensor, maxLength=2500,maxWidth=100,blocksize=10,normalizationDict=False):\n",
    "    \"\"\"\n",
    "    visualize a frame on an white image\n",
    "    Args:\n",
    "        valueVisualize: a list of values, each item in the list can be obtained by function \n",
    "        getValueByLable\n",
    "    Returns:\n",
    "        the image of the input frame\n",
    "    \"\"\"\n",
    "    image=np.ones((maxLength,maxWidth,3),dtype=np.int8)\n",
    "    image=image*255\n",
    "#     figure=plt.figure(figsize=(10,50))\n",
    "#     axe=figure.add_subplot(1,1,1)\n",
    "    xLength=xTensor.shape[0] #the length of y is equivalent to x's\n",
    "#     print('length:',xLength)\n",
    "#     print('xTensor.shape',xTensor.shape)\n",
    "    if doNormalization&(normalizationDict is not False):\n",
    "        originalXTensor=torch.zeros(xLength)\n",
    "        originalYTensor=torch.zeros(xLength) #originalX and Y tensor share the same length\n",
    "        originalXTensor=torch.add(\\\n",
    "                                  torch.mul(xTensor,normalizationDict['positionXMax']-normalizationDict['positionXMin']),\\\n",
    "                                  torch.add(originalXTensor,normalizationDict['positionXMin'])\n",
    "                                 )\n",
    "        originalYTensor=torch.add(\\\n",
    "                                  torch.mul(yTensor,normalizationDict['positionYMax']-normalizationDict['positionYMin']),\\\n",
    "                                  torch.add(originalYTensor,normalizationDict['positionYMin'])\n",
    "                                 )\n",
    "        for i in range(xLength):\n",
    "            x=int(originalXTensor[i])\n",
    "            y=int(originalYTensor[i])\n",
    "            colorR=int((i*17+29)%255)\n",
    "            colorG=int((i*9++93)%255)\n",
    "            colorB=int((i*13+111)%255)\n",
    "            cv2.circle(image,(x,y),int(blocksize/2),(colorB,colorG,colorR),-1) #\n",
    "    #     axe.imshow(image)\n",
    "        return image\n",
    "        \n",
    "    \n",
    "    \n",
    "    for i in range(xLength):\n",
    "        x=int(xTensor[i])\n",
    "        y=int(yTensor[i])\n",
    "        colorR=int((i*17+29)%255)\n",
    "        colorG=int((i*9++93)%255)\n",
    "        colorB=int((i*13+111)%255)\n",
    "        cv2.circle(image,(x,y),int(blocksize/2),(colorB,colorG,colorR),-1) #\n",
    "#     axe.imshow(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-24 22:26:56,401 -225 - switch_backend - DEBUG - Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "if not runOnG814:\n",
    "    %matplotlib inline\n",
    "from IPython import display\n",
    "def visualizeData(valueVisualize, maxLength=1000,maxWidth=100,blocksize=10):\n",
    "    \"\"\"\n",
    "    visualize a frame on an white image\n",
    "    Args:\n",
    "        valueVisualize: a list of values, each item in the list can be obtained by function \n",
    "        getValueByLable\n",
    "    Returns:\n",
    "        the image of the input frame\n",
    "    \"\"\"\n",
    "    image=np.ones((maxLength,maxWidth,3),dtype=np.int8)\n",
    "    image=image*255\n",
    "#     figure=plt.figure(figsize=(10,50))\n",
    "#     axe=figure.add_subplot(1,1,1)\n",
    "    \n",
    "    for item in valueVisualize:\n",
    "        infoList=getValueByLable(['Vehicle_ID','Local_X','Local_Y'],item)\n",
    "        vehicleID=infoList['Vehicle_ID']\n",
    "        x=int(infoList['Local_X'])\n",
    "        y=int(infoList['Local_Y'])\n",
    "        colorR=int((vehicleID+100)%255)\n",
    "        colorG=int((vehicleID+150)%255)\n",
    "        colorB=int((vehicleID+200)%255)\n",
    "        cv2.circle(image,(x,y),int(blocksize/2),(colorB,colorG,colorR),-1) #\n",
    "#     axe.imshow(image)\n",
    "    return image\n",
    "# visualizeData(valueDict[theKey])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input: property tensors, target vehicle ID\n",
    "#output: relations, distances between target vehicle and other vehicles which inside the range\n",
    "def relationCalculateWithRange(propertyTensors, distanceRange, targetVehicleId, maxRelationsNumber=maxRelationsNumberGlobal):\n",
    "    \"\"\"\n",
    "    NOTICE:THE PROPERTIES A ND DISTANCE RANGE SHOULD BOTH BE NORMALIZED OR UNNORMALIZED!!!\n",
    "    Args:\n",
    "        propertyTensors:property tensors of all vehicles, at position 0 and 1 are the x and y positon of the \n",
    "        corresponding vehicle\n",
    "        distanceRange: the distance on which we decide to take other vehicles into account\n",
    "        targetVehicleId: the center vehicle which we are going to calculate\n",
    "    \"\"\"\n",
    "    propertyTensorsCopy=copy.deepcopy(propertyTensors)\n",
    "    propertiesDimension=propertyTensorsCopy.shape[0]\n",
    "    reservedIndexes=[]\n",
    "    reservedIndexDistanceDict={}\n",
    "    for i in range(maxMatrixIndex):\n",
    "        reservedIndexes.append(i)\n",
    "    #remove vehicles which position are out-of-range\n",
    "    for i in range(propertiesDimension):\n",
    "        #simply remove the out-of-ranged vehicles by comparing y-axis before computing the distance.\n",
    "        #since most vehicles are out of the given range, this would reduce time cost\n",
    "        if abs(propertyTensorsCopy[i][1]-propertyTensors[targetVehicleId][1])>distanceRange:\n",
    "            reservedIndexes.remove(i)\n",
    "        #to compute if vehicle in the range\n",
    "        else:\n",
    "            distance=((propertyTensorsCopy[i][0]-propertyTensors[targetVehicleId][0])**2+\\\n",
    "                (propertyTensorsCopy[i][1]-propertyTensors[targetVehicleId][1])**2)**0.5\n",
    "            if distance>distanceRange:\n",
    "                reservedIndexes.remove(i)\n",
    "            else:\n",
    "                reservedIndexDistanceDict[i]=distance\n",
    "    #sort dict by value, not by key\n",
    "    sortedReservedIndexDistanceDict=sorted(reservedIndexDistanceDict.items(),key=lambda item:(item[1],item[0]))\n",
    "    #keep the top 'maxRelationsNumber' nearest vehicles to generate relations\n",
    "    if(sortedReservedIndexDistanceDict.__len__()>maxRelationsNumber):\n",
    "        for i in range(maxRelationsNumber,sortedReservedIndexDistanceDict.__len__()):\n",
    "            reservedIndexes.remove(sortedReservedIndexDistanceDict[i][0])\n",
    "    #the final properties tensor is:\n",
    "    finalPropertiesTensor=propertyTensorsCopy[reservedIndexes]\n",
    "    logging.debug('targetVehicleId'+str(targetVehicleId))\n",
    "    logging.debug('reservedIndexes.__len__()'+str(reservedIndexes.__len__()))\n",
    "    logging.debug('propertyTensors[targetVehicleId].shape[0]'+str(propertyTensors[targetVehicleId].shape[0]))\n",
    "    expandedTargetVehicleTensor=propertyTensors[targetVehicleId].expand(reservedIndexes.__len__(),propertyTensors[targetVehicleId].shape[0])\n",
    "    relationTensor=torch.cat((expandedTargetVehicleTensor,finalPropertiesTensor),1)\n",
    "    logging.debug('expandedTargetVehicleTensor:'+str(expandedTargetVehicleTensor))\n",
    "    logging.debug('relationTensor:'+str(relationTensor))\n",
    "    return relationTensor\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discountParameterByExponentialWithDistance(relationTensors, xfactor=1, yfactor=1, w1=1,w2=2,w3=1):\n",
    "    \"\"\"\n",
    "    to calculate a discount parameter matrix from the relations matrix of several vehicl pairs.\n",
    "    Args:\n",
    "        relationTensors: a vehicle pair is a \"relation\" in which two vehilce properties are of the same kind and order,\n",
    "        the relationTensors consists of many vehicle pairs. An extra dimension should be added to the a single vehicle pair\n",
    "        tensor if this function only take as a single vehicle pair. Generally, the relationTensors has to dimension:\n",
    "        the dimension of pairs, the properties of each pair.\n",
    "        xfactor,yfactor:  the weight of x part and y part\n",
    "        w1, w2,w3: facor in exponential operation, w1 and w2 relate to x and w3 relate to y\n",
    "    \"\"\"\n",
    "    vehiclePairDimension=relationTensors.shape[1] #the second dimension of relationTensors is the properties of each vehicle pair\n",
    "    relationsDimension=relationTensors.shape[0]\n",
    "    secondVehiclePropertyStartIndex=int((vehiclePairDimension)/2 )\n",
    "    logging.debug('secondVehiclePropertyStartIndex'+str(secondVehiclePropertyStartIndex))\n",
    "    \n",
    "    discountTensor=torch.zeros(relationsDimension)\n",
    "    for i in range(relationsDimension):\n",
    "        x1,y1,x2,y2=relationTensors[i][0],relationTensors[i][1],\\\n",
    "                    relationTensors[i][secondVehiclePropertyStartIndex],relationTensors[i][secondVehiclePropertyStartIndex+1]\n",
    "        if x2>=x1: #the second vehilce is in front of the first vehicle\n",
    "            xDifference=x2-x1\n",
    "            wx=w1\n",
    "        else: #the second vehicle is after the fist vehicle\n",
    "            xDifference=x1-x2\n",
    "            wx=w2\n",
    "        yDifference=abs(y1-y2)\n",
    "        wy=w3\n",
    "        discountTensor[i]=(xfactor/math.exp(wx*(xDifference)))*(yfactor/math.exp(wx*(yDifference)))\n",
    "    return discountTensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differenceBetweenTwoFrame(frameSeries):\n",
    "    \"\"\"\n",
    "    Given a series of frame, return the difference series of those frame. Since this function \n",
    "    compute diffrences by the gap between two adjacent frames, the quantity of frame in difference \n",
    "    series is one less than the input frame series\n",
    "    Args:\n",
    "        frameSeries: input frames, which dimension is (vehicleQuantity, vehicleProperties)\n",
    "    Returns:\n",
    "        the difference series, which first dimension (quantity dimension) is one less than input frame series.\n",
    "    \"\"\"\n",
    "    frameSeriesWithoutTheFirstFrame=frameSeries[1:]\n",
    "    frameSeriesWithoutTheLastFrame=frameSeries[0:-1]\n",
    "    logging.debug(str(frameSeriesWithoutTheFirstFrame))\n",
    "    logging.debug(str(frameSeriesWithoutTheLastFrame))\n",
    "    logging.debug(str(frameSeries))\n",
    "    differenceSeries=frameSeriesWithoutTheFirstFrame-frameSeriesWithoutTheLastFrame\n",
    "    return differenceSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-24 22:42:52,933 -2 - <module> - DEBUG - tensor([[0.2955, 0.2898, 0.8084, 0.3390, 0.9116],\n",
      "        [0.5311, 0.7934, 0.2336, 0.7299, 0.4377],\n",
      "        [0.0341, 0.5439, 0.2096, 0.2491, 0.6559],\n",
      "        [0.7192, 0.5609, 0.8306, 0.7723, 0.5560],\n",
      "        [0.6913, 0.8128, 0.9927, 0.7789, 0.4715]])\n",
      "2020-07-24 22:42:52,941 -13 - differenceBetweenTwoFrame - DEBUG - tensor([[0.5311, 0.7934, 0.2336, 0.7299, 0.4377],\n",
      "        [0.0341, 0.5439, 0.2096, 0.2491, 0.6559],\n",
      "        [0.7192, 0.5609, 0.8306, 0.7723, 0.5560],\n",
      "        [0.6913, 0.8128, 0.9927, 0.7789, 0.4715]])\n",
      "2020-07-24 22:42:52,944 -14 - differenceBetweenTwoFrame - DEBUG - tensor([[0.2955, 0.2898, 0.8084, 0.3390, 0.9116],\n",
      "        [0.5311, 0.7934, 0.2336, 0.7299, 0.4377],\n",
      "        [0.0341, 0.5439, 0.2096, 0.2491, 0.6559],\n",
      "        [0.7192, 0.5609, 0.8306, 0.7723, 0.5560]])\n",
      "2020-07-24 22:42:52,946 -15 - differenceBetweenTwoFrame - DEBUG - tensor([[0.2955, 0.2898, 0.8084, 0.3390, 0.9116],\n",
      "        [0.5311, 0.7934, 0.2336, 0.7299, 0.4377],\n",
      "        [0.0341, 0.5439, 0.2096, 0.2491, 0.6559],\n",
      "        [0.7192, 0.5609, 0.8306, 0.7723, 0.5560],\n",
      "        [0.6913, 0.8128, 0.9927, 0.7789, 0.4715]])\n"
     ]
    }
   ],
   "source": [
    "inputTensor=torch.rand((5,5))\n",
    "logging.debug(inputTensor)\n",
    "differenceBetweenTwoFrame(inputTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testExpand=torch.tensor(((1,2,3),(3,4,5))).expand(2,6)\n",
    "# print(testExpand)\n",
    "# theList=[1,2,3,5,7]\n",
    "# theList.remove(7)\n",
    "# d = {'lilee':25, 'wangyan':21, 'liqun':32, 'age':19}\n",
    "# print(d)\n",
    "# d=sorted(d.items(), key=lambda item:item[1])\n",
    "# print(d.__len__())\n",
    "# print(theList)\n",
    "# testTensor=torch.rand(10,10)\n",
    "# print(testTensor)\n",
    "# print(testTensor[[1,4,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save visulized images\n",
    "# for key in list(valueDict.keys())[1:10000]:\n",
    "#     image=visualizeData(valueDict[key])\n",
    "#     cv2.imwrite('visualizeFolder/image'+str(key)+'.png',image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensorsDataset(Dataset):\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=100,lableTensorEachBatch=2):\n",
    "        if(numberOfTensorsEachBatch<5):\n",
    "            raise Exception(\"THE NUMBER OF TENSORS IN EACH BATCH IS TOO SMALL\")\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the ture index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                 speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                 accTensor.mul(angleCosTensor)),0)\n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "            if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "            elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "            else:\n",
    "                allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "        return allCombineTensorTrain,allCombineTensorValid\n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class tensorsDatasetV2(Dataset):\n",
    "    \"\"\"\n",
    "    dataset for relation model\n",
    "    \"\"\"\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=1,lableTensorEachBatch=1):\n",
    "        if(numberOfTensorsEachBatch!=1 or lableTensorEachBatch!=1):\n",
    "            raise Exception(\"BOTH TRAIN AND VALID TENSOR NUMBERS SHOULD BE ONE!\")\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        if doNormalization:\n",
    "            self.positionXMax=0\n",
    "            self.positionXMin=999999\n",
    "            self.positionYMax=0\n",
    "            self.positionYMin=99999\n",
    "            self.speedMax=-100\n",
    "            self.speedMin=999999\n",
    "            self.accMax=-100\n",
    "            self.accMin=9999\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            if doNormalization:\n",
    "                #get the max and min value for normalization\n",
    "                maxAndMinDict=getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Vel','v_Acc'])\n",
    "                #position X\n",
    "                if self.positionXMax<maxAndMinDict['Local_X']['max']:\n",
    "                    self.positionXMax=maxAndMinDict['Local_X']['max']\n",
    "                if self.positionXMin>maxAndMinDict['Local_X']['min']:\n",
    "                    self.positionXMin=maxAndMinDict['Local_X']['min']\n",
    "                #position Y\n",
    "                if self.positionYMax<maxAndMinDict['Local_Y']['max']:\n",
    "                    self.positionYMax=maxAndMinDict['Local_Y']['max']\n",
    "                if self.positionYMin>maxAndMinDict['Local_Y']['min']:\n",
    "                    self.positionYMin=maxAndMinDict['Local_Y']['min']\n",
    "                #speed\n",
    "                if self.speedMax<maxAndMinDict['v_Vel']['max']:\n",
    "                    self.speedMax=maxAndMinDict['v_Vel']['max']\n",
    "                if self.speedMin>maxAndMinDict['v_Vel']['min']:\n",
    "                    self.speedMin=maxAndMinDict['v_Vel']['min']\n",
    "                #acc\n",
    "                if self.accMax<maxAndMinDict['v_Acc']['max']:\n",
    "                    self.accMax=maxAndMinDict['v_Acc']['max']\n",
    "                if self.accMin>maxAndMinDict['v_Acc']['min']:\n",
    "                    self.accMin=maxAndMinDict['v_Acc']['min']\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def getNormalizationDict(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            a dict:{'positionXMax':self.positionXMax,'positonYMax':self.self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "        '''\n",
    "        if not doNormalization:\n",
    "            raise Exception('NORMALIZATION IS NOT APPLIED')\n",
    "        return {'positionXMax':self.positionXMax,'positionYMax':self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':self.speedMin,\\\n",
    "               'accMax':self.accMax,'accMin':self.accMin}\n",
    "    \n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the ture index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        #first frame normalization\n",
    "        if doNormalization:\n",
    "#             print('before nomalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                     torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "            speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "            accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "#             print('after normalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        else:\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        #generate relation tensor for all vehicle pairs\n",
    "        print('in getitem, combinedTensor shape: ',combinedTensor.shape)\n",
    "        relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "        relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "        print('in getitem, relation tensorleft shape:',relationTensorLeft.shape)\n",
    "        print('in getitem, relationtensorright shape',relationTensorRight.shape)\n",
    "#         print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "        for i in range(1,combinedTensor.shape[1]):\n",
    "            relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                          combinedTensor[:,i].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "            relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                           torch.transpose(torch.cat((combinedTensor[:,:i],combinedTensor[:,i+1:]),1),0,1)),0)\n",
    "#         print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "        combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1)  \n",
    "        firstCombinedRelationTensor=combinedRelationTensor\n",
    "        \n",
    "        \n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            if doNormalization:\n",
    "                positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                         torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "                speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "                accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                         speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            else:\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            #generate relation tensor for all vehicle pairs\n",
    "            relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "            relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "#             print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "            for j in range(1,combinedTensor.shape[1]):\n",
    "                relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                              combinedTensor[:,j].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "                relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                               torch.transpose(torch.cat((combinedTensor[:,:j],combinedTensor[:,j+1:]),1),0,1)),0)\n",
    "#             print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "            combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1)  \n",
    "            secondRelationTensor=combinedRelationTensor\n",
    "            #since we only need two tensors, which is input and output tensor respectively, we could return\n",
    "            #the two tensors in the first loop\n",
    "            #(ok I admit that the true reason is that I am lazy)\n",
    "            return firstCombinedRelationTensor,secondRelationTensor\n",
    "#             if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "#                 allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "#             elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "#                 allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "#             else:\n",
    "#                 allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "        return allCombineTensorTrain,allCombineTensorValid\n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n",
    "#run on 2080 in g814\n",
    "if runOnG814:\n",
    "    trajectoryFileList=['/home/wangyuchen/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromObjectsToRelationPairsBatchAndTimestepVersion(batchAndTimestepCombinedTensor):\n",
    "    '''\n",
    "    This function based on the other function termed as 'fromObjectsToRelationPairs'. Instead of process a \n",
    "    single frame, this function takes batch and timestep(the other dimension) into consideration.\n",
    "    note: the dimension of combinedTensor is supposed to be (batchs, timesteps, properties, vehicles)\n",
    "    Args:\n",
    "        The input tensor should already be transposed if it is generated from the network's output\n",
    "    Returns:\n",
    "        Relation pairs\n",
    "    '''\n",
    "    #generate relation tensor for all vehicle pairs\n",
    "    batchSize=batchAndTimestepCombinedTensor.shape[0]\n",
    "    timesteps=batchAndTimestepCombinedTensor.shape[1]\n",
    "    for batch in range(batchSize):\n",
    "        for timestep in range(timesteps):\n",
    "            combinedTensor=batchAndTimestepCombinedTensor[batch,timestep,:,:]\n",
    "            relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "            relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "        #         print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "            for i in range(1,combinedTensor.shape[1]):\n",
    "                relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                              combinedTensor[:,i].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "                relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                               torch.transpose(torch.cat((combinedTensor[:,:i],combinedTensor[:,i+1:]),1),0,1)),0)\n",
    "        #         print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "            combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1) \n",
    "            if timestep==0:\n",
    "                combineRelationTensorsTimeStep=combinedRelationTensor.unsqueeze(0)\n",
    "            else:\n",
    "                combineRelationTensorsTimeStep=torch.cat((combineRelationTensorsTimeStep,\\\n",
    "                                                          combinedRelationTensor.unsqueeze(0)),0)\n",
    "        if batch==0:\n",
    "            combinedRelationTensorsTimeStepAndBatch=combineRelationTensorsTimeStep.unsqueeze(0)\n",
    "        else:\n",
    "            combinedRelationTensorsTimeStepAndBatch=torch.cat((combinedRelationTensorsTimeStepAndBatch,\\\n",
    "                                                              combineRelationTensorsTimeStep.unsqueeze(0)),0)\n",
    "    return combinedRelationTensorsTimeStepAndBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the combined tensor and relation tensor i tensorsDataV2\n",
    "import math\n",
    "class tensorsDatasetV2Test(Dataset):\n",
    "    \"\"\"\n",
    "    dataset for relation model\n",
    "    \"\"\"\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=1,lableTensorEachBatch=1):\n",
    "        if(numberOfTensorsEachBatch!=1 or lableTensorEachBatch!=1):\n",
    "            raise Exception(\"BOTH TRAIN AND VALID TENSOR NUMBERS SHOULD BE ONE!\")\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        if doNormalization:\n",
    "            self.positionXMax=0\n",
    "            self.positionXMin=999999\n",
    "            self.positionYMax=0\n",
    "            self.positionYMin=99999\n",
    "            self.speedMax=-100\n",
    "            self.speedMin=999999\n",
    "            self.accMax=-100\n",
    "            self.accMin=9999\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            if doNormalization:\n",
    "                #get the max and min value for normalization\n",
    "                maxAndMinDict=getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Vel','v_Acc'])\n",
    "                #position X\n",
    "                if self.positionXMax<maxAndMinDict['Local_X']['max']:\n",
    "                    self.positionXMax=maxAndMinDict['Local_X']['max']\n",
    "                if self.positionXMin>maxAndMinDict['Local_X']['min']:\n",
    "                    self.positionXMin=maxAndMinDict['Local_X']['min']\n",
    "                #position Y\n",
    "                if self.positionYMax<maxAndMinDict['Local_Y']['max']:\n",
    "                    self.positionYMax=maxAndMinDict['Local_Y']['max']\n",
    "                if self.positionYMin>maxAndMinDict['Local_Y']['min']:\n",
    "                    self.positionYMin=maxAndMinDict['Local_Y']['min']\n",
    "                #speed\n",
    "                if self.speedMax<maxAndMinDict['v_Vel']['max']:\n",
    "                    self.speedMax=maxAndMinDict['v_Vel']['max']\n",
    "                if self.speedMin>maxAndMinDict['v_Vel']['min']:\n",
    "                    self.speedMin=maxAndMinDict['v_Vel']['min']\n",
    "                #acc\n",
    "                if self.accMax<maxAndMinDict['v_Acc']['max']:\n",
    "                    self.accMax=maxAndMinDict['v_Acc']['max']\n",
    "                if self.accMin>maxAndMinDict['v_Acc']['min']:\n",
    "                    self.accMin=maxAndMinDict['v_Acc']['min']\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def getNormalizationDict(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            a dict:{'positionXMax':self.positionXMax,'positonYMax':self.self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "        '''\n",
    "        if not doNormalization:\n",
    "            raise Exception('NORMALIZATION IS NOT APPLIED')\n",
    "        return {'positionXMax':self.positionXMax,'positionYMax':self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':self.speedMin,\\\n",
    "               'accMax':self.accMax,'accMin':self.accMin}\n",
    "    \n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the ture index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        fileName='./'+'tensorFromGetitem'+'/'+str(10000000+idx)+'.png'\n",
    "        image=visualizeTensorData(positionTensor[0,:],positionTensor[1,:])\n",
    "        cv2.imwrite(fileName,image)\n",
    "        #first frame normalization\n",
    "        if doNormalization:\n",
    "#             print('before nomalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                     torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "            speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "            accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "#             print('after normalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        else:\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        #generate relation tensor for all vehicle pairs\n",
    "        print('in getitem, combinedTensor shape: ',combinedTensor.shape)\n",
    "        fileName='./'+'tensorFromGetitemAfterNormalization'+'/'+str(10000000+idx)+'.png'\n",
    "        image=visualizeTensorData(positionTensor[0,:],positionTensor[1,:],normalizationDict=self.getNormalizationDict())\n",
    "        cv2.imwrite(fileName,image)\n",
    "        relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "        relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "        print('in getitem, relation tensorleft shape:',relationTensorLeft.shape)\n",
    "        print('in getitem, relationtensorright shape',relationTensorRight.shape)\n",
    "#         print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "        for i in range(1,combinedTensor.shape[1]):\n",
    "            relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                          combinedTensor[:,i].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "            relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                           torch.transpose(torch.cat((combinedTensor[:,:i],combinedTensor[:,i+1:]),1),0,1)),0)\n",
    "#         print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "        combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1)  \n",
    "        firstCombinedRelationTensor=combinedRelationTensor\n",
    "        firstCombinedTensor=combinedTensor\n",
    "        \n",
    "        \n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            if doNormalization:\n",
    "                positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                         torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "                speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "                accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                         speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            else:\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            #generate relation tensor for all vehicle pairs\n",
    "            relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "            relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "#             print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "            for j in range(1,combinedTensor.shape[1]):\n",
    "                relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                              combinedTensor[:,j].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "                relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                               torch.transpose(torch.cat((combinedTensor[:,:j],combinedTensor[:,j+1:]),1),0,1)),0)\n",
    "#             print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "            combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1)  \n",
    "            secondRelationTensor=combinedRelationTensor\n",
    "            secondCombinedTensor=combinedTensor\n",
    "            #since we only need two tensors, which is input and output tensor respectively, we could return\n",
    "            #the two tensors in the first loop\n",
    "            #(ok I admit that the true reason is that I am lazy)\n",
    "            return firstCombinedTensor,secondCombinedTensor\n",
    "            return firstCombinedRelationTensor,secondRelationTensor\n",
    "#             if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "#                 allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "#             elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "#                 allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "#             else:\n",
    "#                 allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "        return allCombineTensorTrain,allCombineTensorValid\n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n",
    "#run on 2080 in g814\n",
    "if runOnG814:\n",
    "    trajectoryFileList=['/home/wangyuchen/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasetV2Test=tensorsDatasetV2Test(trajectoryFileList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(datasetV2Test.getNormalizationDict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "firstCombined,secondCombined=datasetV2Test.__getitem__(40)\n",
    "print(firstCombined.shape,secondCombined.shape)\n",
    "image=visualizeTensorData(firstCombined[0,:],firstCombined[1,:],normalizationDict=datasetV2Test.getNormalizationDict())\n",
    "dirName='combinedTensorFolder'+str(int(time.time()))\n",
    "os.mkdir(dirName)\n",
    "for i in range(0,2000):\n",
    "    fileName='./'+dirName+'/'+str(10000000+i)+'.png'\n",
    "    firstCombined,secondCombined=datasetV2Test.__getitem__(i)\n",
    "    image=visualizeTensorData(firstCombined[0,:],firstCombined[1,:],normalizationDict=datasetV2Test.getNormalizationDict())\n",
    "    cv2.imwrite(fileName,image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class tensorsDatasetV3(Dataset):\n",
    "    \"\"\"\n",
    "    dataset for relation lstm model\n",
    "    \"\"\"\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=10,lableTensorEachBatch=10):\n",
    "\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        if doNormalization:\n",
    "            self.positionXMax=0\n",
    "            self.positionXMin=999999\n",
    "            self.positionYMax=0\n",
    "            self.positionYMin=99999\n",
    "            self.speedMax=-100\n",
    "            self.speedMin=999999\n",
    "            self.accMax=-100\n",
    "            self.accMin=9999\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            if doNormalization:\n",
    "                #get the max and min value for normalization\n",
    "                maxAndMinDict=getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Vel','v_Acc'])\n",
    "                #position X\n",
    "                if self.positionXMax<maxAndMinDict['Local_X']['max']:\n",
    "                    self.positionXMax=maxAndMinDict['Local_X']['max']\n",
    "                if self.positionXMin>maxAndMinDict['Local_X']['min']:\n",
    "                    self.positionXMin=maxAndMinDict['Local_X']['min']\n",
    "                #position Y\n",
    "                if self.positionYMax<maxAndMinDict['Local_Y']['max']:\n",
    "                    self.positionYMax=maxAndMinDict['Local_Y']['max']\n",
    "                if self.positionYMin>maxAndMinDict['Local_Y']['min']:\n",
    "                    self.positionYMin=maxAndMinDict['Local_Y']['min']\n",
    "                #speed\n",
    "                if self.speedMax<maxAndMinDict['v_Vel']['max']:\n",
    "                    self.speedMax=maxAndMinDict['v_Vel']['max']\n",
    "                if self.speedMin>maxAndMinDict['v_Vel']['min']:\n",
    "                    self.speedMin=maxAndMinDict['v_Vel']['min']\n",
    "                #acc\n",
    "                if self.accMax<maxAndMinDict['v_Acc']['max']:\n",
    "                    self.accMax=maxAndMinDict['v_Acc']['max']\n",
    "                if self.accMin>maxAndMinDict['v_Acc']['min']:\n",
    "                    self.accMin=maxAndMinDict['v_Acc']['min']\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def getNormalizationDict(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            a dict:{'positionXMax':self.positionXMax,'positonYMax':self.self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "        '''\n",
    "        if not doNormalization:\n",
    "            raise Exception('NORMALIZATION IS NOT APPLIED')\n",
    "        return {'positionXMax':self.positionXMax,'positionYMax':self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':self.speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "    \n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the true index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        #first frame normalization\n",
    "        if doNormalization:\n",
    "#             print('before nomalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                     torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "            speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "            accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "#             print('after normalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        else:\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            if doNormalization:\n",
    "                positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                         torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "                speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "                accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                         speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            else:\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "            elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "            else:\n",
    "                allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "        return allCombineTensorTrain,allCombineTensorValid\n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n",
    "#run on 2080 in g814\n",
    "if runOnG814:\n",
    "    trajectoryFileList=['/home/wangyuchen/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class tensorsDatasetV4(Dataset):\n",
    "    \"\"\"\n",
    "    dataset for difference series type\n",
    "    \"\"\"\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=10,lableTensorEachBatch=10):\n",
    "\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        if doNormalization:\n",
    "            self.positionXMax=0\n",
    "            self.positionXMin=999999\n",
    "            self.positionYMax=0\n",
    "            self.positionYMin=99999\n",
    "            self.speedMax=-100\n",
    "            self.speedMin=999999\n",
    "            self.accMax=-100\n",
    "            self.accMin=9999\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            if doNormalization:\n",
    "                #get the max and min value for normalization\n",
    "                maxAndMinDict=getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Vel','v_Acc'])\n",
    "                #position X\n",
    "                if self.positionXMax<maxAndMinDict['Local_X']['max']:\n",
    "                    self.positionXMax=maxAndMinDict['Local_X']['max']\n",
    "                if self.positionXMin>maxAndMinDict['Local_X']['min']:\n",
    "                    self.positionXMin=maxAndMinDict['Local_X']['min']\n",
    "                #position Y\n",
    "                if self.positionYMax<maxAndMinDict['Local_Y']['max']:\n",
    "                    self.positionYMax=maxAndMinDict['Local_Y']['max']\n",
    "                if self.positionYMin>maxAndMinDict['Local_Y']['min']:\n",
    "                    self.positionYMin=maxAndMinDict['Local_Y']['min']\n",
    "                #speed\n",
    "                if self.speedMax<maxAndMinDict['v_Vel']['max']:\n",
    "                    self.speedMax=maxAndMinDict['v_Vel']['max']\n",
    "                if self.speedMin>maxAndMinDict['v_Vel']['min']:\n",
    "                    self.speedMin=maxAndMinDict['v_Vel']['min']\n",
    "                #acc\n",
    "                if self.accMax<maxAndMinDict['v_Acc']['max']:\n",
    "                    self.accMax=maxAndMinDict['v_Acc']['max']\n",
    "                if self.accMin>maxAndMinDict['v_Acc']['min']:\n",
    "                    self.accMin=maxAndMinDict['v_Acc']['min']\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def getNormalizationDict(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            a dict:{'positionXMax':self.positionXMax,'positonYMax':self.self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "        '''\n",
    "        if not doNormalization:\n",
    "            raise Exception('NORMALIZATION IS NOT APPLIED')\n",
    "        return {'positionXMax':self.positionXMax,'positionYMax':self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':self.speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "    \n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the true index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        #first frame normalization\n",
    "        if doNormalization:\n",
    "#             print('before nomalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                     torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "            speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "            accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "#             print('after normalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        else:\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            if doNormalization:\n",
    "                positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                         torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "                speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "                accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                         speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            else:\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "            elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "            else:\n",
    "                allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "        return allCombineTensorTrain,allCombineTensorValid\n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n",
    "#run on 2080 in g814\n",
    "if runOnG814:\n",
    "    trajectoryFileList=['/home/wangyuchen/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasetV2=tensorsDatasetV2(trajectoryFileList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "dataIter=iter(datasetV2)\n",
    "first,second=dataIter.__next__()\n",
    "print(first.shape, second.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxMatrixIndex=250\n",
    "dataloaderV2=DataLoader(datasetV2,batch_size=4,shuffle=True)\n",
    "for i,item in enumerate(dataloaderV2):\n",
    "    if(i>0):\n",
    "        break\n",
    "    print(i)\n",
    "    first,second=item\n",
    "    print(first.shape,second.shape)\n",
    "    print(first[0,:5,:6])\n",
    "    print(first[0,(2,245,246,247,248,249,250,251),6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def generateAdjacencyMatrix(batchedPositionTensor,lambdaX,lambdaY,omegaX,omegaY,m):\n",
    "    \"\"\"\n",
    "    Using batched position tensor generate batched adjacency matrix\n",
    "    Args:\n",
    "        batchedPositionTensor: a batch of position tensor, which size in (batch, timeSequence,2,vehicles), the \n",
    "        value 2 in dim=2 is the position of x and y. \n",
    "        lambda1,lambda2,omega1,omega2,m are parameters of the function. m<1\n",
    "        see detail in my notebook\n",
    "    Returns:\n",
    "        a batch of adjacency matrix\n",
    "    Example:\n",
    "        if given a batch of combined tensor, named theTensor, which size as below:\n",
    "            (4,100,6,250)\n",
    "        which means 4 batches, 100 time step, 6 dimension which respectively of positonx, positony, velocityx, \n",
    "        velocityy, accx,accy.\n",
    "        then we apply the function in such way:\n",
    "        generateAdjacencyMatrix(theTensor(:,:,0:1,:))\n",
    "    \"\"\"\n",
    "    print(batchedPositionTensor.size())\n",
    "    sizeOfEachMatrix=batchedPositionTensor[0,0,0,:].size()[0]\n",
    "    print(sizeOfEachMatrix)\n",
    "    for batchI in range(batchedPositionTensor.size()[0]): #revolve each batch\n",
    "#         print('batchI',batchI)\n",
    "        timeStepsMatrixList=[]\n",
    "        for timeStepI in range(batchedPositionTensor.size()[1]):#revolve each time step\n",
    "#             print('timeStepI:',timeStepI)\n",
    "#             adjacencyMatrix=np.zeros((sizeOfEachMatrix,sizeOfEachMatrix))\n",
    "            adjacencyList=[]\n",
    "            tempPositionList=batchedPositionTensor[batchI,timeStepI,:,:].numpy().tolist()\n",
    "#             start=time.time()\n",
    "            for i in range(sizeOfEachMatrix):\n",
    "                tempLineList=[]\n",
    "                for j in range(sizeOfEachMatrix):\n",
    "#                     adjacencyMatrix[i,j]=1\n",
    "                    if (tempPositionList[1][i]*tempPositionList[1][j]==0):\n",
    "                        toZero=0\n",
    "                    else:\n",
    "                        toZero=1\n",
    "                        \n",
    "                    #calculate original element with linear function\n",
    "#                     tempLineList.append((1-abs(tempPositionList[1][i]-tempPositionList[1][j]))*\\\n",
    "#                         (1-abs(tempPositionList[0][i]-tempPositionList[0][j]))*toZero)\n",
    "                    \n",
    "                    #calculate original element with exponential function\n",
    "                    element=(omegaY/(math.exp(lambdaY*(abs(tempPositionList[1][j]-tempPositionList[1][i])))))*\\\n",
    "                    (omegaX/(math.exp(lambdaX*(abs(tempPositionList[0][j]-tempPositionList[0][i])))))*toZero\n",
    "                    tempLineList.append(element)\n",
    "#                     adjacencyMatrix[i,j]=(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])*\\\n",
    "#                         (batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])\n",
    "#                     (omegaY/math.exp(lambdaX*abs(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])))*\\\n",
    "#                     (omegaX/math.exp(lambdaY*abs(batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])))\n",
    "                    \n",
    "                    #calculate original element with expenential\n",
    "#                     adjacencyMatrix[i,j]=\n",
    "#                     (omegaY/math.exp(lambdaX*abs(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])))*\\\n",
    "#                     (omegaX/math.exp(lambdaY*abs(batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])))\n",
    "                    if(tempPositionList[1][j]-tempPositionList[1][i]<0):\n",
    "                        #if i follows j, then multiple m, m<1\n",
    "                        tempLineList[j]=tempLineList[j]*m\n",
    "                adjacencyList.append(tempLineList)\n",
    "            \n",
    "#             end=time.time()\n",
    "#             print(end-start)\n",
    "            adjacencyMatrix=torch.tensor(adjacencyList).unsqueeze(0)\n",
    "            if timeStepI==0:\n",
    "                matrixSequenceInTimeStepDim=adjacencyMatrix\n",
    "            else:\n",
    "                matrixSequenceInTimeStepDim=\\\n",
    "                torch.cat((matrixSequenceInTimeStepDim,adjacencyMatrix),0)\n",
    "        matrixSequenceInTimeStepDim=matrixSequenceInTimeStepDim.unsqueeze(0)\n",
    "        if batchI==0:\n",
    "            matrixSequenceInBatchDim=matrixSequenceInTimeStepDim\n",
    "        else:\n",
    "            matrixSequenceInBatchDim=torch.cat((matrixSequenceInBatchDim,matrixSequenceInTimeStepDim),0)            \n",
    "    return matrixSequenceInBatchDim\n",
    "\n",
    "def tensorNormalization(inputTensor,minValue,maxValue):\n",
    "    inputTensor.div_(maxValue)\n",
    "    \n",
    "def batchNormalizationForCombinedTensor(inputBatchedTensor,minX,maxX,minY,maxY,minV,maxV,minA,maxA):\n",
    "    tensorNormalization(inputBatchedTensor[:,:,0,:],minX,maxX)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,1,:],minY,maxY)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,2:4,:],minV,maxV)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,4:6,:],minA,maxA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    __init__(self, input_size, cell_size, hidden_size, output_last = True)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, cell_size, hidden_size, output_last = True):\n",
    "        \"\"\"\n",
    "        cell_size is the size of cell_state.\n",
    "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, input, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((input, Hidden_State), 1)\n",
    "        f = F.sigmoid(self.fl(combined))\n",
    "        i = F.sigmoid(self.il(combined))\n",
    "        o = F.sigmoid(self.ol(combined))\n",
    "        C = F.tanh(self.Cl(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        time_step = inputs.size(1)\n",
    "        Hidden_State, Cell_State = self.initHidden(batch_size)\n",
    "        \n",
    "        if self.output_last:\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:]), Hidden_State, Cell_State)  \n",
    "            return Hidden_State\n",
    "        else:\n",
    "            outputs = None\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:]), Hidden_State, Cell_State)  \n",
    "                if outputs is None:\n",
    "                    outputs = Hidden_State.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "            return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        if use_gpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            return Hidden_State, Cell_State\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            return Hidden_State, Cell_State\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# maxMatrixIndex=250\n",
    "# trajectorDataSet=tensorsDataset(trajectoryFileList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataLoader=DataLoader(trajectorDataSet,batch_size=1,shuffle=True,num_workers=4)\n",
    "# for i,data in enumerate(dataLoader):\n",
    "#     print('11111')\n",
    "#     if(i>10):\n",
    "#         break\n",
    "#     print(data[0].shape)\n",
    "#     print(data[0][0,:,1,1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code fragment below is used to visualize tensor data\n",
    "# dataLoader=DataLoader(trajectorDataSet,batch_size=1,shuffle=True,num_workers=4)\n",
    "# for dataI,data in enumerate(dataLoader):\n",
    "#     if(dataI>10):\n",
    "#         break\n",
    "#     for i in range(int(data[0][0,:,0,0].shape[0])):\n",
    "#         tensorImage=visualizeTensorData(data[0][0,i,0,:],data[0][0,i,1,:],2500,100,10) \n",
    "#         fileName=str(100000+dataI)+'_'+str(100000+i)+'.png'\n",
    "#         cv2.imwrite('./tensorVisualizeFolder/'+fileName,tensorImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process objects to generate relation tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30,size1=10,size2=10,size3=10,size4=10):\n",
    "        super(relationNetwork,self).__init__()\n",
    "        self.size1=size1\n",
    "        self.size2=size2\n",
    "        self.size3=size3\n",
    "        self.size4=size4\n",
    "        self.layer1=nn.Linear(inputSize,size1)\n",
    "        self.layer2=nn.Linear(size1,size2)\n",
    "        self.layer3=nn.Linear(size2,size3)\n",
    "        self.layer4=nn.Linear(size3,size4)\n",
    "        self.layer5=nn.Linear(size4,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.ReLU(self.layer5(x4))\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class effectNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process objects to generate relation tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30,size1=10,size2=10,size3=10,size4=10):\n",
    "        super(effectNetwork,self).__init__()\n",
    "        self.size1=size1\n",
    "        self.size2=size2\n",
    "        self.size3=size3\n",
    "        self.size4=size4\n",
    "        self.layer1=nn.Linear(inputSize,size1)\n",
    "        self.layer2=nn.Linear(size1,size2)\n",
    "        self.layer3=nn.Linear(size2,size3)\n",
    "        self.layer4=nn.Linear(size3,size4)\n",
    "        self.layer5=nn.Linear(size4,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.ReLU(self.layer5(x4))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class effectCombinationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process objects to generate relation tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30,size1=10,size2=10,size3=10,size4=10):\n",
    "        super(effectCombinationNetwork,self).__init__()\n",
    "        self.size1=size1\n",
    "        self.size2=size2\n",
    "        self.size3=size3\n",
    "        self.size4=size4\n",
    "        self.layer1=nn.Linear(inputSize,size1)\n",
    "        self.layer2=nn.Linear(size1,size2)\n",
    "        self.layer3=nn.Linear(size2,size3)\n",
    "        self.layer4=nn.Linear(size3,size4)\n",
    "        self.layer5=nn.Linear(size4,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.ReLU(self.layer5(x4))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectModifyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process objects to generate relation tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30,size1=10,size2=10,size3=10,size4=10):\n",
    "        super(objectModifyNetwork,self).__init__()\n",
    "        self.size1=size1\n",
    "        self.size2=size2\n",
    "        self.size3=size3\n",
    "        self.size4=size4\n",
    "        self.layer1=nn.Linear(inputSize,size1)\n",
    "        self.layer2=nn.Linear(size1,size2)\n",
    "        self.layer3=nn.Linear(size2,size3)\n",
    "        self.layer4=nn.Linear(size3,size4)\n",
    "        self.layer5=nn.Linear(size4,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.layer5(x4)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#this block build for relation network testing\n",
    "#delete later if needed\n",
    "relationTensorSize=40\n",
    "positionTuple=(0,1,6,7)\n",
    "velocityTuple=(2,3,8,9)\n",
    "acclerateTuple=(4,5,10,11)\n",
    "positionRelationNet=relationNetwork(outputSize=relationTensorSize)\n",
    "velocityRelationNet=relationNetwork(outputSize=relationTensorSize)\n",
    "accelerateRelationNet=relationNetwork(outputSize=relationTensorSize)\n",
    "\n",
    "maxMatrixIndex=250\n",
    "\n",
    "#load test data in network testing block\n",
    "dataloaderV2=DataLoader(datasetV2,batch_size=1,shuffle=True)\n",
    "for i,item in enumerate(dataloaderV2):\n",
    "    if(i>0):\n",
    "        break\n",
    "    print(i)\n",
    "    first,second=item\n",
    "#     print(first.shape,second.shape)\n",
    "#     print(first[0,:5,:6])\n",
    "#     print(first[0,(2,245,246,247,248,249,250,251),6:])\n",
    "    #from frame to position, velocity, accelerate\n",
    "    positionRelationTensors=positionRelationNet(first[:,:,positionTuple])\n",
    "    velocityRelationTensors=velocityRelationNet(first[:,:,velocityTuple])\n",
    "    accelerateRelationTensors=accelerateRelationNet(first[:,:,acclerateTuple])\n",
    "    print(positionRelationTensors.shape)\n",
    "    objectsAndRelationTensors=torch.cat((first[:,:,positionTuple],positionRelationTensors),2)\n",
    "    print(objectsAndRelationTensors.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "objectAndTensorSize=relationTensorSize+4 \n",
    "effectOutputTensorSize=20\n",
    "#the number 4 is is the size of positon(or velocity or accelerate) pairs,\n",
    "#such as (positonxObject1,positonyObject1,positionxObject2,positionyObject2)\n",
    "positionEffectNet=effectNetwork(inputSize=objectAndTensorSize,outputSize=effectOutputTensorSize)\n",
    "velocityEffectNet=effectNetwork(inputSize=objectAndTensorSize,outputSize=effectOutputTensorSize)\n",
    "accelerateEffectNet=effectNetwork(inputSize=objectAndTensorSize,outputSize=effectOutputTensorSize)\n",
    "first,second=next(iter(dataloaderV2))\n",
    "print(first.shape)\n",
    "\n",
    "#relation computation\n",
    "positionRelationTensors=positionRelationNet(first[:,:,positionTuple])\n",
    "velocityRelationTensors=velocityRelationNet(first[:,:,velocityTuple])\n",
    "accelerateRelationTensors=accelerateRelationNet(first[:,:,acclerateTuple])\n",
    "                                                      \n",
    "objectsAndPositionRelationTensors=torch.cat((first[:,:,positionTuple],positionRelationTensors),2)\n",
    "objectsAndVelocityRelationTensors=torch.cat((first[:,:,positionTuple],velocityRelationTensors),2)\n",
    "objectsAndAccelerateRelationTensors=torch.cat((first[:,:,positionTuple],accelerateRelationTensors),2)\n",
    "\n",
    "#effect computation\n",
    "positionEffectTensors=positionEffectNet(objectsAndPositionRelationTensors)\n",
    "velocityEffectTensors=velocityEffectNet(objectsAndVelocityRelationTensors)\n",
    "accelerateEffectTensors=accelerateEffectNet(objectsAndAccelerateRelationTensors)\n",
    "\n",
    "#effect combination type 1\n",
    "#tensor summation\n",
    "batchSize=positionRelationTensors.shape[0]\n",
    "positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "for i in range(maxMatrixIndex):\n",
    "    positionEffectSummation[:,i,:]=torch.sum(positionEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "    velocityEffectSummation[:,i,:]=torch.sum(velocityEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "    accelerateEffectSummation[:,i,:]=torch.sum(accelerateEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "    \n",
    "#effect combination net initial\n",
    "#combined tensors length\n",
    "combinedTensorSize=20\n",
    "effectCombinationNet=effectCombinationNetwork(inputSize=effectOutputTensorSize*3,outputSize=combinedTensorSize)\n",
    "\n",
    "#combine tensors and process the combined one\n",
    "combinedEffectTensors=torch.cat((positionEffectSummation,velocityEffectSummation,accelerateEffectSummation),2)\n",
    "print(combinedEffectTensors.shape)\n",
    "processedCombinedEffectTensors=effectCombinationNet(combinedEffectTensors)\n",
    "print(processedCombinedEffectTensors.shape)\n",
    "\n",
    "#generate a tuple in which each element is the index of a vehicle\n",
    "#the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "#this part has been put into init function of Module class\n",
    "listForEachVehicle=[]\n",
    "for i in range(maxMatrixIndex):\n",
    "    listForEachVehicle.append(i*(maxMatrixIndex-1))\n",
    "tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "\n",
    "#the property of each vehicle\n",
    "vehicleProperty=first[:,tupleForEachVehicle,0:6]\n",
    "\n",
    "objectAndFinalEffect=torch.cat((vehicleProperty,processedCombinedEffectTensors),2)\n",
    "print(objectAndFinalEffect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#initialize the final network to generate new objects properties\n",
    "objectModifyNet=objectModifyNetwork(inputSize=combinedTensorSize+6,outputSize=6)\n",
    "finalObjectState=objectModifyNet(objectAndFinalEffect)\n",
    "print(finalObjectState.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from relation to new object network\n",
    "class fromRelationToObjectNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(fromRelationToObjectNetwork,self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        \n",
    "        #position relation network initialize\n",
    "        self.relationTensorSize=40\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "        self.velocityTuple=(2,3,8,9)\n",
    "        self.acclerateTuple=(4,5,10,11)\n",
    "        self.positionRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        self.velocityRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        self.accelerateRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        \n",
    "        #effect network initialize\n",
    "        self.objectAndTensorSize=self.relationTensorSize+4 \n",
    "        self.effectOutputTensorSize=20\n",
    "        #the number 4 is is the size of positon(or velocity or accelerate) pairs,\n",
    "        #such as (positonxObject1,positonyObject1,positionxObject2,positionyObject2)\n",
    "        self.positionEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        self.velocityEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        self.accelerateEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        \n",
    "        #effect combination net initialize\n",
    "        #combined tensors length\n",
    "        self.combinedTensorSize=20\n",
    "        self.effectCombinationNet=\\\n",
    "        effectCombinationNetwork(inputSize=self.effectOutputTensorSize*3,outputSize=self.combinedTensorSize)\n",
    "        \n",
    "        #initialize the final network to generate new objects properties\n",
    "        self.objectModifyNet=objectModifyNetwork(inputSize=self.combinedTensorSize+6,outputSize=6)\n",
    "        \n",
    "    def forward(self,inputObjectsPairs):\n",
    "        #relation computation\n",
    "        positionRelationTensors=self.positionRelationNet(inputObjectsPairs[:,:,self.positionTuple])\n",
    "        velocityRelationTensors=self.velocityRelationNet(inputObjectsPairs[:,:,self.velocityTuple])\n",
    "        accelerateRelationTensors=self.accelerateRelationNet(inputObjectsPairs[:,:,self.acclerateTuple])\n",
    "\n",
    "        objectsAndPositionRelationTensors=torch.cat((inputObjectsPairs[:,:,self.positionTuple],positionRelationTensors),2)\n",
    "        objectsAndVelocityRelationTensors=torch.cat((inputObjectsPairs[:,:,self.velocityTuple],velocityRelationTensors),2)\n",
    "        objectsAndAccelerateRelationTensors=torch.cat((inputObjectsPairs[:,:,self.acclerateTuple],accelerateRelationTensors),2)\n",
    "\n",
    "        #effect computation\n",
    "        positionEffectTensors=self.positionEffectNet(objectsAndPositionRelationTensors)\n",
    "        velocityEffectTensors=self.velocityEffectNet(objectsAndVelocityRelationTensors)\n",
    "        accelerateEffectTensors=self.accelerateEffectNet(objectsAndAccelerateRelationTensors)\n",
    "\n",
    "        \n",
    "        #effect combination type 1\n",
    "        #tensor summation\n",
    "        batchSize=positionRelationTensors.shape[0]\n",
    "        if useGpu==True:\n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "            velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "            accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "        else: \n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "            velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "            accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "        for i in range(maxMatrixIndex):\n",
    "            positionEffectSummation[:,i,:]=torch.sum(positionEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "            velocityEffectSummation[:,i,:]=torch.sum(velocityEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "            accelerateEffectSummation[:,i,:]=torch.sum(accelerateEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "        combinedEffectTensors=torch.cat((positionEffectSummation,velocityEffectSummation,accelerateEffectSummation),2)\n",
    "        processedCombinedEffectTensors=self.effectCombinationNet(combinedEffectTensors)\n",
    "        \n",
    "        #the property of each vehicle\n",
    "        vehicleProperty=inputObjectsPairs[:,self.tupleForEachVehicle,0:6]\n",
    "        objectAndFinalEffect=torch.cat((vehicleProperty,processedCombinedEffectTensors),2)\n",
    "        \n",
    "        #compute final state\n",
    "        finalObjectState=self.objectModifyNet(objectAndFinalEffect)\n",
    "        return finalObjectState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This module is supposed to be placed in LSTM model\n",
    "class fromRelationToEffectNetwork(nn.Module):\n",
    "    def __init__(self,effectOutputTensorSize=20):\n",
    "        super(fromRelationToEffectNetwork,self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        \n",
    "        #position relation network initialize\n",
    "        self.relationTensorSize=40\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "        self.velocityTuple=(2,3,8,9)\n",
    "        self.acclerateTuple=(4,5,10,11)\n",
    "        self.positionRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        self.velocityRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        self.accelerateRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        \n",
    "        #effect network initialize\n",
    "        self.objectAndTensorSize=self.relationTensorSize+4 \n",
    "        self.effectOutputTensorSize=effectOutputTensorSize\n",
    "        #the number 4 is the size of positon(or velocity or accelerate) pairs,\n",
    "        #such as (positonxObject1,positonyObject1,positionxObject2,positionyObject2)\n",
    "        self.positionEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        self.velocityEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        self.accelerateEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        \n",
    "        #effect combination net initialize\n",
    "        #combined tensors length\n",
    "        self.combinedTensorSize=20\n",
    "        self.effectCombinationNet=\\\n",
    "        effectCombinationNetwork(inputSize=self.effectOutputTensorSize*3,outputSize=self.combinedTensorSize)\n",
    "        \n",
    "        #remove objectModifyNet in objectToEffectModel\n",
    "        #initialize the final network to generate new objects properties\n",
    "#         self.objectModifyNet=objectModifyNetwork(inputSize=self.combinedTensorSize+6,outputSize=6)\n",
    "        \n",
    "    def forward(self,inputObjectsPairs):\n",
    "        #relation computation\n",
    "        effectOutputTensorSize=self.effectOutputTensorSize\n",
    "        positionRelationTensors=self.positionRelationNet(inputObjectsPairs[:,:,self.positionTuple])\n",
    "        velocityRelationTensors=self.velocityRelationNet(inputObjectsPairs[:,:,self.velocityTuple])\n",
    "        accelerateRelationTensors=self.accelerateRelationNet(inputObjectsPairs[:,:,self.acclerateTuple])\n",
    "\n",
    "        objectsAndPositionRelationTensors=torch.cat((inputObjectsPairs[:,:,self.positionTuple],positionRelationTensors),2)\n",
    "        objectsAndVelocityRelationTensors=torch.cat((inputObjectsPairs[:,:,self.velocityTuple],velocityRelationTensors),2)\n",
    "        objectsAndAccelerateRelationTensors=torch.cat((inputObjectsPairs[:,:,self.acclerateTuple],accelerateRelationTensors),2)\n",
    "\n",
    "        #effect computation\n",
    "        positionEffectTensors=self.positionEffectNet(objectsAndPositionRelationTensors)\n",
    "        velocityEffectTensors=self.velocityEffectNet(objectsAndVelocityRelationTensors)\n",
    "        accelerateEffectTensors=self.accelerateEffectNet(objectsAndAccelerateRelationTensors)\n",
    "\n",
    "        \n",
    "        #effect combination type 1\n",
    "        #tensor summation\n",
    "        batchSize=positionRelationTensors.shape[0]\n",
    "        if useGpu==True:\n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "            velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "            accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "        else: \n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "            velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "            accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "        for i in range(maxMatrixIndex):\n",
    "            positionEffectSummation[:,i,:]=torch.sum(positionEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "            velocityEffectSummation[:,i,:]=torch.sum(velocityEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "            accelerateEffectSummation[:,i,:]=torch.sum(accelerateEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "        combinedEffectTensors=torch.cat((positionEffectSummation,velocityEffectSummation,accelerateEffectSummation),2)\n",
    "        processedCombinedEffectTensors=self.effectCombinationNet(combinedEffectTensors)\n",
    "        \n",
    "        #remove object extraction component and computation component\n",
    "#         #the property of each vehicle\n",
    "#         vehicleProperty=inputObjectsPairs[:,self.tupleForEachVehicle,0:6]\n",
    "#         objectAndFinalEffect=torch.cat((vehicleProperty,processedCombinedEffectTensors),2)\n",
    "        \n",
    "#         #compute final state\n",
    "#         finalObjectState=self.objectModifyNet(objectAndFinalEffect)\n",
    "        return processedCombinedEffectTensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This module is supposed to be placed in LSTM model\n",
    "class fromRelationToEffectNetworkPositionOnly(nn.Module):\n",
    "    def __init__(self,effectOutputTensorSize=20):\n",
    "        super(fromRelationToEffectNetwork,self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        \n",
    "        #position relation network initialize\n",
    "        self.relationTensorSize=40\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "#         self.velocityTuple=(2,3,8,9)\n",
    "#         self.acclerateTuple=(4,5,10,11)\n",
    "        self.positionRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "#         self.velocityRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "#         self.accelerateRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        \n",
    "        #effect network initialize\n",
    "        self.objectAndTensorSize=self.relationTensorSize+4 \n",
    "        self.effectOutputTensorSize=effectOutputTensorSize\n",
    "        #the number 4 is the size of positon(or velocity or accelerate) pairs,\n",
    "        #such as (positonxObject1,positonyObject1,positionxObject2,positionyObject2)\n",
    "        self.positionEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "#         self.velocityEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "#         self.accelerateEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        \n",
    "        #effect combination net initialize\n",
    "        #combined tensors length\n",
    "#         self.combinedTensorSize=20\n",
    "#         self.effectCombinationNet=\\\n",
    "#         effectCombinationNetwork(inputSize=self.effectOutputTensorSize*3,outputSize=self.combinedTensorSize)\n",
    "        \n",
    "        #remove objectModifyNet in objectToEffectModel\n",
    "        #initialize the final network to generate new objects properties\n",
    "#         self.objectModifyNet=objectModifyNetwork(inputSize=self.combinedTensorSize+6,outputSize=6)\n",
    "        \n",
    "    def forward(self,inputObjectsPairs):\n",
    "        #relation computation\n",
    "        effectOutputTensorSize=self.effectOutputTensorSize\n",
    "        positionRelationTensors=self.positionRelationNet(inputObjectsPairs[:,:,self.positionTuple])\n",
    "#         velocityRelationTensors=self.velocityRelationNet(inputObjectsPairs[:,:,self.velocityTuple])\n",
    "#         accelerateRelationTensors=self.accelerateRelationNet(inputObjectsPairs[:,:,self.acclerateTuple])\n",
    "\n",
    "        objectsAndPositionRelationTensors=torch.cat((inputObjectsPairs[:,:,self.positionTuple],positionRelationTensors),2)\n",
    "#         objectsAndVelocityRelationTensors=torch.cat((inputObjectsPairs[:,:,self.velocityTuple],velocityRelationTensors),2)\n",
    "#         objectsAndAccelerateRelationTensors=torch.cat((inputObjectsPairs[:,:,self.acclerateTuple],accelerateRelationTensors),2)\n",
    "\n",
    "        #effect computation\n",
    "        positionEffectTensors=self.positionEffectNet(objectsAndPositionRelationTensors)\n",
    "#         velocityEffectTensors=self.velocityEffectNet(objectsAndVelocityRelationTensors)\n",
    "#         accelerateEffectTensors=self.accelerateEffectNet(objectsAndAccelerateRelationTensors)\n",
    "\n",
    "        \n",
    "        #effect combination type 1\n",
    "        #tensor summation\n",
    "        batchSize=positionRelationTensors.shape[0]\n",
    "        if useGpu==True:\n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "#             velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "#             accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "        else: \n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "#             velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "#             accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "        for i in range(maxMatrixIndex):\n",
    "            positionEffectSummation[:,i,:]=torch.sum(positionEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "#             velocityEffectSummation[:,i,:]=torch.sum(velocityEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "#             accelerateEffectSummation[:,i,:]=torch.sum(accelerateEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "#         combinedEffectTensors=torch.cat((positionEffectSummation,velocityEffectSummation,accelerateEffectSummation),2)\n",
    "#         processedCombinedEffectTensors=self.effectCombinationNet(combinedEffectTensors)\n",
    "        \n",
    "        #remove object extraction component and computation component\n",
    "#         #the property of each vehicle\n",
    "#         vehicleProperty=inputObjectsPairs[:,self.tupleForEachVehicle,0:6]\n",
    "#         objectAndFinalEffect=torch.cat((vehicleProperty,processedCombinedEffectTensors),2)\n",
    "        \n",
    "#         #compute final state\n",
    "#         finalObjectState=self.objectModifyNet(objectAndFinalEffect)\n",
    "        return positionEffectSummation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationLSTM(nn.Module):\n",
    "    def __init__(self, input_size=20, cell_size=20, hidden_size=20, output_last = True):\n",
    "        \"\"\"\n",
    "        cell_size is the size of cell_state.\n",
    "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
    "        the input of LSTM  structure was the output of 'fromRelationToEffectNet' module, so that \n",
    "        the effectOutputTensorSize has the same number as input_size. \n",
    "        \n",
    "        \"\"\"\n",
    "        super(RelationLSTM, self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        \n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "        self.velocityTuple=(2,3,8,9)\n",
    "        self.acclerateTuple=(4,5,10,11)\n",
    "        \n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.fromRelationToEffectNet=fromRelationToEffectNetwork(effectOutputTensorSize=input_size)\n",
    "        self.objectModifyNet=objectModifyNetwork(inputSize=hidden_size+6,outputSize=6)\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, inputEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputEffectTensor, Hidden_State), 2)\n",
    "        print('in step,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl(combined))\n",
    "        i = F.sigmoid(self.il(combined))\n",
    "        o = F.sigmoid(self.ol(combined))\n",
    "        C = F.tanh(self.Cl(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        time_step = inputs.size(1)\n",
    "        Hidden_State, Cell_State = self.initHidden(batch_size)\n",
    "        \n",
    "        if self.output_last:\n",
    "            for i in range(time_step):\n",
    "                effects=self.fromRelationToEffectNet(inputs[:,i,:,:])\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(effects), Hidden_State, Cell_State) \n",
    "                #the property of each vehicle\n",
    "            print(inputs.shape)\n",
    "            vehicleProperty=inputs[:,-1,self.tupleForEachVehicle,0:6].squeeze()\n",
    "            objectAndFinalEffect=torch.cat((vehicleProperty,Hidden_State),2)\n",
    "            outputState=self.objectModifyNet(objectAndFinalEffect)\n",
    "            return outputState\n",
    "        else:\n",
    "            outputs = None\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:,:]), Hidden_State, Cell_State)  \n",
    "                if outputs is None:\n",
    "                    outputs = Hidden_State.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "            return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "#         use_gpu = torch.cuda.is_available()\n",
    "        if useGpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Cell_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            return Hidden_State, Cell_State\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            Cell_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            return Hidden_State, Cell_State\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def fromObjectsToRelationPairs(combinedTensor):\n",
    "    '''\n",
    "    Args:\n",
    "        The input tensor should already be transposed if it is generated from the network's output\n",
    "    Returns:\n",
    "        Relation pairs\n",
    "    '''\n",
    "    #generate relation tensor for all vehicle pairs\n",
    "    relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "    relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "#         print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "    for i in range(1,combinedTensor.shape[1]):\n",
    "        relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                      combinedTensor[:,i].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "        relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                       torch.transpose(torch.cat((combinedTensor[:,:i],combinedTensor[:,i+1:]),1),0,1)),0)\n",
    "#         print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "    combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1) \n",
    "    return combinedRelationTensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationLSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, input_size=20, cell_size=20, hidden_size=20, \\\n",
    "                 input_size_2=20, cell_size_2=20,hidden_size_2=20, outputTimeFrame=5,output_last = True):\n",
    "        \"\"\"\n",
    "        cell_size is the size of cell_state.\n",
    "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
    "        the input of LSTM  structure was the output of 'fromRelationToEffectNet' module, so that \n",
    "        the effectOutputTensorSize has the same number as input_size. \n",
    "        \n",
    "        \"\"\"\n",
    "        super(RelationLSTMSeq2Seq, self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        \n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "        self.velocityTuple=(2,3,8,9)\n",
    "        self.acclerateTuple=(4,5,10,11)\n",
    "        \n",
    "        #effect representive vector computation lstm\n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        #object modifying lstm\n",
    "        self.cell_size2 = cell_size_2\n",
    "        self.hidden_size2 = hidden_size_2\n",
    "        self.fl2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.il2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.ol2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.Cl2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        \n",
    "        self.outputTimeFrame=outputTimeFrame\n",
    "\n",
    "        self.fromRelationToEffectNet=fromRelationToEffectNetwork(effectOutputTensorSize=input_size)\n",
    "        self.objectModifyNet=objectModifyNetwork(inputSize=hidden_size+6,outputSize=6)\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, inputEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputEffectTensor, Hidden_State), 2)\n",
    "#         print('in step,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl(combined))\n",
    "        i = F.sigmoid(self.il(combined))\n",
    "        o = F.sigmoid(self.ol(combined))\n",
    "        C = F.tanh(self.Cl(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def step2(self, inputPreEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputPreEffectTensor, Hidden_State), 2)\n",
    "#         print('in step2,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl2(combined))\n",
    "        i = F.sigmoid(self.il2(combined))\n",
    "        o = F.sigmoid(self.ol2(combined))\n",
    "        C = F.tanh(self.Cl2(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        time_step = inputs.size(1)\n",
    "        Hidden_State, Cell_State,Hidden_State_2,Cell_State_2 = self.initHidden(batch_size)\n",
    "        \n",
    "        if self.output_last:\n",
    "            #effect computation\n",
    "            for i in range(time_step):\n",
    "                effects=self.fromRelationToEffectNet(inputs[:,i,:,:])\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(effects), Hidden_State, Cell_State) \n",
    "                #the property of each vehicle\n",
    "                \n",
    "            Hidden_State_2=Hidden_State\n",
    "            Cell_State_2=Cell_State\n",
    "            \n",
    "            #the lstm process below take as inputs the previous object states and output the next predicted states\n",
    "            #applying function permute to deal with the dimension inconsistency\n",
    "            '''\n",
    "            inputs should be processed by function \"fromObjectsToRelationPairsBatchAndTimestepVersion\" \n",
    "            IF DATASETV3 VERSION IS USED TO GET DATA\n",
    "            '''\n",
    "            print('in seq2seq, inputs shape:',inputs.shape)\n",
    "            preVehicleProperty=inputs[:,-1,self.tupleForEachVehicle,0:6].squeeze()\n",
    "            \n",
    "            #object properties computation\n",
    "            for i in range(outputTimeFrame):\n",
    "                objectAndFinalEffect=torch.cat((preVehicleProperty,Hidden_State_2),2)  \n",
    "                preVehicleProperty=self.objectModifyNet(objectAndFinalEffect)\n",
    "                if i==0:\n",
    "                    #add dimension timestep, which in dimension 1\n",
    "                    outputVehicleProperties=preVehicleProperty.unsqueeze(1) \n",
    "                else:\n",
    "                    outputVehicleProperties=torch.cat((outputVehicleProperties,preVehicleProperty.unsqueeze(1)),1)\n",
    "                #don't need to compute new effect vector after the prediction of the last time step\n",
    "                if i<outputTimeFrame-1:\n",
    "                    preVehicleProperty.cpu()\n",
    "                    #add timestep dimension for further process\n",
    "                    preVehiclePropertyAddTimestep=preVehicleProperty.unsqueeze(1)\n",
    "                    relationPairs=fromObjectsToRelationPairsBatchAndTimestepVersion(preVehiclePropertyAddTimestep).squeeze()\n",
    "                    if useGpu:\n",
    "                        relationPairs.cuda()\n",
    "                    effectInPropertiesComputation=self.fromRelationToEffectNet(relationPairs)\n",
    "                    Hidden_State_2,Cell_State_2=self.step2(effectInPropertiesComputation,Hidden_State_2,Cell_State_2)\n",
    "                if i==outputTimeFrame:\n",
    "                    break\n",
    "#             print(inputs.shape)\n",
    "#             vehicleProperty=inputs[:,-1,self.tupleForEachVehicle,0:6].squeeze()\n",
    "#             objectAndFinalEffect=torch.cat((vehicleProperty,Hidden_State),2)\n",
    "#             outputState=self.objectModifyNet(objectAndFinalEffect)\n",
    "            return outputVehicleProperties\n",
    "        else:\n",
    "            outputs = None\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:,:]), Hidden_State, Cell_State)  \n",
    "                if outputs is None:\n",
    "                    outputs = Hidden_State.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "            return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "#         use_gpu = torch.cuda.is_available()\n",
    "        if useGpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Cell_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Hidden_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2).cuda())\n",
    "            Cell_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2).cuda())\n",
    "            return Hidden_State, Cell_State,Hidden_State_2,Cell_State_2\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            Cell_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            Hidden_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2))\n",
    "            Cell_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2))\n",
    "            return Hidden_State, Cell_State,Hidden_State_2,Cell_State_2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationLSTMSeq2SeqPositionOnly(nn.Module):\n",
    "    def __init__(self, input_size=20, cell_size=20, hidden_size=20, \\\n",
    "                 input_size_2=20, cell_size_2=20,hidden_size_2=20, outputTimeFrame=5,output_last = True):\n",
    "        \"\"\"\n",
    "        cell_size is the size of cell_state.\n",
    "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
    "        the input of LSTM  structure was the output of 'fromRelationToEffectNet' module, so that \n",
    "        the effectOutputTensorSize has the same number as input_size. \n",
    "        \n",
    "        \"\"\"\n",
    "        super(RelationLSTMSeq2Seq, self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        \n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "#         self.velocityTuple=(2,3,8,9)\n",
    "#         self.acclerateTuple=(4,5,10,11)\n",
    "        \n",
    "        #effect representive vector computation lstm\n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        #object modifying lstm\n",
    "        self.cell_size2 = cell_size_2\n",
    "        self.hidden_size2 = hidden_size_2\n",
    "        self.fl2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.il2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.ol2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.Cl2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        \n",
    "        self.outputTimeFrame=outputTimeFrame\n",
    "\n",
    "        self.fromRelationToEffectNet=fromRelationToEffectNetworkPositionOnly(effectOutputTensorSize=input_size)\n",
    "        self.objectModifyNet=objectModifyNetwork(inputSize=hidden_size+6,outputSize=6)\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, inputEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputEffectTensor, Hidden_State), 2)\n",
    "#         print('in step,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl(combined))\n",
    "        i = F.sigmoid(self.il(combined))\n",
    "        o = F.sigmoid(self.ol(combined))\n",
    "        C = F.tanh(self.Cl(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def step2(self, inputPreEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputPreEffectTensor, Hidden_State), 2)\n",
    "#         print('in step2,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl2(combined))\n",
    "        i = F.sigmoid(self.il2(combined))\n",
    "        o = F.sigmoid(self.ol2(combined))\n",
    "        C = F.tanh(self.Cl2(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        time_step = inputs.size(1)\n",
    "        Hidden_State, Cell_State,Hidden_State_2,Cell_State_2 = self.initHidden(batch_size)\n",
    "        \n",
    "        if self.output_last:\n",
    "            #effect computation\n",
    "            for i in range(time_step):\n",
    "                effects=self.fromRelationToEffectNet(inputs[:,i,:,:])\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(effects), Hidden_State, Cell_State) \n",
    "                #the property of each vehicle\n",
    "                \n",
    "            Hidden_State_2=Hidden_State\n",
    "            Cell_State_2=Cell_State\n",
    "            \n",
    "            #the lstm process below take as inputs the previous object states and output the next predicted states\n",
    "            #applying function permute to deal with the dimension inconsistency\n",
    "            '''\n",
    "            inputs should be processed by function \"fromObjectsToRelationPairsBatchAndTimestepVersion\" \n",
    "            IF DATASETV3 VERSION IS USED TO GET DATA\n",
    "            '''\n",
    "            print('in seq2seq, inputs shape:',inputs.shape)\n",
    "            preVehicleProperty=inputs[:,-1,self.tupleForEachVehicle,0:6].squeeze()\n",
    "            \n",
    "            #object properties computation\n",
    "            for i in range(outputTimeFrame):\n",
    "                objectAndFinalEffect=torch.cat((preVehicleProperty,Hidden_State_2),2)  \n",
    "                preVehicleProperty=self.objectModifyNet(objectAndFinalEffect)\n",
    "                if i==0:\n",
    "                    #add dimension timestep, which in dimension 1\n",
    "                    outputVehicleProperties=preVehicleProperty.unsqueeze(1) \n",
    "                else:\n",
    "                    outputVehicleProperties=torch.cat((outputVehicleProperties,preVehicleProperty.unsqueeze(1)),1)\n",
    "                #don't need to compute new effect vector after the prediction of the last time step\n",
    "                if i<outputTimeFrame-1:\n",
    "                    preVehicleProperty.cpu()\n",
    "                    #add timestep dimension for further process\n",
    "                    preVehiclePropertyAddTimestep=preVehicleProperty.unsqueeze(1)\n",
    "                    relationPairs=fromObjectsToRelationPairsBatchAndTimestepVersion(preVehiclePropertyAddTimestep).squeeze()\n",
    "                    if useGpu:\n",
    "                        relationPairs.cuda()\n",
    "                    effectInPropertiesComputation=self.fromRelationToEffectNet(relationPairs)\n",
    "                    Hidden_State_2,Cell_State_2=self.step2(effectInPropertiesComputation,Hidden_State_2,Cell_State_2)\n",
    "                if i==outputTimeFrame:\n",
    "                    break\n",
    "#             print(inputs.shape)\n",
    "#             vehicleProperty=inputs[:,-1,self.tupleForEachVehicle,0:6].squeeze()\n",
    "#             objectAndFinalEffect=torch.cat((vehicleProperty,Hidden_State),2)\n",
    "#             outputState=self.objectModifyNet(objectAndFinalEffect)\n",
    "            return outputVehicleProperties\n",
    "        else:\n",
    "            outputs = None\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:,:]), Hidden_State, Cell_State)  \n",
    "                if outputs is None:\n",
    "                    outputs = Hidden_State.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "            return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "#         use_gpu = torch.cuda.is_available()\n",
    "        if useGpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Cell_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Hidden_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2).cuda())\n",
    "            Cell_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2).cuda())\n",
    "            return Hidden_State, Cell_State,Hidden_State_2,Cell_State_2\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            Cell_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            Hidden_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2))\n",
    "            Cell_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2))\n",
    "            return Hidden_State, Cell_State,Hidden_State_2,Cell_State_2\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class elementWeightedLoss(nn.Module):\n",
    "    def __init__(self,lambdaDistance,lambdaVelocity,lambdaAccelerate):\n",
    "        super(elementWeightedLoss,self).__init__()\n",
    "        self.lambdaDistance,self.lambdaVelocity,self.lambdaAccelerate=\\\n",
    "        lambdaDistance,lambdaVelocity,lambdaAccelerate\n",
    "        \n",
    "    def forward(self,output,label):\n",
    "        '''\n",
    "        output and label dimension: [batch, timestep, vehiclenum, properties]\n",
    "        properties dimension:(distancex,distancey,velocityx,velocityy,acceleratex,acceleratey)\n",
    "        '''\n",
    "        distanceLoss=torch.mean(torch.pow(output[:,:,:,0:2]-label[:,:,:,0:2],2))\n",
    "        velocityLoss=torch.mean(torch.pow(output[:,:,:,2:4]-label[:,:,:,2:4],2))\n",
    "        accelerateLoss=torch.mean(torch.pow(output[:,:,:,4:6]-label[:,:,:,4:6],2))\n",
    "        return torch.mul(distanceLoss,self.lambdaDistance)+torch.mul(velocityLoss,self.lambdaVelocity)+\\\n",
    "                torch.mul(accelerateLoss,self.lambdaAccelerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST RELATION WITHIN A GIVEN RANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetV3Instance=tensorsDatasetV3(trajectoryFileList,lableTensorEachBatch=5)\n",
    "datasetV3Loader=DataLoader(datasetV3Instance,batch_size=5,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6, 250])\n",
      "torch.Size([10, 250, 6])\n"
     ]
    }
   ],
   "source": [
    "theItemInput,theItemLabel=datasetV3Instance.__getitem__(3000)\n",
    "print(theItemInput.shape)\n",
    "theItemInput=theItemInput.reshape((10,250,6))\n",
    "print(theItemInput.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-23 23:59:28,919 -40 - relationCalculateWithRange - DEBUG - targetVehicleId20\n",
      "2020-07-23 23:59:28,919 -41 - relationCalculateWithRange - DEBUG - reservedIndexes.__len__()6\n",
      "2020-07-23 23:59:28,920 -42 - relationCalculateWithRange - DEBUG - propertyTensors[targetVehicleId].shape[0]6\n",
      "2020-07-23 23:59:28,921 -45 - relationCalculateWithRange - DEBUG - expandedTargetVehicleTensor:tensor([[0.2373, 0.7161, 0.3967, 0.0854, 0.2160, 0.0891],\n",
      "        [0.2373, 0.7161, 0.3967, 0.0854, 0.2160, 0.0891],\n",
      "        [0.2373, 0.7161, 0.3967, 0.0854, 0.2160, 0.0891],\n",
      "        [0.2373, 0.7161, 0.3967, 0.0854, 0.2160, 0.0891],\n",
      "        [0.2373, 0.7161, 0.3967, 0.0854, 0.2160, 0.0891],\n",
      "        [0.2373, 0.7161, 0.3967, 0.0854, 0.2160, 0.0891]])\n",
      "2020-07-23 23:59:28,923 -46 - relationCalculateWithRange - DEBUG - relationTensor:tensor([[0.2373, 0.7161, 0.3967, 0.0854, 0.2160, 0.0891, 0.1052, 0.6237, 0.2542,\n",
      "         0.7952, 0.5492, 0.0760],\n",
      "        [0.2373, 0.7161, 0.3967, 0.0854, 0.2160, 0.0891, 0.2479, 0.5413, 0.7013,\n",
      "         0.2614, 0.0936, 0.5392],\n",
      "        [0.2373, 0.7161, 0.3967, 0.0854, 0.2160, 0.0891, 0.2373, 0.7161, 0.3967,\n",
      "         0.0854, 0.2160, 0.0891],\n",
      "        [0.2373, 0.7161, 0.3967, 0.0854, 0.2160, 0.0891, 0.0733, 0.6540, 0.7109,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.2373, 0.7161, 0.3967, 0.0854, 0.2160, 0.0891, 0.1746, 0.5728, 0.3150,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.2373, 0.7161, 0.3967, 0.0854, 0.2160, 0.0891, 0.2759, 0.8629, 0.6920,\n",
      "         0.3937, 0.0799, 0.8545]])\n",
      "2020-07-23 23:59:28,923 -15 - discountParameterByExponentialWithDistance - DEBUG - secondVehiclePropertyStartIndex6\n",
      "2020-07-23 23:59:28,924 -3 - <module> - DEBUG - discountTensor:tensor([0.6383, 0.8308, 1.0000, 0.6362, 0.6623, 0.8308])\n"
     ]
    }
   ],
   "source": [
    "relationTensor=relationCalculateWithRange(theItemInput[0],0.2,20,20)\n",
    "discountTensor=discountParameterByExponentialWithDistance(relationTensor)\n",
    "logging.debug('discountTensor:'+str(discountTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-21 15:04:42,320 -1 - <module> - INFO - test\n",
      "2020-07-21 15:04:42,322 -3 - <module> - DEBUG - tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "logging.info('test')\n",
    "logTensor=torch.zeros(3,3)\n",
    "logging.debug(str(logTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-21 15:04:42,328 -2 - <module> - DEBUG - tensor([0.0164, 0.6609, 0.1991, 0.0609, 0.6587, 0.5410])\n",
      "2020-07-21 15:04:42,330 -3 - <module> - DEBUG - tensor([[0.0164, 0.6609, 0.1991, 0.0609, 0.6587, 0.5410],\n",
      "        [0.0164, 0.6609, 0.1991, 0.0609, 0.6587, 0.5410],\n",
      "        [0.0164, 0.6609, 0.1991, 0.0609, 0.6587, 0.5410]])\n"
     ]
    }
   ],
   "source": [
    "testTensor=torch.rand((6))\n",
    "logging.debug(testTensor)\n",
    "logging.debug(testTensor.expand(3,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test seq2seq relation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test code\n",
    "if runSeq2SeqRelationModel:\n",
    "    outputTimeFrame=5\n",
    "    RelationLSTMSeq2SeqModel=RelationLSTMSeq2Seq(outputTimeFrame=outputTimeFrame)\n",
    "    datasetV3Instance=tensorsDatasetV3(trajectoryFileList,lableTensorEachBatch=outputTimeFrame)\n",
    "    datasetV3Loader=DataLoader(datasetV3Instance,batch_size=2)\n",
    "    V3iter=iter(datasetV3Loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test code\n",
    "if runSeq2SeqRelationModel:\n",
    "    RelationLSTMSeq2SeqModel=RelationLSTMSeq2Seq(outputTimeFrame=outputTimeFrame)\n",
    "    inputs, labels=V3iter.__next__()\n",
    "    inputs=fromObjectsToRelationPairsBatchAndTimestepVersion(inputs)\n",
    "    outputs=RelationLSTMSeq2SeqModel(inputs)\n",
    "    print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model and dataset\n",
    "if runSeq2SeqRelationModel:\n",
    "    epochs=500\n",
    "    itersInEachEpoch=100\n",
    "    outputTimeFrame=5\n",
    "    relationLSTMSeq2SeqModel=RelationLSTMSeq2Seq(outputTimeFrame=outputTimeFrame)\n",
    "    datasetV3Instance=tensorsDatasetV3(trajectoryFileList,lableTensorEachBatch=outputTimeFrame)\n",
    "    datasetV3Loader=DataLoader(datasetV3Instance,batch_size=5,num_workers=4)\n",
    "    V3iter=iter(datasetV3Loader)\n",
    "    if useGpu:\n",
    "        relationLSTMSeq2SeqModel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runSeq2SeqRelationModel:\n",
    "    if not isTest:\n",
    "        learningRate=1e-3\n",
    "        lossFn=elementWeightedLoss(100,0.1,0.1)\n",
    "        lambdaWholeNet=lambda epoch: 0.5**(epoch//30)\n",
    "        optim=torch.optim.RMSprop([{'params':relationLSTMSeq2SeqModel.parameters(),'initial_lr':learningRate}],lr=learningRate)\n",
    "        lrSchedule=torch.optim.lr_scheduler.LambdaLR(optim,lambdaWholeNet,last_epoch=10)\n",
    "        relationLSTMSeq2SeqModel.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training seq2seqRelation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runSeq2SeqRelationModel:\n",
    "    if not isTest:\n",
    "        for epoch in range(epochs):\n",
    "            for iteration in range(itersInEachEpoch):\n",
    "                try:\n",
    "                    inputs, labels=V3iter.__next__()\n",
    "                except StopIteration:\n",
    "                    V3iter=iter(datasetV3Loader)\n",
    "                    inputs, labels=V3iter.__next__()\n",
    "                inputs=fromObjectsToRelationPairsBatchAndTimestepVersion(inputs)\n",
    "                labels=labels.permute(0,1,3,2)\n",
    "                if useGpu:\n",
    "                    inputs=Variable(inputs.cuda())\n",
    "                    labels=Variable(labels.cuda())\n",
    "                outputs=relationLSTMSeq2SeqModel(inputs)\n",
    "                print('compare the shape of outputs and labels:',outputs.shape, labels.shape)\n",
    "                loss=lossFn(outputs,labels)\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                print('loss in opoch ',epoch,',iteration',iteration,':',loss)\n",
    "            lrSchedule.step()\n",
    "            if epoch%10==0:\n",
    "                torch.save(relationLSTMSeq2SeqModel.state_dict(),\\\n",
    "                           'relationLSTMSeq2SeqMode_in_epoch_weightedLoss_'+str(epoch)+'.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing seq2seqRelation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetV3Loader=DataLoader(datasetV3Instance,batch_size=2,num_workers=4)\n",
    "V3iter=iter(datasetV3Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 6, 250]) torch.Size([5, 5, 6, 250])\n",
      "torch.Size([5, 10, 62250, 12])\n",
      "in seq2seq, inputs shape: torch.Size([5, 10, 62250, 12])\n",
      "torch.Size([5, 5, 250, 6])\n"
     ]
    }
   ],
   "source": [
    "if isTest:\n",
    "    relationLSTMSeq2SeqModel.eval()\n",
    "    with torch.no_grad():\n",
    "        relationLSTMSeq2SeqModel.load_state_dict(torch.load('relationLSTMSeq2SeqMode_in_epoch_weightedLoss_80.pt'))\n",
    "#         inputs,labels=V3iter.__next__()\n",
    "        itemIndex=8000\n",
    "        inputs,labels=datasetV3Instance.__getitem__(itemIndex)\n",
    "        inputs=inputs.unsqueeze(0)\n",
    "        labels=labels.unsqueeze(0)\n",
    "        for ii in range(4):\n",
    "            newInput, newLabel=datasetV3Instance.__getitem__(itemIndex+ii)\n",
    "            newInput=newInput.unsqueeze(0)\n",
    "            newLabel=newLabel.unsqueeze(0)\n",
    "            inputs=torch.cat((inputs,newInput),0)\n",
    "            labels=torch.cat((labels,newLabel),0)\n",
    "            \n",
    "        print(inputs.shape,labels.shape)\n",
    "\n",
    "        inputs=fromObjectsToRelationPairsBatchAndTimestepVersion(inputs)\n",
    "        print(inputs.shape)\n",
    "        labels=labels.permute(0,1,3,2)\n",
    "        outputs=relationLSTMSeq2SeqModel(inputs)\n",
    "        print(outputs.shape)\n",
    "        normalizationDict=datasetV3Instance.getNormalizationDict()\n",
    "        for i in range(labels.shape[1]):\n",
    "            resultImage=visualizeTensorData(outputs[0,i,:,0],outputs[0,i,:,1],normalizationDict=normalizationDict)\n",
    "            fileName='./predictWithRelationSeq2Seq/'+str(i)+'.png'\n",
    "            cv2.imwrite(fileName,resultImage)\n",
    "            resultImage=visualizeTensorData(labels[0,i,:,0],labels[0,i,:,1],normalizationDict=normalizationDict)\n",
    "            fileName='./predictWithRelationSeq2Seq/'+'l'+str(i)+'.png'\n",
    "            cv2.imwrite(fileName,resultImage)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test lstm relation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start test lstm relation model\n"
     ]
    }
   ],
   "source": [
    "print('start test lstm relation model')\n",
    "if runRelationLSTM:\n",
    "    datasetV3Instance=tensorsDatasetV3(trajectoryFileList)\n",
    "    datasetV3Loader=DataLoader(datasetV3Instance,batch_size=2)\n",
    "    V3iter=iter(datasetV3Loader)\n",
    "    relationLSTMInstance=RelationLSTM()\n",
    "    if useGpu:\n",
    "        relationLSTMInstance.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "def calculateXandN(x,n):\n",
    "    return (1+x)/(n+1+2*x)\n",
    "ns=[]\n",
    "xs=[]\n",
    "for i in range(0,100):\n",
    "    xs.append(i)\n",
    "for i in range(0,10):\n",
    "    ns.append(i)\n",
    "lines=[]\n",
    "for i in range(20):\n",
    "    lines.append([])\n",
    "for x in xs:\n",
    "    for n in ns:\n",
    "        lines[n].append(calculateXandN(x,n))\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(241)\n",
    "plt.plot(xs,lines[0],'b')\n",
    "plt.plot(xs,lines[1],'g')\n",
    "plt.plot(xs,lines[2],'k')\n",
    "plt.plot(xs,lines[3],'y')\n",
    "plt.plot(xs,lines[4],'m')\n",
    "plt.plot(xs,lines[5],'-')\n",
    "plt.subplot(242)\n",
    "plt.plot(ns,lines[0],'b')\n",
    "plt.subplot(243)\n",
    "plt.plot(ns,lines[1],'b')\n",
    "plt.subplot(244)\n",
    "plt.plot(ns,lines[2],'b')\n",
    "plt.subplot(245)\n",
    "plt.plot(ns,lines[3],'b')\n",
    "plt.subplot(246)\n",
    "plt.plot(ns,lines[4],'b')\n",
    "plt.subplot(247)\n",
    "plt.plot(ns,lines[5],'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training relation lstm\n",
    "if runRelationLSTM:\n",
    "    if not isTest:\n",
    "        learningRate=1e-3\n",
    "        MSELoss=nn.MSELoss()\n",
    "        lambdaWholeNet=lambda epoch: 0.5**(epoch//30)\n",
    "        optim=torch.optim.RMSprop([{'params':relationLSTMInstance.parameters(),'initial_lr':learningRate}],lr=learningRate)\n",
    "        lrSchedule=torch.optim.lr_scheduler.LambdaLR(optim,lambdaWholeNet,last_epoch=10)\n",
    "        relationLSTMInstance.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "label.shape torch.Size([2, 10, 6, 250])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "torch.Size([2, 10, 62250, 12])\n",
      "output.shape torch.Size([2, 250, 6])\n",
      "epoch  0  i 1  loss tensor(0.0382, grad_fn=<MseLossBackward>)\n",
      "label.shape torch.Size([2, 10, 6, 250])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "torch.Size([2, 10, 62250, 12])\n",
      "output.shape torch.Size([2, 250, 6])\n",
      "epoch  0  i 2  loss tensor(0.0286, grad_fn=<MseLossBackward>)\n",
      "label.shape torch.Size([2, 10, 6, 250])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "torch.Size([2, 10, 62250, 12])\n",
      "output.shape torch.Size([2, 250, 6])\n",
      "epoch  0  i 3  loss tensor(0.0229, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-c7320c2e96b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m#             outputView=output.reshape((7,6,250)).cpu()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if runRelationLSTM:\n",
    "    #training relation lstm\n",
    "    if not runOnG814:\n",
    "        %matplotlib inline\n",
    "    from IPython.display import clear_output\n",
    "    normalizationDict=datasetV3Instance.getNormalizationDict()\n",
    "    optim.zero_grad()\n",
    "    if not isTest:  \n",
    "        losses=[]\n",
    "        iterInEpoch=50\n",
    "        for epoch in range(5):\n",
    "            print(epoch)\n",
    "            i=0\n",
    "            for inputs,label in V3iter:\n",
    "                i=i+1\n",
    "                if i>iterInEpoch*(epoch+1):\n",
    "                    break\n",
    "        #         print(i)\n",
    "                inputs=fromObjectsToRelationPairsBatchAndTimestepVersion(inputs)\n",
    "                print('label.shape',label.shape)\n",
    "                label=label[:,0,:,:].squeeze()\n",
    "                label=label.permute(0,2,1)\n",
    "#                 label=label[:,tupleForEachVehicle,0:6]\n",
    "                if useGpu:\n",
    "                    inputs=Variable(inputs.cuda())\n",
    "                    label=Variable(label.cuda())\n",
    "                output=relationLSTMInstance(inputs)\n",
    "\n",
    "        #         print(output[0,0:10,:],secondObjects[0,0:10,:])\n",
    "                print('output.shape',output.shape)\n",
    "                loss=MSELoss(output,label)\n",
    "                if i<5:\n",
    "                    print('epoch ',epoch, ' i', i,' loss',loss)\n",
    "        #         print(loss)\n",
    "                losses.append(loss.item())\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "    #             outputView=output.reshape((7,6,250)).cpu()\n",
    "    #             for j in range(10):\n",
    "    #                 inputs=inputs.cpu()\n",
    "    #                 resultImage=visualizeTensorData(inputs[0,j,0,:],inputs[0,j,1,:],normalizationDict=normalizationDict)\n",
    "    #                 fileName='./predictWithRelationLSTM/'+str((epoch+1)*10000000+i*100000+j)+'.png'\n",
    "    #                 cv2.imwrite(fileName,resultImage)\n",
    "    #             resultImage=visualizeTensorData(outputView[0,0,:],outputView[0,1,:],normalizationDict=normalizationDict)\n",
    "    #             fileName='./predictWithLSTMOnly/'+str((epoch+1)*10000000+i*100000+50)+'.png' #the predicted image is named with string which last two number is 50(because j < 50)\n",
    "    #             cv2.imwrite(fileName,resultImage)\n",
    "            lrSchedule.step()\n",
    "        plt.figure(figsize=(30,30))\n",
    "        plt.plot(losses)\n",
    "        plt.savefig('./losses.png')\n",
    "        torch.save(relationLSTMInstance.state_dict(),'./relationLSTM.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test relation-object model over a period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runObjectRelationNet:\n",
    "    normalizationDict=datasetV2.getNormalizationDict()\n",
    "\n",
    "    datas=[]\n",
    "    for i in range(0,6):\n",
    "        break\n",
    "        datas.append([])\n",
    "    for ii in range(0,2000):\n",
    "        break\n",
    "        theInput,second=datasetV2.__getitem__(ii)\n",
    "        theInput=theInput.unsqueeze(0)\n",
    "        second=second.unsqueeze(0)\n",
    "        #         print(i)\n",
    "        #         theInput,second=item\n",
    "        if useGpu:\n",
    "            theInput=Variable(theInput.cuda())\n",
    "            second=Variable(second.cuda())\n",
    "        inputObjects=theInput[:,tupleForEachVehicle,0:6]\n",
    "        secondObjects=second[:,tupleForEachVehicle,0:6]\n",
    "        for i in range(0,6):\n",
    "            datas[i].append(inputObjects[0,0,i])\n",
    "    #     print('inputObjects.shape',inputObjects.shape)\n",
    "    #     resultImage=visualizeTensorData(inputObjects[0,:,0].cpu(),inputObjects[0,:,1].cpu(),normalizationDict=normalizationDict)\n",
    "    #     fileName='./resultImage/'+str(ii)+'.png'\n",
    "    #     cv2.imwrite(fileName,resultImage)\n",
    "    timeStamp=int(time.time())\n",
    "    dirName='resultImage'+str(timeStamp)\n",
    "    os.mkdir(dirName)\n",
    "    if isTest:\n",
    "        with torch.no_grad():\n",
    "            wholeNet.eval()\n",
    "            theInput,second=datasetV2.__getitem__(5000)\n",
    "            theInput=theInput.unsqueeze(0)\n",
    "            second=second.unsqueeze(0)\n",
    "        #         print(i)\n",
    "    #         theInput,second=item\n",
    "            if useGpu:\n",
    "                theInput=Variable(theInput.cuda())\n",
    "                second=Variable(second.cuda())\n",
    "            inputObjects=theInput[:,tupleForEachVehicle,0:6]\n",
    "            secondObjects=second[:,tupleForEachVehicle,0:6]\n",
    "            print('inputObjects.shape',inputObjects.shape)\n",
    "            resultImage=visualizeTensorData(inputObjects[0,:,0].cpu(),inputObjects[0,:,1].cpu(),normalizationDict=normalizationDict)\n",
    "            fileName='./resultImage/'+'0000000000000000'+'.png'\n",
    "            cv2.imwrite(fileName,resultImage)\n",
    "            stepInput=fromObjectsToRelationPairs(inputObjects[0].permute(1,0)).unsqueeze(0)\n",
    "        #             output=wholeNet(theInput)\n",
    "            print(stepInput.shape)\n",
    "        #             print(output.shape)\n",
    "            #predict step by step\n",
    "            for step in range(500):\n",
    "                output=wholeNet(stepInput)\n",
    "                print('step: ',step)\n",
    "                for ii in range(output.shape[1]):\n",
    "                    print(output[0,ii])\n",
    "    #             break\n",
    "    #             for j in range(10):\n",
    "    #                 print(stepInput[:,tupleForEachVehicle,0:6][0,j,:])\n",
    "    #                 print(output[0,j,:])\n",
    "    #                 print()\n",
    "                stepInput=fromObjectsToRelationPairs(output[0].permute(1,0)).unsqueeze(0)\n",
    "    #             print('outputShape',output.shape)\n",
    "    #             print('outputshape[0]',output[0].shape)\n",
    "                resultImage=visualizeTensorData(output[0,:,0].cpu(),output[0,:,1].cpu(),normalizationDict=normalizationDict)\n",
    "\n",
    "                import os\n",
    "                fileName='./'+dirName+'/'+str(1000000+step)+'.png'\n",
    "                cv2.imwrite(fileName,resultImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training and testing process for 'fromRelationToObjectnetwork'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runObjectRelationNet:\n",
    "    #generate a tuple in which each element is the index of a vehicle\n",
    "    #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "    listForEachVehicle=[]\n",
    "    for i in range(maxMatrixIndex):\n",
    "        listForEachVehicle.append(i*(maxMatrixIndex-1))\n",
    "    tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "\n",
    "    dataloaderV2=DataLoader(datasetV2,batch_size=1,shuffle=True)\n",
    "\n",
    "\n",
    "    wholeNet=fromRelationToObjectNetwork()\n",
    "    if isTest:\n",
    "        wholeNet.load_state_dict(torch.load(modelPath))\n",
    "\n",
    "\n",
    "    if useGpu:\n",
    "        wholeNet.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runObjectRelationNet:\n",
    "    if not isTest:\n",
    "        learningRate=1e-3\n",
    "        MSELoss=nn.MSELoss()\n",
    "        lambdaWholeNet=lambda epoch: 0.5**(epoch//30)\n",
    "        optim=torch.optim.RMSprop([{'params':wholeNet.parameters(),'initial_lr':learningRate}],lr=learningRate)\n",
    "        lrSchedule=torch.optim.lr_scheduler.LambdaLR(optim,lambdaWholeNet,last_epoch=10)\n",
    "        wholeNet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if runObjectRelationNet:\n",
    "    if not runOnG814:\n",
    "        %matplotlib inline\n",
    "    from IPython.display import clear_output\n",
    "    if not isTest:  \n",
    "        losses=[]\n",
    "        iterInEpoch=50\n",
    "        for epoch in range(300):\n",
    "            print(epoch)\n",
    "            for i,item in enumerate(dataloaderV2):\n",
    "                if i>iterInEpoch:\n",
    "                    break\n",
    "        #         print(i)\n",
    "                theInput,second=item\n",
    "                if useGpu:\n",
    "                    theInput=Variable(theInput.cuda())\n",
    "                    second=Variable(second.cuda())\n",
    "                secondObjects=second[:,tupleForEachVehicle,0:6]\n",
    "\n",
    "                output=wholeNet(theInput)\n",
    "        #         print(output[0,0:10,:],secondObjects[0,0:10,:])\n",
    "                loss=MSELoss(output,secondObjects)\n",
    "                if i<5:\n",
    "                    print('epoch ',epoch, ' i', i,' loss',loss)\n",
    "        #         print(loss)\n",
    "                losses.append(loss.item())\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            lrSchedule.step()\n",
    "        plt.figure(figsize=(30,30))\n",
    "        plt.plot(losses)\n",
    "        plt.savefig('./losses.png')\n",
    "        torch.save(wholeNet.state_dict(),'./wholeNet_300epoch_50perEpoch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training simple LSTM module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runLSTM:\n",
    "    #lstm version \n",
    "    if not isTest:\n",
    "        learningRate=1e-3\n",
    "        MSELoss=nn.MSELoss()\n",
    "        lambdaWholeNet=lambda epoch: 0.5**(epoch//30)\n",
    "        optim=torch.optim.RMSprop([{'params':lstmModel.parameters(),'initial_lr':learningRate}],lr=learningRate)\n",
    "        lrSchedule=torch.optim.lr_scheduler.LambdaLR(optim,lambdaWholeNet,last_epoch=10)\n",
    "        lstmModel.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if runLSTM:\n",
    "    #lstm version\n",
    "    if not runOnG814:\n",
    "        %matplotlib inline\n",
    "    from IPython.display import clear_output\n",
    "    normalizationDict=datasetV3.getNormalizationDict()\n",
    "    if not isTest:  \n",
    "        losses=[]\n",
    "        iterInEpoch=50\n",
    "        for epoch in range(5):\n",
    "            print(epoch)\n",
    "            i=0\n",
    "            for inputs,label in iterV3:\n",
    "                i=i+1\n",
    "                if i>iterInEpoch*(epoch+1):\n",
    "                    break\n",
    "        #         print(i)\n",
    "                if useGpu:\n",
    "                    inputs=Variable(inputs.cuda())\n",
    "                    label=Variable(label.cuda())\n",
    "                output=lstmModel(inputs.reshape((inputs.shape[0],inputs.shape[1],-1)))\n",
    "\n",
    "        #         print(output[0,0:10,:],secondObjects[0,0:10,:])\n",
    "                loss=MSELoss(output,label.squeeze().reshape((label.shape[0],-1)))\n",
    "                if i<5:\n",
    "                    print('epoch ',epoch, ' i', i,' loss',loss)\n",
    "        #         print(loss)\n",
    "                losses.append(loss.item())\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                outputView=output.reshape((7,6,250)).cpu()\n",
    "                for j in range(10):\n",
    "                    inputs=inputs.cpu()\n",
    "                    resultImage=visualizeTensorData(inputs[0,j,0,:],inputs[0,j,1,:],normalizationDict=normalizationDict)\n",
    "                    fileName='./predictWithLSTMOnly/'+str((epoch+1)*10000000+i*100000+j)+'.png'\n",
    "                    cv2.imwrite(fileName,resultImage)\n",
    "                resultImage=visualizeTensorData(outputView[0,0,:],outputView[0,1,:],normalizationDict=normalizationDict)\n",
    "                fileName='./predictWithLSTMOnly/'+str((epoch+1)*10000000+i*100000+50)+'.png' #the predicted image is named with string which last two number is 50(because j < 50)\n",
    "                cv2.imwrite(fileName,resultImage)\n",
    "            lrSchedule.step()\n",
    "        plt.figure(figsize=(30,30))\n",
    "        plt.plot(losses)\n",
    "        plt.savefig('./losses.png')\n",
    "        torch.save(wholeNet.state_dict(),'./wholeNet_300epoch_50perEpoch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#draw properties of a single tensor over time\n",
    "#output results step by step\n",
    "plt.subplot(321)\n",
    "plt.plot(datas[0])\n",
    "plt.subplot(322)\n",
    "plt.plot(datas[1])\n",
    "plt.subplot(323)\n",
    "plt.plot(datas[2])\n",
    "plt.subplot(324)\n",
    "plt.plot(datas[3])\n",
    "plt.subplot(325)\n",
    "plt.plot(datas[4])\n",
    "plt.subplot(326)\n",
    "plt.plot(datas[5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
