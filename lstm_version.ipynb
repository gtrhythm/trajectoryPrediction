{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import threading\n",
    "from multiprocessing import Process\n",
    "from multiprocessing import Manager\n",
    "from tensorboardX import SummaryWriter\n",
    "#global variable\n",
    "doNormalization=True\n",
    "useGpu=False\n",
    "runOnG814=False\n",
    "isTest=False\n",
    "modelPath='/home/wangyuchen/wholeNet_300epoch_50perEpoch.pt'\n",
    "maxMatrixIndex=250\n",
    "runRelationLSTM=False\n",
    "runObjectRelationNet=False\n",
    "runSeq2SeqRelationModel=False\n",
    "runLSTM=False\n",
    "runDifferenceLSTMModel=True\n",
    "\n",
    "\n",
    "\n",
    "maxRelationsNumberGlobal=20 # the maximum nubmer of relation in the \"relation within the given range\" version \n",
    "\n",
    "if useGpu:\n",
    "    import os\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromAllToStr(*args):\n",
    "    returnedStr=str()\n",
    "    for eachItem in args:\n",
    "        returnedStr=returnedStr+str(eachItem)\n",
    "    return returnedStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='logging.txt',level=logging.DEBUG, format='%(asctime)s -%(lineno)d - %(funcName)s - %(levelname)s - %(message)s',)\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s -%(lineno)d - %(funcName)s - %(levelname)s - %(message)s')\n",
    "console.setFormatter(formatter)\n",
    "logging.getLogger('').addHandler(console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a tuple in which each element is the index of a vehicle\n",
    "#the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "listForEachVehicle=[]\n",
    "for i in range(maxMatrixIndex):\n",
    "    listForEachVehicle.append(i*(maxMatrixIndex-1))\n",
    "tupleForEachVehicle=tuple(listForEachVehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def getValueByLable(lableList,valueList):\n",
    "    \"\"\"\n",
    "    For instance, given a lable list ['Local_X','Local_Y'] and a value list [2.0, 24.0, 437.0, 1118846981300.0, 16.254, \n",
    "    79.349, 6451167.199, 1873312.382, 14.5, 4.9, 2.0, 39.14, -5.73, 2.0, 0.0, 13.0, 0.0, 0.0] which values sorted by the \n",
    "    order of allLableList below, the function return a value Dict {'Local_X':16.254, 'Local_Y':79.349}\n",
    "    Args:\n",
    "        lableList: the list of lables you've required, such as['Vehicle_ID', 'Total_Frames','Global_Time']\n",
    "        valueList: the list contains all legally value, sorted by:['Vehicle_ID', 'Frame_ID','Total_Frames','Global_Time','Local_X','Local_Y','Global_X','Global_Y',\\\n",
    "                      'v_Length','v_Width','v_Class','v_Vel','v_Acc','Lane_ID','Preceding','Following','Space_Headway',\\\n",
    "                      'Time_Headway']\n",
    "    Returns: \n",
    "        value dict of the input lables\n",
    "    For instance, given a lable list ['Local_X','Local_Y'] and a value list [2.0, 24.0, 437.0, 1118846981300.0, 16.254, \n",
    "    79.349, 6451167.199, 1873312.382, 14.5, 4.9, 2.0, 39.14, -5.73, 2.0, 0.0, 13.0, 0.0, 0.0] which values sorted by the \n",
    "    order of allLableList above, the function return a value List [16.254, 79.349]\n",
    "\n",
    "    \"\"\"\n",
    "    allLableList=['Vehicle_ID', 'Frame_ID','Total_Frames','Global_Time','Local_X','Local_Y','Global_X','Global_Y',\\\n",
    "                  'v_Length','v_Width','v_Class','v_Vel','v_Acc','Lane_ID','Preceding','Following','Space_Headway',\\\n",
    "                  'Time_Headway']\n",
    "    valueDictReturn={}\n",
    "    for lableItem in lableList:\n",
    "        valueDictReturn[lableItem]=valueList[allLableList.index(lableItem)]\n",
    "    return valueDictReturn\n",
    "\n",
    "def rearrangeDataByGlobalTime(allValueLists):\n",
    "    '''\n",
    "    Args:\n",
    "        allValueLists: all values have been read from a txt file which have already been converted to a list\n",
    "    Returns:\n",
    "        dict have been arranged by global time. One single global time generally contains several value lists.\n",
    "    '''\n",
    "    valueDict={}\n",
    "    for valueList in allValueLists:\n",
    "        dictKey=getValueByLable(['Global_Time'],valueList)['Global_Time']\n",
    "        if dictKey in valueDict:\n",
    "            # if dictKey already there, then add valueList to the list of the key\n",
    "            valueDict[dictKey].append(valueList)\n",
    "        else:\n",
    "            #else, create a list and append valueList on it\n",
    "            valueDict[dictKey]=[valueList]\n",
    "    return valueDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def readFirstFrame(matrixIndexAndVehicleIDRecordDictParam, valueLists):\n",
    "    \"\"\"\n",
    "    To generate the first set of tensors from the first frame\n",
    "    Args:\n",
    "        matrixIndexAndVehicleIDRecordDictParam: just as its name\n",
    "        valueLists: a list consists of all valuelist at one time\n",
    "    Returns:\n",
    "        several tensors arranged by: positionTensor, speedTensor, accTensor, angleTensor,newVehicleList(type:list)\n",
    "    \n",
    "    \"\"\"\n",
    "    maxMatrixIndex=matrixIndexAndVehicleIDRecordDictParam.keys().__len__()-1\n",
    "    #tensors initialize\n",
    "    positionTensor=torch.zeros(2,maxMatrixIndex)\n",
    "    speedTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    accTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    angleTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    newVehicleIDList=[]\n",
    "    curMatrixIndex=0\n",
    "    matrixIndexAndVehicleIDRecordDictParam['time']=getValueByLable([\"Global_Time\"],valueLists[0])['Global_Time']\n",
    "    #fill out all tensors\n",
    "    for eachValueList in valueLists:\n",
    "        #get values from eachValueList, generate dict\n",
    "        returnedEachValueDict=getValueByLable(['Vehicle_ID','Local_X','Local_Y','v_Vel','v_Acc'],eachValueList)\n",
    "        #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "        #angle Tensor assignment is not neeed for the initial value of each element in it is already zero\n",
    "        positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "        speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "        accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "        #then handle the record matrix\n",
    "        matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['Vehicle_ID']=returnedEachValueDict['Vehicle_ID']\n",
    "        matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['refresh']=0\n",
    "        newVehicleIDList.append(copy.deepcopy(returnedEachValueDict['Vehicle_ID']))\n",
    "        curMatrixIndex=curMatrixIndex+1\n",
    "    return positionTensor,speedTensor,accTensor,angleTensor,newVehicleIDList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMatrixIndexByVehicleID(matrixIndexAndVehicleIDRecordDictParam, vehicle_ID):\n",
    "    for i in range(0, len(matrixIndexAndVehicleIDRecordDictParam)-1):\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']==vehicle_ID:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def findEmptyMatrixIndex(matrixIndexAndVehicleIDRecordDictParam):\n",
    "    for i in range(0, len(matrixIndexAndVehicleIDRecordDictParam)-1):\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']==-1:\n",
    "            #Vehicle_ID=-1 when there is no existed vehicle ID bounding to the index\n",
    "            return i\n",
    "    raise Exception(\"NO EMPTY ELEMENT IN MATRIX\")\n",
    "\n",
    "def readGeneralFrame(matrixIndexAndVehicleIDRecordDictParam, valueLists, prePositionTensor):\n",
    "    \"\"\"\n",
    "    To generate the first set of tensors from the general frame that have a preceding one.\n",
    "    In this version, we ignore the new vehicle appeared among a serial of frame.\n",
    "    Args:\n",
    "        matrixIndexAndVehicleIDRecordDictParam: just as its name\n",
    "        valueLists: a list consists of all valuelist at one time\n",
    "        prePositionTensor: positionTensor from the preceding frame, which is used to calculate angle tensor\n",
    "    Returns:\n",
    "        everal tensors arranged by: positionTensor, speedTensor, accTensor, angleTensor,newVehicleList(type:list),\n",
    "        vanishedVehicleList(type:list)\n",
    "    \n",
    "    \"\"\"\n",
    "    #tensors initialize\n",
    "    maxMatrixIndex=matrixIndexAndVehicleIDRecordDictParam.keys().__len__()-1\n",
    "    positionTensor=torch.zeros(2,maxMatrixIndex)\n",
    "    speedTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    accTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    angleTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    newVehicleIDList=[]\n",
    "    vanishedVehicleList=[]\n",
    "    curMatrixIndex=0\n",
    "    matrixIndexAndVehicleIDRecordDictParam['time']=getValueByLable([\"Global_Time\"],valueLists[0])['Global_Time']\n",
    "    #fill out all tensors\n",
    "    for eachValueList in valueLists:\n",
    "        #get values from eachValueList, generate dict\n",
    "        returnedEachValueDict=getValueByLable(['Vehicle_ID','Local_X','Local_Y','v_Vel','v_Acc'],eachValueList)\n",
    "        indexOfVehicle=findMatrixIndexByVehicleID(matrixIndexAndVehicleIDRecordDictParam,returnedEachValueDict['Vehicle_ID'])\n",
    "        if indexOfVehicle!=-1:\n",
    "        #if index exist then the vehicle already existed in the preceded frame\n",
    "            matrixIndexAndVehicleIDRecordDictParam[indexOfVehicle]['refresh']=1\n",
    "            curMatrixIndex=indexOfVehicle\n",
    "            #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "            positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "            speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "            accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "            angleTensor[:,curMatrixIndex]=math.atan2(positionTensor[0,curMatrixIndex]-\\\n",
    "                                                     prePositionTensor[0,curMatrixIndex],\\\n",
    "                                                    positionTensor[1,curMatrixIndex]-prePositionTensor[1,curMatrixIndex])\n",
    "        else:\n",
    "            pass #ignore new vehicleID\n",
    "        #a new vehicle ID\n",
    "#             newVehicleIDList.append(copy.deepcopy(returnedEachValueDict['Vehicle_ID']))\n",
    "#             curMatrixIndex=findEmptyMatrixIndex(matrixIndexAndVehicleIDRecordDictParam)\n",
    "#             matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['Vehicle_ID']=copy.deepcopy(returnedEachValueDict['Vehicle_ID'])\n",
    "#             matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['refresh']=1\n",
    "#             #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "#             positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "#             speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "#             accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "#             angleTensor[:,curMatrixIndex]=math.atan2(positionTensor[0,curMatrixIndex]-\\\n",
    "#                                                      prePositionTensor[0,curMatrixIndex],\\\n",
    "#                                                     positionTensor[1,curMatrixIndex]-prePositionTensor[1,curMatrixIndex])\n",
    "    for i in range(0,maxMatrixIndex):\n",
    "    #find vanished vehicle and remove from dict\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['refresh']==0:\n",
    "            #if refresh=0 then the corresponding vehicle ID was not found in this frame\n",
    "            vanishedVehicleList.append(copy.deepcopy(matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']))\n",
    "            matrixIndexAndVehicleIDRecordDictParam[i]['refresh']=-1\n",
    "            matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']=-1\n",
    "    \n",
    "    for i in range(0,maxMatrixIndex):\n",
    "    #set all refrshed which equivalent to 1 to 0 to prepare for the next frame\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['refresh']==1:\n",
    "                #if refresh=0 then the corresponding vehicle ID was not found in this frame\n",
    "                matrixIndexAndVehicleIDRecordDictParam[i]['refresh']=0\n",
    "\n",
    "    return positionTensor,speedTensor,accTensor,angleTensor,newVehicleIDList,vanishedVehicleList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as utils\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "if not runOnG814:\n",
    "    %matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromDirGenerateDict(trajectoryDir):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        valueDict: the key is global time, and the value of each key contain SEVERAL LIST of properties, \n",
    "                   each list consist of all property of a single vehicle at one time.\n",
    "    \"\"\"\n",
    "    trajectoryDataFile=open(trajectoryDir)\n",
    "    count=0\n",
    "    allLineList=[]\n",
    "    count=0\n",
    "    for count,line in enumerate(trajectoryDataFile):\n",
    "        #read a single line, remove space and enter\n",
    "        lineList=line.split(' ')\n",
    "        try:\n",
    "            while True:\n",
    "                lineList.remove('')\n",
    "        except:\n",
    "            try:\n",
    "                lineList.remove('\\n')\n",
    "            except:\n",
    "                pass\n",
    "            pass\n",
    "        for i in range(0,lineList.__len__()):\n",
    "            # convert string to float\n",
    "            lineList[i]=float(lineList[i])\n",
    "        allLineList.append(lineList)\n",
    "    valueDict=rearrangeDataByGlobalTime(allLineList)\n",
    "    return valueDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxAndMinValueFromValueDict(valueDict,lableList):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        valueDict: each key in dict is global time, the value of each key is a list of all value at one time\n",
    "        lableList: lables from which you want to get the max and min value. the type of each value in the list \n",
    "                    is str.\n",
    "    Returns:\n",
    "        a dict, which has keys keys from the input lable list and the value of each key is a dict which formed\n",
    "        as 'max':value, 'min':value\n",
    "    \"\"\"\n",
    "    maxAndMinDict={}\n",
    "    keys=list(valueDict.keys())\n",
    "    for lable in lableList:\n",
    "        max=0\n",
    "        min=0 #speed,  positon are all from 0 to max, accelerate from - to +\n",
    "        for eachKey in keys:\n",
    "            valueLists=valueDict[eachKey]\n",
    "            for valueList in valueLists:\n",
    "                value=getValueByLable([lable],valueList)[lable]\n",
    "                if value>max:\n",
    "                    max=value\n",
    "                if value<min:\n",
    "                    min=value\n",
    "        maxAndMinDict[lable]={'max':max,'min':min}\n",
    "    return maxAndMinDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test function of finding the max and min value: block 1, get valueDict for saving time from file reaidng\n",
    "# valueDict=fromDirGenerateDict(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test function of finding the max and min value: block 1, get valueDict for saving time from file reaidng\n",
    "# getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Acc','v_Vel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valueDict=fromDirGenerateDict(1)\n",
    "# theKey=list(valueDict.keys())[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeTensorData(xTensor,yTensor, maxLength=2500,maxWidth=100,blocksize=10,normalizationDict=False):\n",
    "    \"\"\"\n",
    "    visualize a frame on an white image\n",
    "    Args:\n",
    "        valueVisualize: a list of values, each item in the list can be obtained by function \n",
    "        getValueByLable\n",
    "    Returns:\n",
    "        the image of the input frame\n",
    "    \"\"\"\n",
    "    image=np.ones((maxLength,maxWidth,3),dtype=np.int8)\n",
    "    #set background to white\n",
    "    image=image*255\n",
    "#     figure=plt.figure(figsize=(10,50))\n",
    "#     axe=figure.add_subplot(1,1,1)\n",
    "    xLength=xTensor.shape[0] #the length of y is equivalent to x's\n",
    "#     print('length:',xLength)\n",
    "#     print('xTensor.shape',xTensor.shape)\n",
    "    if doNormalization&(normalizationDict is not False):\n",
    "        originalXTensor=torch.zeros(xLength)\n",
    "        originalYTensor=torch.zeros(xLength) #originalX and Y tensor share the same length\n",
    "        originalXTensor=torch.add(\\\n",
    "                                  torch.mul(xTensor,normalizationDict['positionXMax']-normalizationDict['positionXMin']),\\\n",
    "                                  torch.add(originalXTensor,normalizationDict['positionXMin'])\n",
    "                                 )\n",
    "        originalYTensor=torch.add(\\\n",
    "                                  torch.mul(yTensor,normalizationDict['positionYMax']-normalizationDict['positionYMin']),\\\n",
    "                                  torch.add(originalYTensor,normalizationDict['positionYMin'])\n",
    "                                 )\n",
    "        for i in range(xLength):\n",
    "            x=int(originalXTensor[i])\n",
    "            y=int(originalYTensor[i])\n",
    "            colorR=int((i*17+29)%255)\n",
    "            colorG=int((i*9++93)%255)\n",
    "            colorB=int((i*13+111)%255)\n",
    "            cv2.circle(image,(x,y),int(blocksize/2),(colorB,colorG,colorR),-1) #\n",
    "    #     axe.imshow(image)\n",
    "        return image\n",
    "        \n",
    "    \n",
    "    \n",
    "    for i in range(xLength):\n",
    "        x=int(xTensor[i])\n",
    "        y=int(yTensor[i])\n",
    "        colorR=int((i*17+29)%255)\n",
    "        colorG=int((i*9++93)%255)\n",
    "        colorB=int((i*13+111)%255)\n",
    "        cv2.circle(image,(x,y),int(blocksize/2),(colorB,colorG,colorR),-1) #\n",
    "#     axe.imshow(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeTrajectory(inputTensor, maxLength=2500, maxWidth=100,radius=5,thickness=2,normalizationDict=None, vehicleList=None):\n",
    "    \"\"\"\n",
    "    to visualize the trajectory of all or selected vehicles\n",
    "    Args:\n",
    "        inputTensor: Tensor to be visualized. The dimension of the tensor is supposed to be (batch, timestep, vehicles, properties)\n",
    "        vehicleList: If vehicleList is not none, visualize all vehicles; if not, then only visualize vehicles of the given number.\n",
    "    Returns:\n",
    "        imageList: the visualized results.\n",
    "    \"\"\"\n",
    "    imageList=[]\n",
    "    for image_i in range(inputTensor.shape[0]):\n",
    "        # set the horizontal image\n",
    "        image=np.ones((maxWidth,maxLength,3),dtype=np.int8)\n",
    "        #set background to white\n",
    "        image=image*255\n",
    "        imageList.append(image)\n",
    "    \n",
    "    if doNormalization&(normalizationDict is not False):\n",
    "        newInputTensor=torch.zeros(inputTensor.shape)\n",
    "        #To compute de-normalized x tensors\n",
    "        newInputTensor[:,:,:,0]=torch.add(\\\n",
    "                                  torch.mul(inputTensor[:,:,:,0],normalizationDict['positionXMax']-normalizationDict['positionXMin']),\\\n",
    "                                  normalizationDict['positionXMin']\n",
    "                                 )\n",
    "        newInputTensor[:,:,:,1]=torch.add(\\\n",
    "                                  torch.mul(inputTensor[:,:,:,1],normalizationDict['positionYMax']-normalizationDict['positionYMin']),\\\n",
    "                                  normalizationDict['positionYMin']\n",
    "                                 )\n",
    "        inputTensor=newInputTensor\n",
    "        \n",
    "    if vehicleList is None:\n",
    "        #vehicleList is none then visualize all vehicle\n",
    "        vehicleList=list(range(inputTensor.shape[2]))\n",
    "        \n",
    "    for batch_I in range(inputTensor.shape[0]):\n",
    "        for timestep_I in range(inputTensor.shape[1]):\n",
    "            for vehicle_I in vehicleList:\n",
    "                currentX=inputTensor[batch_I,timestep_I,vehicle_I,0]\n",
    "                currentY=inputTensor[batch_I,timestep_I,vehicle_I,1]\n",
    "                colorR=int((vehicle_I*17+29)%255)\n",
    "                colorG=int((vehicle_I*9++93)%255)\n",
    "                colorB=int((vehicle_I*13+111)%255)\n",
    "                cv2.circle(imageList[batch_I],(currentY,currentX),radius,(colorB,colorG,colorR),-1)\n",
    "                if timestep_I==0:\n",
    "                    continue\n",
    "                else:\n",
    "                    preX=inputTensor[batch_I,timestep_I-1,vehicle_I,0]\n",
    "                    preY=inputTensor[batch_I,timestep_I-1,vehicle_I,1]\n",
    "                    cv2.line(imageList[batch_I],(preY,preX),(currentY,currentX),(colorB, colorG, colorR),thickness=thickness)\n",
    "    return imageList  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test=list(range(20))\n",
    "print(test)\n",
    "print(test[15])\n",
    "print(type(test))\n",
    "\n",
    "test=None\n",
    "if test is None:\n",
    "    print(12983721908371293)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeTensorDataHorizontal(xTensor,yTensor, maxLength=2500,maxWidth=100,blocksize=10,normalizationDict=False):\n",
    "    \"\"\"\n",
    "    visualize a frame on an white image\n",
    "    Args:\n",
    "        valueVisualize: a list of values, each item in the list can be obtained by function \n",
    "        getValueByLable\n",
    "    Returns:\n",
    "        the image of the input frame\n",
    "    \"\"\"\n",
    "    image=np.ones((maxWidth,maxLength,3),dtype=np.int8)\n",
    "    #set background to white\n",
    "    image=image*255\n",
    "#     figure=plt.figure(figsize=(10,50))\n",
    "#     axe=figure.add_subplot(1,1,1)\n",
    "    xLength=xTensor.shape[0] #the length of y is equivalent to x's\n",
    "#     print('length:',xLength)\n",
    "#     print('xTensor.shape',xTensor.shape)\n",
    "    if doNormalization&(normalizationDict is not False):\n",
    "        originalXTensor=torch.zeros(xLength)\n",
    "        originalYTensor=torch.zeros(xLength) #originalX and Y tensor share the same length\n",
    "        originalXTensor=torch.add(\\\n",
    "                                  torch.mul(xTensor,normalizationDict['positionXMax']-normalizationDict['positionXMin']),\\\n",
    "                                  torch.add(originalXTensor,normalizationDict['positionXMin'])\n",
    "                                 )\n",
    "        originalYTensor=torch.add(\\\n",
    "                                  torch.mul(yTensor,normalizationDict['positionYMax']-normalizationDict['positionYMin']),\\\n",
    "                                  torch.add(originalYTensor,normalizationDict['positionYMin'])\n",
    "                                 )\n",
    "        for i in range(xLength):\n",
    "            x=int(originalXTensor[i])\n",
    "            y=int(originalYTensor[i])\n",
    "            colorR=int((i*17+29)%255)\n",
    "            colorG=int((i*9++93)%255)\n",
    "            colorB=int((i*13+111)%255)\n",
    "            cv2.circle(image,(y,x),int(blocksize/2),(colorB,colorG,colorR),-1) #\n",
    "    #     axe.imshow(image)\n",
    "        return image\n",
    "        \n",
    "    \n",
    "    \n",
    "    for i in range(xLength):\n",
    "        x=int(xTensor[i])\n",
    "        y=int(yTensor[i])\n",
    "        colorR=int((i*17+29)%255)\n",
    "        colorG=int((i*9++93)%255)\n",
    "        colorB=int((i*13+111)%255)\n",
    "        cv2.circle(image,(y,x),int(blocksize/2),(colorB,colorG,colorR),-1) #\n",
    "#     axe.imshow(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "if not runOnG814:\n",
    "    %matplotlib inline\n",
    "from IPython import display\n",
    "def visualizeData(valueVisualize, maxLength=1000,maxWidth=100,blocksize=10):\n",
    "    \"\"\"\n",
    "    visualize a frame on an white image\n",
    "    Args:\n",
    "        valueVisualize: a list of values, each item in the list can be obtained by function \n",
    "        getValueByLable\n",
    "    Returns:\n",
    "        the image of the input frame\n",
    "    \"\"\"\n",
    "    image=np.ones((maxLength,maxWidth,3),dtype=np.int8)\n",
    "    image=image*255\n",
    "#     figure=plt.figure(figsize=(10,50))\n",
    "#     axe=figure.add_subplot(1,1,1)\n",
    "    \n",
    "    for item in valueVisualize:\n",
    "        infoList=getValueByLable(['Vehicle_ID','Local_X','Local_Y'],item)\n",
    "        vehicleID=infoList['Vehicle_ID']\n",
    "        x=int(infoList['Local_X'])\n",
    "        y=int(infoList['Local_Y'])\n",
    "        colorR=int((vehicleID+100)%255)\n",
    "        colorG=int((vehicleID+150)%255)\n",
    "        colorB=int((vehicleID+200)%255)\n",
    "        cv2.circle(image,(x,y),int(blocksize/2),(colorB,colorG,colorR),-1) #\n",
    "#     axe.imshow(image)\n",
    "    return image\n",
    "# visualizeData(valueDict[theKey])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalEvaluation(resultTensor,labelTensor,normalizedDict=None,maxMatrixIndex=maxMatrixIndex):\n",
    "    \"\"\"\n",
    "    numericla evalution for models\n",
    "    REMOVE VELOCITY AND ACCELERATE FROM THE INPUT TENSOR IF THEY EXISTS IN THE INPUT TENSORS!!!\n",
    "    Args:\n",
    "        resultTensor: the predicted tensor of model, which dimension is (batch, timestep, vehicles, properties)\n",
    "        labelTensor: the label tensor from dataset, which dimension is (batch, timestep, vehicles, properties)\n",
    "        normalizedDict: the dict of normalization\n",
    "    Returns:\n",
    "        differenceEachVehicleEachFrame: the difference of each vehicle in each single frame\n",
    "        differenceEachVehicleAllFrame: the difference of each vehicle in all frame\n",
    "        averageDifferenceAllVehicleEachFrame: the average difference of all vehicle in each singel frame\n",
    "        averageDifferenceAllVehicleAllFrame: the average difference of all vehicle in over all frame\n",
    "    \"\"\"\n",
    "    differenceEachVehicleEachFrame=torch.abs(resultTensor-labelTensor)\n",
    "    differenceEachVehicleAllFrame=torch.sum(differenceEachVehicleEachFrame,dim=1,keepdim=True)\n",
    "    averageDifferenceAllVehicleEachFrame=torch.sum(differenceEachVehicleEachFrame,dim=2, keepdim=True)\n",
    "    averageDifferenceAllVehicleEachFrame=torch.div(averageDifferenceAllVehicleEachFrame,resultTensor.shape[2])\n",
    "    averageDifferenceAllVehicleAllFrame=torch.sum(averageDifferenceAllVehicleEachFrame,dim=1,keepdim=True)\n",
    "    return differenceEachVehicleEachFrame,differenceEachVehicleAllFrame,averageDifferenceAllVehicleEachFrame,\\\n",
    "            averageDifferenceAllVehicleAllFrame\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discountParameterByExponentialWithDistance(relationTensors, xfactor=1, yfactor=1, w1=1,w2=2,w3=1):\n",
    "    \"\"\"\n",
    "    to calculate a discount parameter matrix from the relations matrix of several vehicl pairs.\n",
    "    Args:\n",
    "        relationTensors: a vehicle pair is a \"relation\" in which two vehilce properties are of the same kind and order,\n",
    "        the relationTensors consists of many vehicle pairs. An extra dimension should be added to the a single vehicle pair\n",
    "        tensor if this function only take as a single vehicle pair. Generally, the relationTensors has to dimension:\n",
    "        the dimension of pairs, the properties of each pair.\n",
    "        xfactor,yfactor:  the weight of x part and y part\n",
    "        w1, w2,w3: facor in exponential operation, w1 and w2 relate to x and w3 relate to y\n",
    "    \"\"\"\n",
    "    vehiclePairDimension=relationTensors.shape[1] #the second dimension of relationTensors is the properties of each vehicle pair\n",
    "    relationsDimension=relationTensors.shape[0]\n",
    "    computationTensor=torch.zeros((relationsDimension,5)) #save the xDifference and yDifference for further computation\n",
    "    secondVehiclePropertyStartIndex=int((vehiclePairDimension)/2 )\n",
    "    logging.debug('secondVehiclePropertyStartIndex'+str(secondVehiclePropertyStartIndex))\n",
    "    \n",
    "    discountTensor=torch.zeros(relationsDimension)\n",
    "    for i in range(relationsDimension):\n",
    "        x1,y1,x2,y2=relationTensors[i][0],relationTensors[i][1],\\\n",
    "                    relationTensors[i][secondVehiclePropertyStartIndex],relationTensors[i][secondVehiclePropertyStartIndex+1]\n",
    "        if y2>=y1: #the second vehilce is in front of the first vehicle\n",
    "            yDifference=y2-y1\n",
    "            wy=w1\n",
    "        else: #the second vehicle is after the fist vehicle\n",
    "            yDifference=y1-y2\n",
    "            wy=w2\n",
    "        xDifference=abs(x1-x2)\n",
    "        wx=w3\n",
    "        computationTensor[i][0]=xDifference\n",
    "        computationTensor[i][1]=yDifference\n",
    "        computationTensor[i][2]=wx\n",
    "        computationTensor[i][3]=wy\n",
    "        if (x1==0 and y1==0)or(x2==0 and y2==0):\n",
    "            computationTensor[i][4]=0\n",
    "        else:\n",
    "            computationTensor[i][4]=1\n",
    "#         discountTensor[i]=(xfactor/math.exp(wx*(xDifference)))*(yfactor/math.exp(wx*(yDifference)))\n",
    "    logging.debug(fromAllToStr('computationTensor:\\n',computationTensor))\n",
    "    discountTensor=torch.mul(torch.mul((xfactor/torch.exp(torch.mul(computationTensor[:,0],computationTensor[:,2]))),\\\n",
    "                             (yfactor/torch.exp(torch.mul(computationTensor[:,1],computationTensor[:,3])))),\n",
    "                             computationTensor[:,4])\n",
    "    return discountTensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discountParameterByExponentialWithDistance(relationTensors, xfactor=1, yfactor=1, w1=1,w2=2,w3=1,isInModel=False):\n",
    "    \"\"\"\n",
    "    to calculate a discount parameter matrix from the relations matrix of several vehicl pairs.\n",
    "    This function is employed in model, especially for situation when cuda is used.\n",
    "    Args:\n",
    "        relationTensors: a vehicle pair is a \"relation\" in which two vehilce properties are of the same kind and order,\n",
    "        the relationTensors consists of many vehicle pairs. An extra dimension should be added to the a single vehicle pair\n",
    "        tensor if this function only take as a single vehicle pair. Generally, the relationTensors has to dimension:\n",
    "        the dimension of pairs, the properties of each pair.\n",
    "        xfactor,yfactor:  the weight of x part and y part\n",
    "        w1, w2,w3: facor in exponential operation, w1 and w2 relate to x and w3 relate to y\n",
    "    \"\"\"\n",
    "    vehiclePairDimension=relationTensors.shape[1] #the second dimension of relationTensors is the properties of each vehicle pair\n",
    "    relationsDimension=relationTensors.shape[0]\n",
    "    computationTensor=torch.zeros((relationsDimension,5)) #save the xDifference and yDifference for further computation\n",
    "    if isInModel:\n",
    "        if useGpu:\n",
    "            computationTensor=Variable(computationTensor.cuda())\n",
    "    secondVehiclePropertyStartIndex=int((vehiclePairDimension)/2)\n",
    "    logging.debug('secondVehiclePropertyStartIndex'+str(secondVehiclePropertyStartIndex))\n",
    "    discountTensor=torch.zeros(relationsDimension)\n",
    "    if isInModel:\n",
    "        if useGpu:\n",
    "            discountTensor=Variable(discountTensor.cuda())\n",
    "    for i in range(relationsDimension):\n",
    "        x1,y1,x2,y2=relationTensors[i][0],relationTensors[i][1],\\\n",
    "                    relationTensors[i][secondVehiclePropertyStartIndex],relationTensors[i][secondVehiclePropertyStartIndex+1]\n",
    "        if y2>=y1: #the second vehilce is in front of the first vehicle\n",
    "            yDifference=y2-y1\n",
    "            wy=w1\n",
    "        else: #the second vehicle is after the fist vehicle\n",
    "            yDifference=y1-y2\n",
    "            wy=w2\n",
    "        xDifference=abs(x1-x2)\n",
    "        wx=w3\n",
    "        computationTensor[i][0]=xDifference\n",
    "        computationTensor[i][1]=yDifference\n",
    "        computationTensor[i][2]=wx\n",
    "        computationTensor[i][3]=wy\n",
    "        if (x1==0 and y1==0)or(x2==0 and y2==0):\n",
    "            computationTensor[i][4]=0\n",
    "        else:\n",
    "            computationTensor[i][4]=1\n",
    "#         discountTensor[i]=(xfactor/math.exp(wx*(xDifference)))*(yfactor/math.exp(wx*(yDifference)))\n",
    "    logging.debug(fromAllToStr('computationTensor:\\n',computationTensor))\n",
    "    discountTensor=torch.mul(torch.mul((xfactor/torch.exp(torch.mul(computationTensor[:,0],computationTensor[:,2]))),\\\n",
    "                             (yfactor/torch.exp(torch.mul(computationTensor[:,1],computationTensor[:,3])))),\n",
    "                             computationTensor[:,4])\n",
    "    \n",
    "    return discountTensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input: property tensors, target vehicle ID\n",
    "#output: relations, distances between target vehicle and other vehicles which inside the range\n",
    "def relationCalculateWithRange(propertyTensors, distanceRange, targetVehicleId, maxRelationsNumber=maxRelationsNumberGlobal, isInModel=False,\\\n",
    "                              customMaxMatrixIndex=maxMatrixIndex):\n",
    "    \"\"\"\n",
    "    NOTICE:THE PROPERTIES AND DISTANCE RANGE SHOULD BOTH BE NORMALIZED OR UNNORMALIZED!!\n",
    "    Args:\n",
    "        propertyTensors:property tensors of all vehicles, at position 0 and 1 are the x and y positon of the \n",
    "        corresponding vehicle\n",
    "        distanceRange: the distance on which we decide to take other vehicles into account\n",
    "        targetVehicleId: the center vehicle which we are going to calculate\n",
    "        isInModel: True when this function is employed in neural network model.\n",
    "    \"\"\"\n",
    "    logging.debug(fromAllToStr('customMaxMatrixIndex：',customMaxMatrixIndex))\n",
    "    propertyTensorsCopy=copy.deepcopy(propertyTensors)\n",
    "    propertiesDimension=propertyTensorsCopy.shape[0]\n",
    "    reservedIndexes=[]\n",
    "    reservedIndexDistanceDict={}\n",
    "    for i in range(customMaxMatrixIndex):\n",
    "        reservedIndexes.append(i)\n",
    "    #remove vehicles which position are out-of-range\n",
    "    for i in range(propertiesDimension):\n",
    "        #simply remove the out-of-ranged vehicles by comparing y-axis before computing the distance.\n",
    "        #since most vehicles are out of the given range, this would reduce time cost\n",
    "        if abs(propertyTensorsCopy[i][1]-propertyTensors[targetVehicleId][1])>distanceRange:\n",
    "            reservedIndexes.remove(i)\n",
    "        #to compute if vehicle in the range\n",
    "        else:\n",
    "            distance=((propertyTensorsCopy[i][0]-propertyTensors[targetVehicleId][0])**2+\\\n",
    "                (propertyTensorsCopy[i][1]-propertyTensors[targetVehicleId][1])**2)**0.5\n",
    "            if distance>distanceRange:\n",
    "                reservedIndexes.remove(i)\n",
    "            else:\n",
    "                reservedIndexDistanceDict[i]=distance\n",
    "    #sort dict by value, not by key\n",
    "    sortedReservedIndexDistanceDict=sorted(reservedIndexDistanceDict.items(),key=lambda item:(item[1],item[0]))\n",
    "    #keep the top 'maxRelationsNumber' nearest vehicles in the generated relations\n",
    "    if(sortedReservedIndexDistanceDict.__len__()>maxRelationsNumber):\n",
    "        for i in range(maxRelationsNumber,sortedReservedIndexDistanceDict.__len__()):\n",
    "            reservedIndexes.remove(sortedReservedIndexDistanceDict[i][0])\n",
    "    #the final properties tensor is:\n",
    "    logging.debug(fromAllToStr('propertyTensorCopy.shape:',propertyTensorsCopy.shape))\n",
    "    logging.debug(fromAllToStr('reservedIndexes:',reservedIndexes))\n",
    "    finalPropertiesTensor=propertyTensorsCopy[reservedIndexes]\n",
    "    if(sortedReservedIndexDistanceDict.__len__()<customMaxMatrixIndex):\n",
    "        #make sure the length of all relation are equanl to the value of maxRelationNumber\n",
    "        zeroTensor=torch.zeros((maxRelationsNumber-finalPropertiesTensor.shape[0],finalPropertiesTensor.shape[1]))\n",
    "        if isInModel:\n",
    "            if useGpu:\n",
    "                zeroTensor=Variable(zeroTensor.cuda())\n",
    "        finalPropertiesTensor=torch.cat((finalPropertiesTensor,\\\n",
    "                                         zeroTensor))\n",
    "#     logging.debug('targetVehicleId'+str(targetVehicleId))\n",
    "#     logging.debug('reservedIndexes.__len__()'+str(reservedIndexes.__len__()))\n",
    "#     logging.debug('propertyTensors[targetVehicleId].shape[0]'+str(propertyTensors[targetVehicleId].shape[0]))\n",
    "    expandedTargetVehicleTensor=propertyTensors[targetVehicleId].expand(maxRelationsNumber,propertyTensors[targetVehicleId].shape[0])\n",
    "    relationTensor=torch.cat((expandedTargetVehicleTensor,finalPropertiesTensor),1)\n",
    "#     logging.debug('expandedTargetVehicleTensor:'+str(expandedTargetVehicleTensor))\n",
    "#     logging.debug('relationTensor:'+str(relationTensor))\n",
    "    return relationTensor\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeRelationAndAllTheOtherTensorsWithDistance(inputFrameTensor,theGivenRange,maxRelationsNumber=20,isInModel=False,\\\n",
    "                                                    customMaxMatrixIndex=maxMatrixIndex):\n",
    "    '''\n",
    "        to generate relation tensors, discount parameter tensor with relations and the relation quantity tensor\n",
    "    of each vehicle\n",
    "        This function employs in model, especially when cuda was used.\n",
    "    Args:\n",
    "        inputFrameTensor: vehicle properties graph, which dimension are (timestep, property, vehicle).\n",
    "        To illustrate the meaning of dimensions, supposing we have a inputFrameTensor which \n",
    "        timestep is 10, all vehicle have 6 properties and there are 250 vehicles, then the dimension of the \n",
    "        input tensor are(10, 6, 250)\n",
    "        theGivenRange: only take vehicle pairs which distance are inside the given range into account.\n",
    "    Returns:\n",
    "        relationTensor: the relation tensors of each vehicle pairs in the given range\n",
    "        discountParameterTensor: the discount tensor of each relation, computed by the distance between vehicle pairs\n",
    "        \n",
    "    '''\n",
    "    logging.debug(fromAllToStr('customMaxMatrixIndex:',customMaxMatrixIndex))\n",
    "    #circulation for batch list\n",
    "    logging.debug(fromAllToStr('inputFrameTensor.shape:',inputFrameTensor.shape))\n",
    "    timeStepSize=inputFrameTensor.shape[0]\n",
    "    for timeStepCount in range(0,timeStepSize):\n",
    "        for vehicleId in range(0, inputFrameTensor.shape[2]):\n",
    "            relationTensor=relationCalculateWithRange(inputFrameTensor[timeStepCount].permute(1,0),\n",
    "                                                      theGivenRange,vehicleId,maxRelationsNumber=maxRelationsNumber,\\\n",
    "                                                      isInModel=isInModel,customMaxMatrixIndex=customMaxMatrixIndex)\n",
    "            if vehicleId==0:\n",
    "                relationTensorInOneTimeStep=relationTensor\n",
    "            elif vehicleId>0:\n",
    "                relationTensorInOneTimeStep=torch.cat((relationTensorInOneTimeStep,relationTensor),dim=0)\n",
    "        logging.debug(fromAllToStr('relationTensorInOneTimeStep.shape:\\n',relationTensorInOneTimeStep.shape))\n",
    "        discountParameterTensorInOneTimeStep=discountParameterByExponentialWithDistance(relationTensorInOneTimeStep,\\\n",
    "                                                                                        isInModel=isInModel)\n",
    "        if timeStepCount==0:\n",
    "            relationTensorOfAllTimeSteps=relationTensorInOneTimeStep.unsqueeze(0)\n",
    "            discountParameterTensorofAllTimeSteps=discountParameterTensorInOneTimeStep.unsqueeze(0)\n",
    "        else:\n",
    "            relationTensorOfAllTimeSteps=\\\n",
    "            torch.cat((relationTensorOfAllTimeSteps,relationTensorInOneTimeStep.unsqueeze(0)),dim=0)\n",
    "            discountParameterTensorofAllTimeSteps=\\\n",
    "            torch.cat((discountParameterTensorofAllTimeSteps,discountParameterTensorInOneTimeStep.unsqueeze(0)),dim=0)\n",
    "    return relationTensorOfAllTimeSteps,discountParameterTensorofAllTimeSteps\n",
    "    \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differenceBetweenTwoFrame(frameSeries):\n",
    "    \"\"\"\n",
    "    Given a series of frame, return the difference series of those frame. Since this function \n",
    "    compute diffrences by the gap between two adjacent frames, the quantity of frame in difference \n",
    "    series is one less than the input frame series\n",
    "    Args:\n",
    "        frameSeries: input frames, which dimension is (vehicleQuantity, vehicleProperties)\n",
    "    Returns:\n",
    "        the difference series, which first dimension (quantity dimension) is one less than input frame series.\n",
    "    \"\"\"\n",
    "    frameSeriesWithoutTheFirstFrame=frameSeries[1:]\n",
    "    frameSeriesWithoutTheLastFrame=frameSeries[0:-1]\n",
    "    logging.debug(str(frameSeriesWithoutTheFirstFrame))\n",
    "    logging.debug(str(frameSeriesWithoutTheLastFrame))\n",
    "    logging.debug(str(frameSeries))\n",
    "    differenceSeries=frameSeriesWithoutTheFirstFrame-frameSeriesWithoutTheLastFrame\n",
    "    return differenceSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differenceBetweenTwoFrameForTimeSteps(frameSeries):\n",
    "    \"\"\"\n",
    "    Given several time stpes of series of frame, return the difference series of all those frames. Since this function \n",
    "    compute diffrences by the gap between two adjacent frames, the quantity of frame in difference \n",
    "    series is one less than the input frame series\n",
    "    Args:\n",
    "        frameSeries: input frames, which dimension is (timeSteps,vehicleQuantity, vehicleProperties)\n",
    "    Returns:\n",
    "        differenceSeries:the difference series, which second dimension (quantity dimension) is one less than input frame series,\n",
    "        and the dimension of differenceSeries is (timeSteps, vehicleQuantity, vehiclePropertiesDifference)\n",
    "    \"\"\"\n",
    "    \n",
    "    frameSeriesWithoutTheFirstFrame=frameSeries[1:]\n",
    "    frameSeriesWithoutTheLastFrame=frameSeries[0:-1]\n",
    "    logging.debug(str(frameSeriesWithoutTheFirstFrame.shape))\n",
    "    logging.debug(str(frameSeriesWithoutTheLastFrame.shape))\n",
    "    logging.debug(str(frameSeries.shape))\n",
    "    differenceSeries=frameSeriesWithoutTheFirstFrame-frameSeriesWithoutTheLastFrame\n",
    "    return differenceSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputTensor=torch.rand((5,5))\n",
    "logging.debug(inputTensor)\n",
    "differenceBetweenTwoFrame(inputTensor)\n",
    "logging.debug(fromAllToStr('shapesize',inputTensor.shape.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testExpand=torch.tensor(((1,2,3),(3,4,5))).expand(2,6)\n",
    "# print(testExpand)\n",
    "# theList=[1,2,3,5,7]\n",
    "# theList.remove(7)\n",
    "# d = {'lilee':25, 'wangyan':21, 'liqun':32, 'age':19}\n",
    "# print(d)\n",
    "# d=sorted(d.items(), key=lambda item:item[1])\n",
    "# print(d.__len__())\n",
    "# print(theList)\n",
    "# testTensor=torch.rand(10,10)\n",
    "# print(testTensor)\n",
    "# print(testTensor[[1,4,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save visulized images\n",
    "# for key in list(valueDict.keys())[1:10000]:\n",
    "#     image=visualizeData(valueDict[key])\n",
    "#     cv2.imwrite('visualizeFolder/image'+str(key)+'.png',image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensorsDataset(Dataset):\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=100,lableTensorEachBatch=2):\n",
    "        if(numberOfTensorsEachBatch<5):\n",
    "            raise Exception(\"THE NUMBER OF TENSORS IN EACH BATCH IS TOO SMALL\")\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the ture index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                 speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                 accTensor.mul(angleCosTensor)),0)\n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "            if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "            elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "            else:\n",
    "                allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "        return allCombineTensorTrain,allCombineTensorValid\n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class tensorsDatasetV2(Dataset):\n",
    "    \"\"\"\n",
    "    dataset for relation model\n",
    "    \"\"\"\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=1,lableTensorEachBatch=1):\n",
    "        if(numberOfTensorsEachBatch!=1 or lableTensorEachBatch!=1):\n",
    "            raise Exception(\"BOTH TRAIN AND VALID TENSOR NUMBERS SHOULD BE ONE!\")\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        if doNormalization:\n",
    "            self.positionXMax=0\n",
    "            self.positionXMin=999999\n",
    "            self.positionYMax=0\n",
    "            self.positionYMin=99999\n",
    "            self.speedMax=-100\n",
    "            self.speedMin=999999\n",
    "            self.accMax=-100\n",
    "            self.accMin=9999\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            if doNormalization:\n",
    "                #get the max and min value for normalization\n",
    "                maxAndMinDict=getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Vel','v_Acc'])\n",
    "                #position X\n",
    "                if self.positionXMax<maxAndMinDict['Local_X']['max']:\n",
    "                    self.positionXMax=maxAndMinDict['Local_X']['max']\n",
    "                if self.positionXMin>maxAndMinDict['Local_X']['min']:\n",
    "                    self.positionXMin=maxAndMinDict['Local_X']['min']\n",
    "                #position Y\n",
    "                if self.positionYMax<maxAndMinDict['Local_Y']['max']:\n",
    "                    self.positionYMax=maxAndMinDict['Local_Y']['max']\n",
    "                if self.positionYMin>maxAndMinDict['Local_Y']['min']:\n",
    "                    self.positionYMin=maxAndMinDict['Local_Y']['min']\n",
    "                #speed\n",
    "                if self.speedMax<maxAndMinDict['v_Vel']['max']:\n",
    "                    self.speedMax=maxAndMinDict['v_Vel']['max']\n",
    "                if self.speedMin>maxAndMinDict['v_Vel']['min']:\n",
    "                    self.speedMin=maxAndMinDict['v_Vel']['min']\n",
    "                #acc\n",
    "                if self.accMax<maxAndMinDict['v_Acc']['max']:\n",
    "                    self.accMax=maxAndMinDict['v_Acc']['max']\n",
    "                if self.accMin>maxAndMinDict['v_Acc']['min']:\n",
    "                    self.accMin=maxAndMinDict['v_Acc']['min']\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def getNormalizationDict(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            a dict:{'positionXMax':self.positionXMax,'positonYMax':self.self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "        '''\n",
    "        if not doNormalization:\n",
    "            raise Exception('NORMALIZATION IS NOT APPLIED')\n",
    "        return {'positionXMax':self.positionXMax,'positionYMax':self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':self.speedMin,\\\n",
    "               'accMax':self.accMax,'accMin':self.accMin}\n",
    "    \n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the ture index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        #first frame normalization\n",
    "        if doNormalization:\n",
    "#             print('before nomalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                     torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "            speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "            accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "#             print('after normalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        else:\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        #generate relation tensor for all vehicle pairs\n",
    "        print('in getitem, combinedTensor shape: ',combinedTensor.shape)\n",
    "        relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "        relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "        print('in getitem, relation tensorleft shape:',relationTensorLeft.shape)\n",
    "        print('in getitem, relationtensorright shape',relationTensorRight.shape)\n",
    "#         print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "        for i in range(1,combinedTensor.shape[1]):\n",
    "            relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                          combinedTensor[:,i].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "            relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                           torch.transpose(torch.cat((combinedTensor[:,:i],combinedTensor[:,i+1:]),1),0,1)),0)\n",
    "#         print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "        combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1)  \n",
    "        firstCombinedRelationTensor=combinedRelationTensor\n",
    "        \n",
    "        \n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            if doNormalization:\n",
    "                positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                         torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "                speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "                accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                         speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            else:\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            #generate relation tensor for all vehicle pairs\n",
    "            relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "            relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "#             print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "            for j in range(1,combinedTensor.shape[1]):\n",
    "                relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                              combinedTensor[:,j].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "                relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                               torch.transpose(torch.cat((combinedTensor[:,:j],combinedTensor[:,j+1:]),1),0,1)),0)\n",
    "#             print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "            combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1)  \n",
    "            secondRelationTensor=combinedRelationTensor\n",
    "            #since we only need two tensors, which is input and output tensor respectively, we could return\n",
    "            #the two tensors in the first loop\n",
    "            #(ok I admit that the true reason is that I am lazy)\n",
    "            return firstCombinedRelationTensor,secondRelationTensor\n",
    "#             if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "#                 allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "#             elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "#                 allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "#             else:\n",
    "#                 allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "        return allCombineTensorTrain,allCombineTensorValid\n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n",
    "#run on 2080 in g814\n",
    "if runOnG814:\n",
    "    trajectoryFileList=['/home/wangyuchen/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromObjectsToRelationPairsBatchAndTimestepVersion(batchAndTimestepCombinedTensor):\n",
    "    '''\n",
    "    This function based on the other function termed as 'fromObjectsToRelationPairs'. Instead of process a \n",
    "    single frame, this function takes batch and timestep(the other dimension) into consideration.\n",
    "    note: the dimension of combinedTensor is supposed to be (batchs, timesteps, properties, vehicles)\n",
    "    Args:\n",
    "        The input tensor should already be transposed if it is generated from the network's output\n",
    "    Returns:\n",
    "        Relation pairs\n",
    "    '''\n",
    "    #generate relation tensor for all vehicle pairs\n",
    "    batchSize=batchAndTimestepCombinedTensor.shape[0]\n",
    "    timesteps=batchAndTimestepCombinedTensor.shape[1]\n",
    "    for batch in range(batchSize):\n",
    "        for timestep in range(timesteps):\n",
    "            combinedTensor=batchAndTimestepCombinedTensor[batch,timestep,:,:]\n",
    "            relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "            relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "        #         print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "            for i in range(1,combinedTensor.shape[1]):\n",
    "                relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                              combinedTensor[:,i].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "                relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                               torch.transpose(torch.cat((combinedTensor[:,:i],combinedTensor[:,i+1:]),1),0,1)),0)\n",
    "        #         print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "            combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1) \n",
    "            if timestep==0:\n",
    "                combineRelationTensorsTimeStep=combinedRelationTensor.unsqueeze(0)\n",
    "            else:\n",
    "                combineRelationTensorsTimeStep=torch.cat((combineRelationTensorsTimeStep,\\\n",
    "                                                          combinedRelationTensor.unsqueeze(0)),0)\n",
    "        if batch==0:\n",
    "            combinedRelationTensorsTimeStepAndBatch=combineRelationTensorsTimeStep.unsqueeze(0)\n",
    "        else:\n",
    "            combinedRelationTensorsTimeStepAndBatch=torch.cat((combinedRelationTensorsTimeStepAndBatch,\\\n",
    "                                                              combineRelationTensorsTimeStep.unsqueeze(0)),0)\n",
    "    return combinedRelationTensorsTimeStepAndBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the combined tensor and relation tensor i tensorsDataV2\n",
    "import math\n",
    "class tensorsDatasetV2Test(Dataset):\n",
    "    \"\"\"\n",
    "    dataset for relation model\n",
    "    \"\"\"\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=1,lableTensorEachBatch=1):\n",
    "        if(numberOfTensorsEachBatch!=1 or lableTensorEachBatch!=1):\n",
    "            raise Exception(\"BOTH TRAIN AND VALID TENSOR NUMBERS SHOULD BE ONE!\")\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        if doNormalization:\n",
    "            self.positionXMax=0\n",
    "            self.positionXMin=999999\n",
    "            self.positionYMax=0\n",
    "            self.positionYMin=99999\n",
    "            self.speedMax=-100\n",
    "            self.speedMin=999999\n",
    "            self.accMax=-100\n",
    "            self.accMin=9999\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            if doNormalization:\n",
    "                #get the max and min value for normalization\n",
    "                maxAndMinDict=getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Vel','v_Acc'])\n",
    "                #position X\n",
    "                if self.positionXMax<maxAndMinDict['Local_X']['max']:\n",
    "                    self.positionXMax=maxAndMinDict['Local_X']['max']\n",
    "                if self.positionXMin>maxAndMinDict['Local_X']['min']:\n",
    "                    self.positionXMin=maxAndMinDict['Local_X']['min']\n",
    "                #position Y\n",
    "                if self.positionYMax<maxAndMinDict['Local_Y']['max']:\n",
    "                    self.positionYMax=maxAndMinDict['Local_Y']['max']\n",
    "                if self.positionYMin>maxAndMinDict['Local_Y']['min']:\n",
    "                    self.positionYMin=maxAndMinDict['Local_Y']['min']\n",
    "                #speed\n",
    "                if self.speedMax<maxAndMinDict['v_Vel']['max']:\n",
    "                    self.speedMax=maxAndMinDict['v_Vel']['max']\n",
    "                if self.speedMin>maxAndMinDict['v_Vel']['min']:\n",
    "                    self.speedMin=maxAndMinDict['v_Vel']['min']\n",
    "                #acc\n",
    "                if self.accMax<maxAndMinDict['v_Acc']['max']:\n",
    "                    self.accMax=maxAndMinDict['v_Acc']['max']\n",
    "                if self.accMin>maxAndMinDict['v_Acc']['min']:\n",
    "                    self.accMin=maxAndMinDict['v_Acc']['min']\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def getNormalizationDict(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            a dict:{'positionXMax':self.positionXMax,'positonYMax':self.self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "        '''\n",
    "        if not doNormalization:\n",
    "            raise Exception('NORMALIZATION IS NOT APPLIED')\n",
    "        return {'positionXMax':self.positionXMax,'positionYMax':self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':self.speedMin,\\\n",
    "               'accMax':self.accMax,'accMin':self.accMin}\n",
    "    \n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the ture index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        fileName='./'+'tensorFromGetitem'+'/'+str(10000000+idx)+'.png'\n",
    "        image=visualizeTensorData(positionTensor[0,:],positionTensor[1,:])\n",
    "        cv2.imwrite(fileName,image)\n",
    "        #first frame normalization\n",
    "        if doNormalization:\n",
    "#             print('before nomalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                     torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "            speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "            accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "#             print('after normalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        else:\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        #generate relation tensor for all vehicle pairs\n",
    "        print('in getitem, combinedTensor shape: ',combinedTensor.shape)\n",
    "        fileName='./'+'tensorFromGetitemAfterNormalization'+'/'+str(10000000+idx)+'.png'\n",
    "        image=visualizeTensorData(positionTensor[0,:],positionTensor[1,:],normalizationDict=self.getNormalizationDict())\n",
    "        cv2.imwrite(fileName,image)\n",
    "        relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "        relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "        print('in getitem, relation tensorleft shape:',relationTensorLeft.shape)\n",
    "        print('in getitem, relationtensorright shape',relationTensorRight.shape)\n",
    "#         print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "        for i in range(1,combinedTensor.shape[1]):\n",
    "            relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                          combinedTensor[:,i].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "            relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                           torch.transpose(torch.cat((combinedTensor[:,:i],combinedTensor[:,i+1:]),1),0,1)),0)\n",
    "#         print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "        combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1)  \n",
    "        firstCombinedRelationTensor=combinedRelationTensor\n",
    "        firstCombinedTensor=combinedTensor\n",
    "        \n",
    "        \n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            if doNormalization:\n",
    "                positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                         torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "                speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "                accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                         speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            else:\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            #generate relation tensor for all vehicle pairs\n",
    "            relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "            relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "#             print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "            for j in range(1,combinedTensor.shape[1]):\n",
    "                relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                              combinedTensor[:,j].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "                relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                               torch.transpose(torch.cat((combinedTensor[:,:j],combinedTensor[:,j+1:]),1),0,1)),0)\n",
    "#             print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "            combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1)  \n",
    "            secondRelationTensor=combinedRelationTensor\n",
    "            secondCombinedTensor=combinedTensor\n",
    "            #since we only need two tensors, which is input and output tensor respectively, we could return\n",
    "            #the two tensors in the first loop\n",
    "            #(ok I admit that the true reason is that I am lazy)\n",
    "            return firstCombinedTensor,secondCombinedTensor\n",
    "            return firstCombinedRelationTensor,secondRelationTensor\n",
    "#             if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "#                 allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "#             elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "#                 allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "#             else:\n",
    "#                 allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "        return allCombineTensorTrain,allCombineTensorValid\n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n",
    "#run on 2080 in g814\n",
    "if runOnG814:\n",
    "    trajectoryFileList=['/home/wangyuchen/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasetV2Test=tensorsDatasetV2Test(trajectoryFileList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(datasetV2Test.getNormalizationDict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "firstCombined,secondCombined=datasetV2Test.__getitem__(40)\n",
    "print(firstCombined.shape,secondCombined.shape)\n",
    "image=visualizeTensorData(firstCombined[0,:],firstCombined[1,:],normalizationDict=datasetV2Test.getNormalizationDict())\n",
    "dirName='combinedTensorFolder'+str(int(time.time()))\n",
    "os.mkdir(dirName)\n",
    "for i in range(0,2000):\n",
    "    fileName='./'+dirName+'/'+str(10000000+i)+'.png'\n",
    "    firstCombined,secondCombined=datasetV2Test.__getitem__(i)\n",
    "    image=visualizeTensorData(firstCombined[0,:],firstCombined[1,:],normalizationDict=datasetV2Test.getNormalizationDict())\n",
    "    cv2.imwrite(fileName,image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class tensorsDatasetV3(Dataset):\n",
    "    \"\"\"\n",
    "    dataset for relation lstm model\n",
    "    \"\"\"\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=10,lableTensorEachBatch=10):\n",
    "\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        if doNormalization:\n",
    "            self.positionXMax=0\n",
    "            self.positionXMin=999999\n",
    "            self.positionYMax=0\n",
    "            self.positionYMin=99999\n",
    "            self.speedMax=-100\n",
    "            self.speedMin=999999\n",
    "            self.accMax=-100\n",
    "            self.accMin=9999\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            if doNormalization:\n",
    "                #get the max and min value for normalization\n",
    "                maxAndMinDict=getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Vel','v_Acc'])\n",
    "                #position X\n",
    "                if self.positionXMax<maxAndMinDict['Local_X']['max']:\n",
    "                    self.positionXMax=maxAndMinDict['Local_X']['max']\n",
    "                if self.positionXMin>maxAndMinDict['Local_X']['min']:\n",
    "                    self.positionXMin=maxAndMinDict['Local_X']['min']\n",
    "                #position Y\n",
    "                if self.positionYMax<maxAndMinDict['Local_Y']['max']:\n",
    "                    self.positionYMax=maxAndMinDict['Local_Y']['max']\n",
    "                if self.positionYMin>maxAndMinDict['Local_Y']['min']:\n",
    "                    self.positionYMin=maxAndMinDict['Local_Y']['min']\n",
    "                #speed\n",
    "                if self.speedMax<maxAndMinDict['v_Vel']['max']:\n",
    "                    self.speedMax=maxAndMinDict['v_Vel']['max']\n",
    "                if self.speedMin>maxAndMinDict['v_Vel']['min']:\n",
    "                    self.speedMin=maxAndMinDict['v_Vel']['min']\n",
    "                #acc\n",
    "                if self.accMax<maxAndMinDict['v_Acc']['max']:\n",
    "                    self.accMax=maxAndMinDict['v_Acc']['max']\n",
    "                if self.accMin>maxAndMinDict['v_Acc']['min']:\n",
    "                    self.accMin=maxAndMinDict['v_Acc']['min']\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def getNormalizationDict(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            a dict:{'positionXMax':self.positionXMax,'positonYMax':self.self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "        '''\n",
    "        if not doNormalization:\n",
    "            raise Exception('NORMALIZATION IS NOT APPLIED')\n",
    "        return {'positionXMax':self.positionXMax,'positionYMax':self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':self.speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "    \n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the true index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        #first frame normalization\n",
    "        if doNormalization:\n",
    "#             print('before nomalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                     torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "            speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "            accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "#             print('after normalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        else:\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            if doNormalization:\n",
    "                positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                         torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "                speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "                accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                         speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            else:\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "            elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "            else:\n",
    "                allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "        return allCombineTensorTrain,allCombineTensorValid\n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n",
    "#run on 2080 in g814\n",
    "if runOnG814:\n",
    "    trajectoryFileList=['/home/wangyuchen/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ABOLISHED\n",
    "class computeRelationAndAllTheOtherTensorsWithDistanceThread(Process):\n",
    "    '''\n",
    "    multi-thread version of function computeRelationAndAllTehOtherTensorsWithDistance.\n",
    "    Normally, the value returned by this class is only of one batch.\n",
    "    '''\n",
    "    def __init__(self,threadID,name,timestepID,inputFrameTensor,theGivenRange,maxRelationsNumber=20):\n",
    "        '''\n",
    "        Args:\n",
    "        threadID,name,batchID,inputFrameTensor,theGivenRange,maxRelationsNumber=20\n",
    "        see details in dos string of computeRelationAndAllTheOtherTensorsWithDistance\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.threadID=threadID\n",
    "        self.name=name\n",
    "        self.timestepID=timestepID\n",
    "        self.inputFrameTensor=inputFrameTensor\n",
    "        self.theGivenRange=theGivenRange\n",
    "        self.maxRelationsNumber=maxRelationsNumber\n",
    "        \n",
    "    def run(self):\n",
    "        logging.debug(fromAllToStr('thread',self.threadID,' start'))\n",
    "        self.relationTensor,self.discountParameterTensor=computeRelationAndAllTheOtherTensorsWithDistance\\\n",
    "        (self.inputFrameTensor,self.theGivenRange,self.maxRelationsNumber) \n",
    "        logging.debug(fromAllToStr('thread',self.threadID,' finished'))\n",
    "    \n",
    "    def getValue(self):\n",
    "        '''\n",
    "        Returns:\n",
    "        relationTensor,discountParameterTensor\n",
    "        '''\n",
    "        return self.relationTensor,self.discountParameterTensor\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relationComputationWorker(relationDict, discountDict, timestepI, inputFrameTensor, theGivenRange, maxRelationsNumber):\n",
    "    relationTensor,discountParameterTensor\\\n",
    "    =computeRelationAndAllTheOtherTensorsWithDistance\\\n",
    "    (inputFrameTensor,theGivenRange,maxRelationsNumber)\n",
    "    relationDict[timestepI]=relationTensor\n",
    "    discountDict[timestepI]=discountParameterTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class tensorsDatasetV4(Dataset):\n",
    "    \"\"\"\n",
    "    dataset for difference series type\n",
    "    Returns:\n",
    "    INPUTS:\n",
    "    relationTensors,discountParameterTensors,\n",
    "    OUTPUTS:\n",
    "    allCombineTensorTrain, allCombineTensorValid,\\\n",
    "        combinedRelationTensors, combinedDiscountParameterTensors,differenceLabels\n",
    "    \"\"\"\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=10,lableTensorEachBatch=10,\\\n",
    "                maxRelationNumber=20,givenRange=0.08):\n",
    "\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        self.maxRelationNumber=maxRelationNumber\n",
    "        self.givenRange=givenRange\n",
    "        if doNormalization:\n",
    "            self.positionXMax=0\n",
    "            self.positionXMin=999999\n",
    "            self.positionYMax=0\n",
    "            self.positionYMin=99999\n",
    "            self.speedMax=-100\n",
    "            self.speedMin=999999\n",
    "            self.accMax=-100\n",
    "            self.accMin=9999\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            if doNormalization:\n",
    "                #get the max and min value for normalization\n",
    "                maxAndMinDict=getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Vel','v_Acc'])\n",
    "                #position X\n",
    "                if self.positionXMax<maxAndMinDict['Local_X']['max']:\n",
    "                    self.positionXMax=maxAndMinDict['Local_X']['max']\n",
    "                if self.positionXMin>maxAndMinDict['Local_X']['min']:\n",
    "                    self.positionXMin=maxAndMinDict['Local_X']['min']\n",
    "                #position Y\n",
    "                if self.positionYMax<maxAndMinDict['Local_Y']['max']:\n",
    "                    self.positionYMax=maxAndMinDict['Local_Y']['max']\n",
    "                if self.positionYMin>maxAndMinDict['Local_Y']['min']:\n",
    "                    self.positionYMin=maxAndMinDict['Local_Y']['min']\n",
    "                #speed\n",
    "                if self.speedMax<maxAndMinDict['v_Vel']['max']:\n",
    "                    self.speedMax=maxAndMinDict['v_Vel']['max']\n",
    "                if self.speedMin>maxAndMinDict['v_Vel']['min']:\n",
    "                    self.speedMin=maxAndMinDict['v_Vel']['min']\n",
    "                #acc\n",
    "                if self.accMax<maxAndMinDict['v_Acc']['max']:\n",
    "                    self.accMax=maxAndMinDict['v_Acc']['max']\n",
    "                if self.accMin>maxAndMinDict['v_Acc']['min']:\n",
    "                    self.accMin=maxAndMinDict['v_Acc']['min']\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def getNormalizationDict(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            a dict:{'positionXMax':self.positionXMax,'positonYMax':self.self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "        '''\n",
    "        if not doNormalization:\n",
    "            raise Exception('NORMALIZATION IS NOT APPLIED')\n",
    "        return {'positionXMax':self.positionXMax,'positionYMax':self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':self.speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "    \n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the true index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        #first frame normalization\n",
    "        if doNormalization:\n",
    "#             print('before nomalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                     torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "            speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "            accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "#             print('after normalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        else:\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            if doNormalization:\n",
    "                positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                         torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "                speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "                accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                         speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            else:\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "            elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "            else:\n",
    "                allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "#         return allCombineTensorTrain,allCombineTensorValid\n",
    "        #only consist of position tensor, ignore speed tensor and accelerate tensor\n",
    "        allCombineTensorTrain=allCombineTensorTrain[:,0:2,:]\n",
    "        allCombineTensorValid=allCombineTensorValid[:,0:2,:]\n",
    "        logging.debug(fromAllToStr('allCombineTensorTrain.shape:',allCombineTensorTrain.shape))\n",
    "        logging.debug(fromAllToStr('allCombineTensorValid.shape',allCombineTensorValid))\n",
    "        combinedRelationTensors,combinedDiscountParameterTensors=\\\n",
    "        computeRelationAndAllTheOtherTensorsWithDistance(\\\n",
    "        allCombineTensorTrain,theGivenRange=self.givenRange,\\\n",
    "        maxRelationsNumber=self.maxRelationNumber)\n",
    "        #permute the dimension order of the valid tensor\n",
    "        differenceLabels=differenceBetweenTwoFrameForTimeSteps(allCombineTensorValid.permute(0,2,1))\n",
    "        logging.debug(fromAllToStr(\"differenceLabels.shape:\",differenceLabels.shape))\n",
    "        return allCombineTensorTrain, allCombineTensorValid,\\\n",
    "        combinedRelationTensors, combinedDiscountParameterTensors,differenceLabels\n",
    "        \n",
    "        \n",
    "            \n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n",
    "#run on 2080 in g814\n",
    "if runOnG814:\n",
    "    trajectoryFileList=['/home/wangyuchen/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class tensorsDatasetV4MultiThread(Dataset):\n",
    "    \"\"\"\n",
    "    dataset for difference series type\n",
    "    Apply multi thread mode when compute relation and discount parameter\n",
    "    Returns:\n",
    "    INPUTS:\n",
    "    relationTensors,discountParameterTensors,\n",
    "    OUTPUTS:\n",
    "    allCombineTensorTrain, allCombineTensorValid,\\\n",
    "        combinedRelationTensors, combinedDiscountParameterTensors,differenceLabels\n",
    "    \"\"\"\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=10,lableTensorEachBatch=10,\\\n",
    "                maxRelationNumber=20,givenRange=0.08):\n",
    "\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        self.maxRelationNumber=maxRelationNumber\n",
    "        self.givenRange=givenRange\n",
    "        if doNormalization:\n",
    "            self.positionXMax=0\n",
    "            self.positionXMin=999999\n",
    "            self.positionYMax=0\n",
    "            self.positionYMin=99999\n",
    "            self.speedMax=-100\n",
    "            self.speedMin=999999\n",
    "            self.accMax=-100\n",
    "            self.accMin=9999\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            if doNormalization:\n",
    "                #get the max and min value for normalization\n",
    "                maxAndMinDict=getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Vel','v_Acc'])\n",
    "                #position X\n",
    "                if self.positionXMax<maxAndMinDict['Local_X']['max']:\n",
    "                    self.positionXMax=maxAndMinDict['Local_X']['max']\n",
    "                if self.positionXMin>maxAndMinDict['Local_X']['min']:\n",
    "                    self.positionXMin=maxAndMinDict['Local_X']['min']\n",
    "                #position Y\n",
    "                if self.positionYMax<maxAndMinDict['Local_Y']['max']:\n",
    "                    self.positionYMax=maxAndMinDict['Local_Y']['max']\n",
    "                if self.positionYMin>maxAndMinDict['Local_Y']['min']:\n",
    "                    self.positionYMin=maxAndMinDict['Local_Y']['min']\n",
    "                #speed\n",
    "                if self.speedMax<maxAndMinDict['v_Vel']['max']:\n",
    "                    self.speedMax=maxAndMinDict['v_Vel']['max']\n",
    "                if self.speedMin>maxAndMinDict['v_Vel']['min']:\n",
    "                    self.speedMin=maxAndMinDict['v_Vel']['min']\n",
    "                #acc\n",
    "                if self.accMax<maxAndMinDict['v_Acc']['max']:\n",
    "                    self.accMax=maxAndMinDict['v_Acc']['max']\n",
    "                if self.accMin>maxAndMinDict['v_Acc']['min']:\n",
    "                    self.accMin=maxAndMinDict['v_Acc']['min']\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def getNormalizationDict(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            a dict:{'positionXMax':self.positionXMax,'positonYMax':self.self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "        '''\n",
    "        if not doNormalization:\n",
    "            raise Exception('NORMALIZATION IS NOT APPLIED')\n",
    "        return {'positionXMax':self.positionXMax,'positionYMax':self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':self.speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "    \n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the true index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        #first frame normalization\n",
    "        if doNormalization:\n",
    "#             print('before nomalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                     torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "            speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "            accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "#             print('after normalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        else:\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            if doNormalization:\n",
    "                positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                         torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "                speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "                accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                         speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            else:\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "            elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "            else:\n",
    "                allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "#         return allCombineTensorTrain,allCombineTensorValid\n",
    "        #only consist of position tensor, ignore speed tensor and accelerate tensor\n",
    "        allCombineTensorTrain=allCombineTensorTrain[:,0:2,:]\n",
    "        allCombineTensorValid=allCombineTensorValid[:,0:2,:]\n",
    "        logging.debug(fromAllToStr('allCombineTensorTrain.shape:',allCombineTensorTrain.shape))\n",
    "        logging.debug(fromAllToStr('allCombineTensorValid.shape',allCombineTensorValid))\n",
    "        #apply multiple thread techenique to handle the time consuming relation computation\n",
    "        threadList=[]\n",
    "        manager=Manager()\n",
    "        relationDict=manager.dict()\n",
    "        discountDict=manager.dict()\n",
    "        for timestepI in range(allCombineTensorTrain.shape[0]):\n",
    "            newThread=Process(target=relationComputationWorker,\\\n",
    "                              args=(relationDict,discountDict,timestepI,\\\n",
    "                                    allCombineTensorTrain[0].squeeze().unsqueeze(0),self.givenRange,self.maxRelationNumber))\n",
    "#             newThread=computeRelationAndAllTheOtherTensorsWithDistanceThread\\\n",
    "#             (timestepI,'computeRelationThread'+str(timestepI),timestepI,\\\n",
    "#              allCombineTensorTrain[0].squeeze().unsqueeze(0),self.givenRange,self.maxRelationNumber)\n",
    "            newThread.start()\n",
    "            threadList.append(newThread)\n",
    "        timestepI=0\n",
    "        while timestepI<allCombineTensorTrain.shape[0]:\n",
    "            threadList[timestepI].join()\n",
    "            timestepI=timestepI+1\n",
    "        \n",
    "        for timestepI in range(allCombineTensorTrain.shape[0]):\n",
    "            if timestepI==0:\n",
    "                combinedRelationTensors,combinedDiscountParameterTensors\\\n",
    "                =relationDict[timestepI],discountDict[timestepI]\n",
    "            else:\n",
    "                newCombinedRelationTensors, newCombinedDiscountParameterTensors\\\n",
    "                =relationDict[timestepI],discountDict[timestepI]\n",
    "                combinedRelationTensors=torch.cat((combinedRelationTensors,newCombinedRelationTensors),0)\n",
    "                combinedDiscountParameterTensors=torch.cat((combinedDiscountParameterTensors,newCombinedDiscountParameterTensors),0)\n",
    "        \n",
    "#         combinedRelationTensors,combinedDiscountParameterTensors=\\\n",
    "#         computeRelationAndAllTheOtherTensorsWithDistance(\\\n",
    "#         allCombineTensorTrain,theGivenRange=self.givenRange,\\\n",
    "#         maxRelationsNumber=self.maxRelationNumber)\n",
    "        #permute the dimension order of the valid tensor\n",
    "        logging.debug(fromAllToStr('allCombineTensorValid.shape:',allCombineTensorValid.shape))\n",
    "        differenceLabels=differenceBetweenTwoFrameForTimeSteps(torch.cat((allCombineTensorTrain[-1].unsqueeze(0),allCombineTensorValid),0).permute(0,2,1)) \n",
    "        logging.debug(fromAllToStr(\"differenceLabels.shape:\",differenceLabels.shape))\n",
    "        return allCombineTensorTrain, allCombineTensorValid,\\\n",
    "        combinedRelationTensors, combinedDiscountParameterTensors,differenceLabels\n",
    "        \n",
    "        \n",
    "            \n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n",
    "#run on 2080 in g814\n",
    "if runOnG814:\n",
    "    trajectoryFileList=['/home/wangyuchen/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasetV2=tensorsDatasetV2(trajectoryFileList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "dataIter=iter(datasetV2)\n",
    "first,second=dataIter.__next__()\n",
    "print(first.shape, second.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxMatrixIndex=250\n",
    "dataloaderV2=DataLoader(datasetV2,batch_size=4,shuffle=True)\n",
    "for i,item in enumerate(dataloaderV2):\n",
    "    if(i>0):\n",
    "        break\n",
    "    print(i)\n",
    "    first,second=item\n",
    "    print(first.shape,second.shape)\n",
    "    print(first[0,:5,:6])\n",
    "    print(first[0,(2,245,246,247,248,249,250,251),6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def generateAdjacencyMatrix(batchedPositionTensor,lambdaX,lambdaY,omegaX,omegaY,m):\n",
    "    \"\"\"\n",
    "    Using batched position tensor generate batched adjacency matrix\n",
    "    Args:\n",
    "        batchedPositionTensor: a batch of position tensor, which size in (batch, timeSequence,2,vehicles), the \n",
    "        value 2 in dim=2 is the position of x and y. \n",
    "        lambda1,lambda2,omega1,omega2,m are parameters of the function. m<1\n",
    "        see detail in my notebook\n",
    "    Returns:\n",
    "        a batch of adjacency matrix\n",
    "    Example:\n",
    "        if given a batch of combined tensor, named theTensor, which size as below:\n",
    "            (4,100,6,250)\n",
    "        which means 4 batches, 100 time step, 6 dimension which respectively of positonx, positony, velocityx, \n",
    "        velocityy, accx,accy.\n",
    "        then we apply the function in such way:\n",
    "        generateAdjacencyMatrix(theTensor(:,:,0:1,:))\n",
    "    \"\"\"\n",
    "    print(batchedPositionTensor.size())\n",
    "    sizeOfEachMatrix=batchedPositionTensor[0,0,0,:].size()[0]\n",
    "    print(sizeOfEachMatrix)\n",
    "    for batchI in range(batchedPositionTensor.size()[0]): #revolve each batch\n",
    "#         print('batchI',batchI)\n",
    "        timeStepsMatrixList=[]\n",
    "        for timeStepI in range(batchedPositionTensor.size()[1]):#revolve each time step\n",
    "#             print('timeStepI:',timeStepI)\n",
    "#             adjacencyMatrix=np.zeros((sizeOfEachMatrix,sizeOfEachMatrix))\n",
    "            adjacencyList=[]\n",
    "            tempPositionList=batchedPositionTensor[batchI,timeStepI,:,:].numpy().tolist()\n",
    "#             start=time.time()\n",
    "            for i in range(sizeOfEachMatrix):\n",
    "                tempLineList=[]\n",
    "                for j in range(sizeOfEachMatrix):\n",
    "#                     adjacencyMatrix[i,j]=1\n",
    "                    if (tempPositionList[1][i]*tempPositionList[1][j]==0):\n",
    "                        toZero=0\n",
    "                    else:\n",
    "                        toZero=1\n",
    "                        \n",
    "                    #calculate original element with linear function\n",
    "#                     tempLineList.append((1-abs(tempPositionList[1][i]-tempPositionList[1][j]))*\\\n",
    "#                         (1-abs(tempPositionList[0][i]-tempPositionList[0][j]))*toZero)\n",
    "                    \n",
    "                    #calculate original element with exponential function\n",
    "                    element=(omegaY/(math.exp(lambdaY*(abs(tempPositionList[1][j]-tempPositionList[1][i])))))*\\\n",
    "                    (omegaX/(math.exp(lambdaX*(abs(tempPositionList[0][j]-tempPositionList[0][i])))))*toZero\n",
    "                    tempLineList.append(element)\n",
    "#                     adjacencyMatrix[i,j]=(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])*\\\n",
    "#                         (batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])\n",
    "#                     (omegaY/math.exp(lambdaX*abs(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])))*\\\n",
    "#                     (omegaX/math.exp(lambdaY*abs(batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])))\n",
    "                    \n",
    "                    #calculate original element with expenential\n",
    "#                     adjacencyMatrix[i,j]=\n",
    "#                     (omegaY/math.exp(lambdaX*abs(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])))*\\\n",
    "#                     (omegaX/math.exp(lambdaY*abs(batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])))\n",
    "                    if(tempPositionList[1][j]-tempPositionList[1][i]<0):\n",
    "                        #if i follows j, then multiple m, m<1\n",
    "                        tempLineList[j]=tempLineList[j]*m\n",
    "                adjacencyList.append(tempLineList)\n",
    "            \n",
    "#             end=time.time()\n",
    "#             print(end-start)\n",
    "            adjacencyMatrix=torch.tensor(adjacencyList).unsqueeze(0)\n",
    "            if timeStepI==0:\n",
    "                matrixSequenceInTimeStepDim=adjacencyMatrix\n",
    "            else:\n",
    "                matrixSequenceInTimeStepDim=\\\n",
    "                torch.cat((matrixSequenceInTimeStepDim,adjacencyMatrix),0)\n",
    "        matrixSequenceInTimeStepDim=matrixSequenceInTimeStepDim.unsqueeze(0)\n",
    "        if batchI==0:\n",
    "            matrixSequenceInBatchDim=matrixSequenceInTimeStepDim\n",
    "        else:\n",
    "            matrixSequenceInBatchDim=torch.cat((matrixSequenceInBatchDim,matrixSequenceInTimeStepDim),0)            \n",
    "    return matrixSequenceInBatchDim\n",
    "\n",
    "def tensorNormalization(inputTensor,minValue,maxValue):\n",
    "    inputTensor.div_(maxValue)\n",
    "    \n",
    "def batchNormalizationForCombinedTensor(inputBatchedTensor,minX,maxX,minY,maxY,minV,maxV,minA,maxA):\n",
    "    tensorNormalization(inputBatchedTensor[:,:,0,:],minX,maxX)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,1,:],minY,maxY)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,2:4,:],minV,maxV)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,4:6,:],minA,maxA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    __init__(self, input_size, cell_size, hidden_size, output_last = True)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, cell_size, hidden_size, output_last = True):\n",
    "        \"\"\"\n",
    "        cell_size is the size of cell_state.\n",
    "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, input, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((input, Hidden_State), 1)\n",
    "        f = F.sigmoid(self.fl(combined))\n",
    "        i = F.sigmoid(self.il(combined))\n",
    "        o = F.sigmoid(self.ol(combined))\n",
    "        C = F.tanh(self.Cl(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        time_step = inputs.size(1)\n",
    "        Hidden_State, Cell_State = self.initHidden(batch_size)\n",
    "        \n",
    "        if self.output_last:\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:]), Hidden_State, Cell_State)  \n",
    "            return Hidden_State\n",
    "        else:\n",
    "            outputs = None\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:]), Hidden_State, Cell_State)  \n",
    "                if outputs is None:\n",
    "                    outputs = Hidden_State.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "            return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        if use_gpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            return Hidden_State, Cell_State\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            return Hidden_State, Cell_State\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# maxMatrixIndex=250\n",
    "# trajectorDataSet=tensorsDataset(trajectoryFileList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataLoader=DataLoader(trajectorDataSet,batch_size=1,shuffle=True,num_workers=4)\n",
    "# for i,data in enumerate(dataLoader):\n",
    "#     print('11111')\n",
    "#     if(i>10):\n",
    "#         break\n",
    "#     print(data[0].shape)\n",
    "#     print(data[0][0,:,1,1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code fragment below is used to visualize tensor data\n",
    "# dataLoader=DataLoader(trajectorDataSet,batch_size=1,shuffle=True,num_workers=4)\n",
    "# for dataI,data in enumerate(dataLoader):\n",
    "#     if(dataI>10):\n",
    "#         break\n",
    "#     for i in range(int(data[0][0,:,0,0].shape[0])):\n",
    "#         tensorImage=visualizeTensorData(data[0][0,i,0,:],data[0][0,i,1,:],2500,100,10) \n",
    "#         fileName=str(100000+dataI)+'_'+str(100000+i)+'.png'\n",
    "#         cv2.imwrite('./tensorVisualizeFolder/'+fileName,tensorImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process objects to generate relation tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30,size1=10,size2=10,size3=10,size4=10):\n",
    "        super(relationNetwork,self).__init__()\n",
    "        self.size1=size1\n",
    "        self.size2=size2\n",
    "        self.size3=size3\n",
    "        self.size4=size4\n",
    "        self.layer1=nn.Linear(inputSize,size1)\n",
    "        self.layer2=nn.Linear(size1,size2)\n",
    "        self.layer3=nn.Linear(size2,size3)\n",
    "        self.layer4=nn.Linear(size3,size4)\n",
    "        self.layer5=nn.Linear(size4,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.ReLU(self.layer5(x4))\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class effectNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process relationTensor to generate effect tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30,size1=10,size2=10,size3=10,size4=10):\n",
    "        super(effectNetwork,self).__init__()\n",
    "        self.size1=size1\n",
    "        self.size2=size2\n",
    "        self.size3=size3\n",
    "        self.size4=size4\n",
    "        self.layer1=nn.Linear(inputSize,size1)\n",
    "        self.layer2=nn.Linear(size1,size2)\n",
    "        self.layer3=nn.Linear(size2,size3)\n",
    "        self.layer4=nn.Linear(size3,size4)\n",
    "        self.layer5=nn.Linear(size4,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.ReLU(self.layer5(x4))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class effectCombinationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process effect tensor to generate combined effect tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30,size1=10,size2=10,size3=10,size4=10):\n",
    "        super(effectCombinationNetwork,self).__init__()\n",
    "        self.size1=size1\n",
    "        self.size2=size2\n",
    "        self.size3=size3\n",
    "        self.size4=size4\n",
    "        self.layer1=nn.Linear(inputSize,size1)\n",
    "        self.layer2=nn.Linear(size1,size2)\n",
    "        self.layer3=nn.Linear(size2,size3)\n",
    "        self.layer4=nn.Linear(size3,size4)\n",
    "        self.layer5=nn.Linear(size4,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.ReLU(self.layer5(x4))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectModifyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    modify object with effect tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30,size1=10,size2=10,size3=10,size4=10):\n",
    "        super(objectModifyNetwork,self).__init__()\n",
    "        self.size1=size1\n",
    "        self.size2=size2\n",
    "        self.size3=size3\n",
    "        self.size4=size4\n",
    "        self.layer1=nn.Linear(inputSize,size1)\n",
    "        self.layer2=nn.Linear(size1,size2)\n",
    "        self.layer3=nn.Linear(size2,size3)\n",
    "        self.layer4=nn.Linear(size3,size4)\n",
    "        self.layer5=nn.Linear(size4,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.layer5(x4)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hiddenStateToEffectNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process hidden state tensor to generate combined effect tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30,size1=10,size2=10,size3=10,size4=10):\n",
    "        super(hiddenStateToEffectNetwork,self).__init__()\n",
    "        self.size1=size1\n",
    "        self.size2=size2\n",
    "        self.size3=size3\n",
    "        self.size4=size4\n",
    "        self.layer1=nn.Linear(inputSize,size1)\n",
    "        self.layer2=nn.Linear(size1,size2)\n",
    "        self.layer3=nn.Linear(size2,size3)\n",
    "        self.layer4=nn.Linear(size3,size4)\n",
    "        self.layer5=nn.Linear(size4,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.ReLU(self.layer5(x4))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hiddenStateToDifferenceNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process hidden state tensor to generate difference tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30,size1=10,size2=10,size3=10,size4=10):\n",
    "        super(hiddenStateToDifferenceNetwork,self).__init__()\n",
    "        self.size1=size1\n",
    "        self.size2=size2\n",
    "        self.size3=size3\n",
    "        self.size4=size4\n",
    "        self.layer1=nn.Linear(inputSize,size1)\n",
    "        self.layer2=nn.Linear(size1,size2)\n",
    "        self.layer3=nn.Linear(size2,size3)\n",
    "        self.layer4=nn.Linear(size3,size4)\n",
    "        self.layer5=nn.Linear(size4,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.ReLU(self.layer5(x4))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#this block build for relation network testing\n",
    "#delete later if needed\n",
    "relationTensorSize=40\n",
    "positionTuple=(0,1,6,7)\n",
    "velocityTuple=(2,3,8,9)\n",
    "acclerateTuple=(4,5,10,11)\n",
    "positionRelationNet=relationNetwork(outputSize=relationTensorSize)\n",
    "velocityRelationNet=relationNetwork(outputSize=relationTensorSize)\n",
    "accelerateRelationNet=relationNetwork(outputSize=relationTensorSize)\n",
    "\n",
    "maxMatrixIndex=250\n",
    "\n",
    "#load test data in network testing block\n",
    "dataloaderV2=DataLoader(datasetV2,batch_size=1,shuffle=True)\n",
    "for i,item in enumerate(dataloaderV2):\n",
    "    if(i>0):\n",
    "        break\n",
    "    print(i)\n",
    "    first,second=item\n",
    "#     print(first.shape,second.shape)\n",
    "#     print(first[0,:5,:6])\n",
    "#     print(first[0,(2,245,246,247,248,249,250,251),6:])\n",
    "    #from frame to position, velocity, accelerate\n",
    "    positionRelationTensors=positionRelationNet(first[:,:,positionTuple])\n",
    "    velocityRelationTensors=velocityRelationNet(first[:,:,velocityTuple])\n",
    "    accelerateRelationTensors=accelerateRelationNet(first[:,:,acclerateTuple])\n",
    "    print(positionRelationTensors.shape)\n",
    "    objectsAndRelationTensors=torch.cat((first[:,:,positionTuple],positionRelationTensors),2)\n",
    "    print(objectsAndRelationTensors.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "objectAndTensorSize=relationTensorSize+4 \n",
    "effectOutputTensorSize=20\n",
    "\n",
    "#the number 4 is is the size of positon(or velocity or accelerate) pairs,\n",
    "#such as (positonxObject1,positonyObject1,positionxObject2,positionyObject2)\n",
    "positionEffectNet=effectNetwork(inputSize=objectAndTensorSize,outputSize=effectOutputTensorSize)\n",
    "velocityEffectNet=effectNetwork(inputSize=objectAndTensorSize,outputSize=effectOutputTensorSize)\n",
    "accelerateEffectNet=effectNetwork(inputSize=objectAndTensorSize,outputSize=effectOutputTensorSize)\n",
    "first,second=next(iter(dataloaderV2))\n",
    "print(first.shape)\n",
    "\n",
    "#relation computation\n",
    "positionRelationTensors=positionRelationNet(first[:,:,positionTuple])\n",
    "velocityRelationTensors=velocityRelationNet(first[:,:,velocityTuple])\n",
    "accelerateRelationTensors=accelerateRelationNet(first[:,:,acclerateTuple])\n",
    "                                                      \n",
    "objectsAndPositionRelationTensors=torch.cat((first[:,:,positionTuple],positionRelationTensors),2)\n",
    "objectsAndVelocityRelationTensors=torch.cat((first[:,:,positionTuple],velocityRelationTensors),2)\n",
    "objectsAndAccelerateRelationTensors=torch.cat((first[:,:,positionTuple],accelerateRelationTensors),2)\n",
    "\n",
    "#effect computation\n",
    "positionEffectTensors=positionEffectNet(objectsAndPositionRelationTensors)\n",
    "velocityEffectTensors=velocityEffectNet(objectsAndVelocityRelationTensors)\n",
    "accelerateEffectTensors=accelerateEffectNet(objectsAndAccelerateRelationTensors)\n",
    "\n",
    "#effect combination type 1\n",
    "#tensor summation\n",
    "batchSize=positionRelationTensors.shape[0]\n",
    "positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "for i in range(maxMatrixIndex):\n",
    "    positionEffectSummation[:,i,:]=torch.sum(positionEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "    velocityEffectSummation[:,i,:]=torch.sum(velocityEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "    accelerateEffectSummation[:,i,:]=torch.sum(accelerateEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "    \n",
    "#effect combination net initial\n",
    "#combined tensors length\n",
    "combinedTensorSize=20\n",
    "effectCombinationNet=effectCombinationNetwork(inputSize=effectOutputTensorSize*3,outputSize=combinedTensorSize)\n",
    "\n",
    "#combine tensors and process the combined one\n",
    "combinedEffectTensors=torch.cat((positionEffectSummation,velocityEffectSummation,accelerateEffectSummation),2)\n",
    "print(combinedEffectTensors.shape)\n",
    "processedCombinedEffectTensors=effectCombinationNet(combinedEffectTensors)\n",
    "print(processedCombinedEffectTensors.shape)\n",
    "\n",
    "#generate a tuple in which each element is the index of a vehicle\n",
    "#the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "#this part has been put into init function of Module class\n",
    "listForEachVehicle=[]\n",
    "for i in range(maxMatrixIndex):\n",
    "    listForEachVehicle.append(i*(maxMatrixIndex-1))\n",
    "tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "\n",
    "#the property of each vehicle\n",
    "vehicleProperty=first[:,tupleForEachVehicle,0:6]\n",
    "\n",
    "objectAndFinalEffect=torch.cat((vehicleProperty,processedCombinedEffectTensors),2)\n",
    "print(objectAndFinalEffect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#initialize the final network to generate new objects properties\n",
    "objectModifyNet=objectModifyNetwork(inputSize=combinedTensorSize+6,outputSize=6)\n",
    "finalObjectState=objectModifyNet(objectAndFinalEffect)\n",
    "print(finalObjectState.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from relation to new object network\n",
    "class fromRelationToObjectNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(fromRelationToObjectNetwork,self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        \n",
    "        #position relation network initialize\n",
    "        self.relationTensorSize=40\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "        self.velocityTuple=(2,3,8,9)\n",
    "        self.acclerateTuple=(4,5,10,11)\n",
    "        self.positionRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        self.velocityRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        self.accelerateRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        \n",
    "        #effect network initialize\n",
    "        self.objectAndTensorSize=self.relationTensorSize+4 \n",
    "        self.effectOutputTensorSize=20\n",
    "        #the number 4 is is the size of positon(or velocity or accelerate) pairs,\n",
    "        #such as (positonxObject1,positonyObject1,positionxObject2,positionyObject2)\n",
    "        self.positionEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        self.velocityEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        self.accelerateEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        \n",
    "        #effect combination net initialize\n",
    "        #combined tensors length\n",
    "        self.combinedTensorSize=20\n",
    "        self.effectCombinationNet=\\\n",
    "        effectCombinationNetwork(inputSize=self.effectOutputTensorSize*3,outputSize=self.combinedTensorSize)\n",
    "        \n",
    "        #initialize the final network to generate new objects properties\n",
    "        self.objectModifyNet=objectModifyNetwork(inputSize=self.combinedTensorSize+6,outputSize=6)\n",
    "        \n",
    "    def forward(self,inputObjectsPairs):\n",
    "        #relation computation\n",
    "        positionRelationTensors=self.positionRelationNet(inputObjectsPairs[:,:,self.positionTuple])\n",
    "        velocityRelationTensors=self.velocityRelationNet(inputObjectsPairs[:,:,self.velocityTuple])\n",
    "        accelerateRelationTensors=self.accelerateRelationNet(inputObjectsPairs[:,:,self.acclerateTuple])\n",
    "\n",
    "        objectsAndPositionRelationTensors=torch.cat((inputObjectsPairs[:,:,self.positionTuple],positionRelationTensors),2)\n",
    "        objectsAndVelocityRelationTensors=torch.cat((inputObjectsPairs[:,:,self.velocityTuple],velocityRelationTensors),2)\n",
    "        objectsAndAccelerateRelationTensors=torch.cat((inputObjectsPairs[:,:,self.acclerateTuple],accelerateRelationTensors),2)\n",
    "\n",
    "        #effect computation\n",
    "        positionEffectTensors=self.positionEffectNet(objectsAndPositionRelationTensors)\n",
    "        velocityEffectTensors=self.velocityEffectNet(objectsAndVelocityRelationTensors)\n",
    "        accelerateEffectTensors=self.accelerateEffectNet(objectsAndAccelerateRelationTensors)\n",
    "\n",
    "        \n",
    "        #effect combination type 1\n",
    "        #tensor summation\n",
    "        batchSize=positionRelationTensors.shape[0]\n",
    "        if useGpu==True:\n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "            velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "            accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "        else: \n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "            velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "            accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "        for i in range(maxMatrixIndex):\n",
    "            positionEffectSummation[:,i,:]=torch.sum(positionEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "            velocityEffectSummation[:,i,:]=torch.sum(velocityEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "            accelerateEffectSummation[:,i,:]=torch.sum(accelerateEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "        combinedEffectTensors=torch.cat((positionEffectSummation,velocityEffectSummation,accelerateEffectSummation),2)\n",
    "        processedCombinedEffectTensors=self.effectCombinationNet(combinedEffectTensors)\n",
    "        \n",
    "        #the property of each vehicle\n",
    "        vehicleProperty=inputObjectsPairs[:,self.tupleForEachVehicle,0:6]\n",
    "        objectAndFinalEffect=torch.cat((vehicleProperty,processedCombinedEffectTensors),2)\n",
    "        \n",
    "        #compute final state\n",
    "        finalObjectState=self.objectModifyNet(objectAndFinalEffect)\n",
    "        return finalObjectState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This module is supposed to be placed in LSTM model\n",
    "class fromRelationToEffectNetwork(nn.Module):\n",
    "    def __init__(self,effectOutputTensorSize=20):\n",
    "        super(fromRelationToEffectNetwork,self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        \n",
    "        #position relation network initialize\n",
    "        self.relationTensorSize=40\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "        self.velocityTuple=(2,3,8,9)\n",
    "        self.acclerateTuple=(4,5,10,11)\n",
    "        self.positionRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        self.velocityRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        self.accelerateRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        \n",
    "        #effect network initialize\n",
    "        self.objectAndTensorSize=self.relationTensorSize+4 \n",
    "        self.effectOutputTensorSize=effectOutputTensorSize\n",
    "        #the number 4 is the size of positon(or velocity or accelerate) pairs,\n",
    "        #such as (positonxObject1,positonyObject1,positionxObject2,positionyObject2)\n",
    "        self.positionEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        self.velocityEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        self.accelerateEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        \n",
    "        #effect combination net initialize\n",
    "        #combined tensors length\n",
    "        self.combinedTensorSize=20\n",
    "        self.effectCombinationNet=\\\n",
    "        effectCombinationNetwork(inputSize=self.effectOutputTensorSize*3,outputSize=self.combinedTensorSize)\n",
    "        \n",
    "        #remove objectModifyNet in objectToEffectModel\n",
    "        #initialize the final network to generate new objects properties\n",
    "#         self.objectModifyNet=objectModifyNetwork(inputSize=self.combinedTensorSize+6,outputSize=6)\n",
    "        \n",
    "    def forward(self,inputObjectsPairs):\n",
    "        #relation computation\n",
    "        effectOutputTensorSize=self.effectOutputTensorSize\n",
    "        positionRelationTensors=self.positionRelationNet(inputObjectsPairs[:,:,self.positionTuple])\n",
    "        velocityRelationTensors=self.velocityRelationNet(inputObjectsPairs[:,:,self.velocityTuple])\n",
    "        accelerateRelationTensors=self.accelerateRelationNet(inputObjectsPairs[:,:,self.acclerateTuple])\n",
    "\n",
    "        objectsAndPositionRelationTensors=torch.cat((inputObjectsPairs[:,:,self.positionTuple],positionRelationTensors),2)\n",
    "        objectsAndVelocityRelationTensors=torch.cat((inputObjectsPairs[:,:,self.velocityTuple],velocityRelationTensors),2)\n",
    "        objectsAndAccelerateRelationTensors=torch.cat((inputObjectsPairs[:,:,self.acclerateTuple],accelerateRelationTensors),2)\n",
    "\n",
    "        #effect computation\n",
    "        positionEffectTensors=self.positionEffectNet(objectsAndPositionRelationTensors)\n",
    "        velocityEffectTensors=self.velocityEffectNet(objectsAndVelocityRelationTensors)\n",
    "        accelerateEffectTensors=self.accelerateEffectNet(objectsAndAccelerateRelationTensors)\n",
    "\n",
    "        \n",
    "        #effect combination type 1\n",
    "        #tensor summation\n",
    "        batchSize=positionRelationTensors.shape[0]\n",
    "        if useGpu==True:\n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "            velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "            accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "        else: \n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "            velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "            accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "        for i in range(maxMatrixIndex):\n",
    "            positionEffectSummation[:,i,:]=torch.sum(positionEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "            velocityEffectSummation[:,i,:]=torch.sum(velocityEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "            accelerateEffectSummation[:,i,:]=torch.sum(accelerateEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "        combinedEffectTensors=torch.cat((positionEffectSummation,velocityEffectSummation,accelerateEffectSummation),2)\n",
    "        processedCombinedEffectTensors=self.effectCombinationNet(combinedEffectTensors)\n",
    "        \n",
    "        #remove object extraction component and computation component\n",
    "#         #the property of each vehicle\n",
    "#         vehicleProperty=inputObjectsPairs[:,self.tupleForEachVehicle,0:6]\n",
    "#         objectAndFinalEffect=torch.cat((vehicleProperty,processedCombinedEffectTensors),2)\n",
    "        \n",
    "#         #compute final state\n",
    "#         finalObjectState=self.objectModifyNet(objectAndFinalEffect)\n",
    "        return processedCombinedEffectTensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This module is supposed to be placed in LSTM model\n",
    "class fromRelationToEffectNetworkPositionOnly(nn.Module):\n",
    "    def __init__(self,effectOutputTensorSize=20):\n",
    "        super(fromRelationToEffectNetworkPositionOnly,self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        \n",
    "        #position relation network initialize\n",
    "        self.relationTensorSize=40\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "#         self.velocityTuple=(2,3,8,9)\n",
    "#         self.acclerateTuple=(4,5,10,11)\n",
    "        self.positionRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "#         self.velocityRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "#         self.accelerateRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        \n",
    "        #effect network initialize\n",
    "        self.objectAndTensorSize=self.relationTensorSize+4 \n",
    "        self.effectOutputTensorSize=effectOutputTensorSize\n",
    "        #the number 4 is the size of positon(or velocity or accelerate) pairs,\n",
    "        #such as (positonxObject1,positonyObject1,positionxObject2,positionyObject2)\n",
    "        self.positionEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "#         self.velocityEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "#         self.accelerateEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        \n",
    "        #effect combination net initialize\n",
    "        #combined tensors length\n",
    "#         self.combinedTensorSize=20\n",
    "#         self.effectCombinationNet=\\\n",
    "#         effectCombinationNetwork(inputSize=self.effectOutputTensorSize*3,outputSize=self.combinedTensorSize)\n",
    "        \n",
    "        #remove objectModifyNet in objectToEffectModel\n",
    "        #initialize the final network to generate new objects properties\n",
    "#         self.objectModifyNet=objectModifyNetwork(inputSize=self.combinedTensorSize+6,outputSize=6)\n",
    "        \n",
    "    def forward(self,inputObjectsPairs):\n",
    "        #relation computation\n",
    "        effectOutputTensorSize=self.effectOutputTensorSize\n",
    "        positionRelationTensors=self.positionRelationNet(inputObjectsPairs[:,:,self.positionTuple])\n",
    "#         velocityRelationTensors=self.velocityRelationNet(inputObjectsPairs[:,:,self.velocityTuple])\n",
    "#         accelerateRelationTensors=self.accelerateRelationNet(inputObjectsPairs[:,:,self.acclerateTuple])\n",
    "\n",
    "        objectsAndPositionRelationTensors=torch.cat((inputObjectsPairs[:,:,self.positionTuple],positionRelationTensors),2)\n",
    "#         objectsAndVelocityRelationTensors=torch.cat((inputObjectsPairs[:,:,self.velocityTuple],velocityRelationTensors),2)\n",
    "#         objectsAndAccelerateRelationTensors=torch.cat((inputObjectsPairs[:,:,self.acclerateTuple],accelerateRelationTensors),2)\n",
    "\n",
    "        #effect computation\n",
    "        positionEffectTensors=self.positionEffectNet(objectsAndPositionRelationTensors)\n",
    "#         velocityEffectTensors=self.velocityEffectNet(objectsAndVelocityRelationTensors)\n",
    "#         accelerateEffectTensors=self.accelerateEffectNet(objectsAndAccelerateRelationTensors)\n",
    "\n",
    "        \n",
    "        #effect combination type 1\n",
    "        #tensor summation\n",
    "        batchSize=positionRelationTensors.shape[0]\n",
    "        if useGpu==True:\n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "#             velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "#             accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "        else: \n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "#             velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "#             accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "        for i in range(maxMatrixIndex):\n",
    "            positionEffectSummation[:,i,:]=torch.sum(positionEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "#             velocityEffectSummation[:,i,:]=torch.sum(velocityEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "#             accelerateEffectSummation[:,i,:]=torch.sum(accelerateEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "#         combinedEffectTensors=torch.cat((positionEffectSummation,velocityEffectSummation,accelerateEffectSummation),2)\n",
    "#         processedCombinedEffectTensors=self.effectCombinationNet(combinedEffectTensors)\n",
    "        \n",
    "        #remove object extraction component and computation component\n",
    "#         #the property of each vehicle\n",
    "#         vehicleProperty=inputObjectsPairs[:,self.tupleForEachVehicle,0:6]\n",
    "#         objectAndFinalEffect=torch.cat((vehicleProperty,processedCombinedEffectTensors),2)\n",
    "        \n",
    "#         #compute final state\n",
    "#         finalObjectState=self.objectModifyNet(objectAndFinalEffect)\n",
    "        return positionEffectSummation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationLSTM(nn.Module):\n",
    "    def __init__(self, input_size=20, cell_size=20, hidden_size=20, output_last = True):\n",
    "        \"\"\"\n",
    "        cell_size is the size of cell_state.\n",
    "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
    "        the input of LSTM  structure was the output of 'fromRelationToEffectNet' module, so that \n",
    "        the effectOutputTensorSize has the same number as input_size. \n",
    "        \n",
    "        \"\"\"\n",
    "        super(RelationLSTM, self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        \n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "        self.velocityTuple=(2,3,8,9)\n",
    "        self.acclerateTuple=(4,5,10,11)\n",
    "        \n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.fromRelationToEffectNet=fromRelationToEffectNetwork(effectOutputTensorSize=input_size)\n",
    "        self.objectModifyNet=objectModifyNetwork(inputSize=hidden_size+6,outputSize=6)\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, inputEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputEffectTensor, Hidden_State), 2)\n",
    "        print('in step,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl(combined))\n",
    "        i = F.sigmoid(self.il(combined))\n",
    "        o = F.sigmoid(self.ol(combined))\n",
    "        C = F.tanh(self.Cl(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        time_step = inputs.size(1)\n",
    "        Hidden_State, Cell_State = self.initHidden(batch_size)\n",
    "        \n",
    "        if self.output_last:\n",
    "            for i in range(time_step):\n",
    "                effects=self.fromRelationToEffectNet(inputs[:,i,:,:])\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(effects), Hidden_State, Cell_State) \n",
    "                #the property of each vehicle\n",
    "            print(inputs.shape)\n",
    "            vehicleProperty=inputs[:,-1,self.tupleForEachVehicle,0:6].squeeze()\n",
    "            objectAndFinalEffect=torch.cat((vehicleProperty,Hidden_State),2)\n",
    "            outputState=self.objectModifyNet(objectAndFinalEffect)\n",
    "            return outputState\n",
    "        else:\n",
    "            outputs = None\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:,:]), Hidden_State, Cell_State)  \n",
    "                if outputs is None:\n",
    "                    outputs = Hidden_State.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "            return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "#         use_gpu = torch.cuda.is_available()\n",
    "        if useGpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Cell_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            return Hidden_State, Cell_State\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            Cell_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            return Hidden_State, Cell_State\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def fromObjectsToRelationPairs(combinedTensor):\n",
    "    '''\n",
    "    Args:\n",
    "        The input tensor should already be transposed if it is generated from the network's output\n",
    "    Returns:\n",
    "        Relation pairs\n",
    "    '''\n",
    "    #generate relation tensor for all vehicle pairs\n",
    "    relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "    relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "#         print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "    for i in range(1,combinedTensor.shape[1]):\n",
    "        relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                      combinedTensor[:,i].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "        relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                       torch.transpose(torch.cat((combinedTensor[:,:i],combinedTensor[:,i+1:]),1),0,1)),0)\n",
    "#         print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "    combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1) \n",
    "    return combinedRelationTensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationLSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, input_size=20, cell_size=20, hidden_size=20, \\\n",
    "                 input_size_2=20, cell_size_2=20,hidden_size_2=20, outputTimeFrame=5,output_last = True):\n",
    "        \"\"\"\n",
    "        cell_size is the size of cell_state.\n",
    "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
    "        the input of LSTM  structure was the output of 'fromRelationToEffectNet' module, so that \n",
    "        the effectOutputTensorSize has the same number as input_size. \n",
    "        \n",
    "        \"\"\"\n",
    "        super(RelationLSTMSeq2Seq, self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        \n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "        self.velocityTuple=(2,3,8,9)\n",
    "        self.acclerateTuple=(4,5,10,11)\n",
    "        \n",
    "        #effect representive vector computation lstm\n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        #object modifying lstm\n",
    "        self.cell_size2 = cell_size_2\n",
    "        self.hidden_size2 = hidden_size_2\n",
    "        self.fl2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.il2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.ol2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.Cl2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        \n",
    "        self.outputTimeFrame=outputTimeFrame\n",
    "\n",
    "        self.fromRelationToEffectNet=fromRelationToEffectNetwork(effectOutputTensorSize=input_size)\n",
    "        self.objectModifyNet=objectModifyNetwork(inputSize=hidden_size+6,outputSize=6)\n",
    "        self.hiddenStateToEffectNetwork=hiddenStateToEffectNetwork(inputSize=hidden_size,outputSize=input_size)\n",
    "        #descreption for the statement above: the input_size applied to the parameter outputSize is \n",
    "        #the size of effect tensor in the init function. \n",
    "        \n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, inputEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputEffectTensor, Hidden_State), 2)\n",
    "#         print('in step,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl(combined))\n",
    "        i = F.sigmoid(self.il(combined))\n",
    "        o = F.sigmoid(self.ol(combined))\n",
    "        C = F.tanh(self.Cl(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def step2(self, inputPreEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputPreEffectTensor, Hidden_State), 2)\n",
    "#         print('in step2,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl2(combined))\n",
    "        i = F.sigmoid(self.il2(combined))\n",
    "        o = F.sigmoid(self.ol2(combined))\n",
    "        C = F.tanh(self.Cl2(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        time_step = inputs.size(1)\n",
    "        Hidden_State, Cell_State,Hidden_State_2,Cell_State_2 = self.initHidden(batch_size)\n",
    "        \n",
    "        if self.output_last:\n",
    "            #effect computation\n",
    "            for i in range(time_step):\n",
    "                effects=self.fromRelationToEffectNet(inputs[:,i,:,:])\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(effects), Hidden_State, Cell_State) \n",
    "                #the property of each vehicle\n",
    "                \n",
    "            Hidden_State_2=Hidden_State\n",
    "            Cell_State_2=Cell_State\n",
    "            \n",
    "            #the lstm process below take as inputs the previous object states and output the next predicted states\n",
    "            #applying function permute to deal with the dimension inconsistency\n",
    "            '''\n",
    "            inputs should be processed by function \"fromObjectsToRelationPairsBatchAndTimestepVersion\" \n",
    "            IF DATASETV3 VERSION IS USED TO GET DATA\n",
    "            '''\n",
    "            print('in seq2seq, inputs shape:',inputs.shape)\n",
    "            preVehicleProperty=inputs[:,-1,self.tupleForEachVehicle,0:6].squeeze()\n",
    "            preVehicleRelations=inputs[:,-1,:,:]\n",
    "            \n",
    "            #object properties computation\n",
    "            for i in range(outputTimeFrame):\n",
    "                objectAndFinalEffect=torch.cat((preVehicleProperty,Hidden_State_2),2)  \n",
    "                preVehicleProperty=self.objectModifyNet(objectAndFinalEffect)\n",
    "                effects=self.fromRelationToEffectNet(preVehicleRelations)\n",
    "                Hidden_State_2,Cell_State_2=self.step2(torch.squeeze(effects),Hidden_State_2,Cell_State_2)\n",
    "                if i==0:\n",
    "                    #add dimension timestep, which in dimension 1\n",
    "                    outputVehicleProperties=preVehicleProperty.unsqueeze(1) \n",
    "                else:\n",
    "                    outputVehicleProperties=torch.cat((outputVehicleProperties,preVehicleProperty.unsqueeze(1)),1)\n",
    "                #don't need to compute new effect vector after the prediction of the last time step\n",
    "                if i<outputTimeFrame-1:\n",
    "                    effectFromHiddenState=self.hiddenStateToEffectNetwork(Hidden_State_2)\n",
    "                    self.objectModifyNet()\n",
    "                    preVehicleProperty.cpu()\n",
    "                    #add timestep dimension for further process\n",
    "                    preVehiclePropertyAddTimestep=preVehicleProperty.unsqueeze(1)\n",
    "                    relationPairs=fromObjectsToRelationPairsBatchAndTimestepVersion(preVehiclePropertyAddTimestep).squeeze()\n",
    "                    if useGpu:\n",
    "                        relationPairs.cuda()\n",
    "                    effectInPropertiesComputation=self.fromRelationToEffectNet(relationPairs)\n",
    "                    Hidden_State_2,Cell_State_2=self.step2(effectInPropertiesComputation,Hidden_State_2,Cell_State_2)\n",
    "                if i==outputTimeFrame:\n",
    "                    break\n",
    "#             print(inputs.shape)\n",
    "#             vehicleProperty=inputs[:,-1,self.tupleForEachVehicle,0:6].squeeze()\n",
    "#             objectAndFinalEffect=torch.cat((vehicleProperty,Hidden_State),2)\n",
    "#             outputState=self.objectModifyNet(objectAndFinalEffect)\n",
    "            return outputVehicleProperties\n",
    "        else:\n",
    "            outputs = None\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:,:]), Hidden_State, Cell_State)  \n",
    "                if outputs is None:\n",
    "                    outputs = Hidden_State.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "            return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "#         use_gpu = torch.cuda.is_available()\n",
    "        if useGpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Cell_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Hidden_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2).cuda())\n",
    "            Cell_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2).cuda())\n",
    "            return Hidden_State, Cell_State,Hidden_State_2,Cell_State_2\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            Cell_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            Hidden_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2))\n",
    "            Cell_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2))\n",
    "            return Hidden_State, Cell_State,Hidden_State_2,Cell_State_2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationLSTMSeq2SeqPositionOnly(nn.Module):\n",
    "    def __init__(self, input_size=20, cell_size=20, hidden_size=20, \\\n",
    "                 input_size_2=20, cell_size_2=20,hidden_size_2=20, outputTimeFrame=5,output_last = True):\n",
    "        \"\"\"\n",
    "        cell_size is the size of cell_state.\n",
    "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
    "        the input of LSTM  structure is the output of 'fromRelationToEffectNet' module, so that \n",
    "        the effectOutputTensorSize has the same number as input_size. \n",
    "        \n",
    "        \"\"\"\n",
    "        super(RelationLSTMSeq2SeqPositionOnly, self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        \n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "#         self.velocityTuple=(2,3,8,9)\n",
    "#         self.acclerateTuple=(4,5,10,11)\n",
    "        \n",
    "        #effect representive vector computation lstm\n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        #object modifying lstm\n",
    "        self.cell_size2 = cell_size_2\n",
    "        self.hidden_size2 = hidden_size_2\n",
    "        self.fl2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.il2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.ol2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.Cl2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        \n",
    "        self.outputTimeFrame=outputTimeFrame\n",
    "\n",
    "        self.fromRelationToEffectNet=fromRelationToEffectNetworkPositionOnly(effectOutputTensorSize=input_size)\n",
    "        self.objectModifyNet=objectModifyNetwork(inputSize=input_size+6,outputSize=6)\n",
    "        self.hiddenStateToEffectNetwork=hiddenStateToEffectNetwork(inputSize=hidden_size,outputSize=input_size)\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, inputEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputEffectTensor, Hidden_State), 2)\n",
    "#         print('in step,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl(combined))\n",
    "        i = F.sigmoid(self.il(combined))\n",
    "        o = F.sigmoid(self.ol(combined))\n",
    "        C = F.tanh(self.Cl(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def step2(self, inputPreEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputPreEffectTensor, Hidden_State), 2)\n",
    "#         print('in step2,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl2(combined))\n",
    "        i = F.sigmoid(self.il2(combined))\n",
    "        o = F.sigmoid(self.ol2(combined))\n",
    "        C = F.tanh(self.Cl2(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        time_step = inputs.size(1)\n",
    "        Hidden_State, Cell_State,Hidden_State_2,Cell_State_2 = self.initHidden(batch_size)\n",
    "        \n",
    "        if self.output_last:\n",
    "            #effect computation\n",
    "            for i in range(time_step):\n",
    "                effects=self.fromRelationToEffectNet(inputs[:,i,:,:])\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(effects), Hidden_State, Cell_State) \n",
    "                #the property of each vehicle\n",
    "                \n",
    "            Hidden_State_2=Hidden_State\n",
    "            Cell_State_2=Cell_State\n",
    "            \n",
    "            #the lstm process below take as inputs the previous object states and output the next predicted states\n",
    "            #applying function permute to deal with the dimension inconsistency\n",
    "            '''\n",
    "            inputs should be processed by function \"fromObjectsToRelationPairsBatchAndTimestepVersion\" \n",
    "            IF DATASETV3 VERSION IS USED TO GET DATA\n",
    "            '''\n",
    "            print('in seq2seq, inputs shape:',inputs.shape)\n",
    "            preVehicleProperty=inputs[:,-1,self.tupleForEachVehicle,0:6].squeeze()\n",
    "            preRelations=inputs[:,-1,:,:]\n",
    "            #object properties computation\n",
    "            for i in range(outputTimeFrame):\n",
    "                #objectAndFinalEffect=torch.cat((preVehicleProperty,Hidden_State_2),2)  #??????????\n",
    "                #WRONG!!!!The final hidden state of the first lstm model is not a effect tensor!!\n",
    "                #The hidden state of the first lstm represents somehow the encode hidden state in translation network,\n",
    "                #and obviously, the hidden state of encode component in translation network \n",
    "                #does not contain the result of translation. We could get the result from the output of\n",
    "                #the decode part of the translation network\n",
    "                #the shape of input tensor to the function fromRelationToEffectNet is (batch, vehicles, properties)\n",
    "                preEffect=self.fromRelationToEffectNet(preRelations)#the effect produced by this function is already a combined effect\n",
    "                logging.debug(fromAllToStr('preEffect.shape: ',preEffect.shape))\n",
    "                Hidden_State_2,Cell_State_2=self.step2(preEffect,Hidden_State_2,Cell_State_2)\n",
    "                logging.debug(fromAllToStr('Hidden_State_2.shape: ',Hidden_State_2.shape))\n",
    "                effectFromHiddenState=self.hiddenStateToEffectNetwork(Hidden_State_2)\n",
    "                logging.debug(fromAllToStr('effectFromeHiddenState.shape: ',effectFromHiddenState.shape))\n",
    "                objectAndEffectTensor=torch.cat((effectFromHiddenState,preVehicleProperty),2)\n",
    "                logging.debug(fromAllToStr('objectAndEffectTensor.shape: ',objectAndEffectTensor.shape))\n",
    "                modifiedObject=self.objectModifyNet(objectAndEffectTensor)\n",
    "                logging.debug(fromAllToStr('modifiedObject.shape: ',modifiedObject.shape))\n",
    "                newRelation=fromObjectsToRelationPairsBatchAndTimestepVersion(modifiedObject.unsqueeze(1).permute(0,1,3,2))\n",
    "                preRelations=newRelation.squeeze()\n",
    "                if i==0:\n",
    "                    #add dimension timestep, which in dimension 1\n",
    "                    outputVehicleProperties=modifiedObject.unsqueeze(1) \n",
    "                else:\n",
    "                    outputVehicleProperties=torch.cat((outputVehicleProperties,modifiedObject.unsqueeze(1)),1)\n",
    "                #doesn't need to compute new effect vector after the prediction of the last time step\n",
    "                if i==outputTimeFrame:\n",
    "                    break\n",
    "#             print(inputs.shape)\n",
    "#             vehicleProperty=inputs[:,-1,self.tupleForEachVehicle,0:6].squeeze()\n",
    "#             objectAndFinalEffect=torch.cat((vehicleProperty,Hidden_State),2)\n",
    "#             outputState=self.objectModifyNet(objectAndFinalEffect)\n",
    "            return outputVehicleProperties\n",
    "        else:\n",
    "            outputs = None\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:,:]), Hidden_State, Cell_State)  \n",
    "                if outputs is None:\n",
    "                    outputs = Hidden_State.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "            return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "#         use_gpu = torch.cuda.is_available()\n",
    "        if useGpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Cell_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Hidden_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2).cuda())\n",
    "            Cell_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2).cuda())\n",
    "            return Hidden_State, Cell_State,Hidden_State_2,Cell_State_2\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            Cell_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            Hidden_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2))\n",
    "            Cell_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2))\n",
    "            return Hidden_State, Cell_State,Hidden_State_2,Cell_State_2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fromRelationToEffectNetworkWithDiscountParameter(nn.Module):\n",
    "    '''\n",
    "    A new network for producing effect from relation with a discount parameter tensor \n",
    "    the dimension of the input relation tensor is (batches, timesteps, properties)\n",
    "    '''\n",
    "    def __init__(self,effectOutputTensorSize=20, maxRelationNumber=20,customMaxMatrixIndex=maxMatrixIndex,relationTensorSize=40):\n",
    "        super(fromRelationToEffectNetworkWithDiscountParameter,self).__init__()\n",
    "        self.maxMatrixIndex=customMaxMatrixIndex\n",
    "\n",
    "        \n",
    "        #position relation network initialize\n",
    "        self.relationTensorSize=relationTensorSize\n",
    "        self.positionTuple=(0,1,2,3)\n",
    "        self.positionRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        self.maxRelationNumber=maxRelationNumber\n",
    "       \n",
    "        #effect network initialize\n",
    "        self.objectAndTensorSize=self.relationTensorSize+4 #NOTE HERE: does it neccessary to contain both the tow object positions in the effect tensor?\n",
    "        self.effectOutputTensorSize=effectOutputTensorSize\n",
    "        #the number 4 is the size of positon(or velocity or accelerate) pairs,\n",
    "        #such as (positonxObject1,positonyObject1,positionxObject2,positionyObject2)\n",
    "        self.positionEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "\n",
    "  \n",
    "    def forward(self,inputObjectsPairs,discountParameterTensor):\n",
    "        #relation computation\n",
    "        effectOutputTensorSize=self.effectOutputTensorSize\n",
    "        positionRelationTensors=self.positionRelationNet(inputObjectsPairs[:,:,self.positionTuple])\n",
    "        objectsAndPositionRelationTensors=torch.cat((inputObjectsPairs[:,:,self.positionTuple],positionRelationTensors),2)\n",
    "        #effect computation\n",
    "        positionEffectTensors=self.positionEffectNet(objectsAndPositionRelationTensors)\n",
    "        #effect combination with discount parameter\n",
    "        #tensor summation\n",
    "        batchSize=positionRelationTensors.shape[0]\n",
    "        #apply discount parameters of distance to effectTensors\n",
    "        logging.debug(fromAllToStr('positionEffectTensors.shape: ',positionEffectTensors.shape))\n",
    "        logging.debug(fromAllToStr('discountParameterTensor.shape: ',discountParameterTensor.shape))\n",
    "        positionEffectTensors=torch.mul(positionEffectTensors,discountParameterTensor.unsqueeze(2))\n",
    "        if useGpu==True:\n",
    "            positionEffectSummation=torch.zeros(batchSize,self.maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "        else: \n",
    "            positionEffectSummation=torch.zeros(batchSize,self.maxMatrixIndex,effectOutputTensorSize)\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            positionEffectSummation[:,i,:]=torch.sum(positionEffectTensors[:,(i*self.maxRelationNumber):((i+1)*self.maxRelationNumber),:],1)\n",
    "        \n",
    "        #remove object extraction component and computation component\n",
    "#         #the property of each vehicle\n",
    "#         vehicleProperty=inputObjectsPairs[:,self.tupleForEachVehicle,0:6]\n",
    "#         objectAndFinalEffect=torch.cat((vehicleProperty,processedCombinedEffectTensors),2)\n",
    "        \n",
    "#         #compute final state\n",
    "#         finalObjectState=self.objectModifyNet(objectAndFinalEffect)\n",
    "        return positionEffectSummation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class differenceLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=20, cell_size=20, hidden_size=20, \\\n",
    "                 input_size_2=20, cell_size_2=20,hidden_size_2=20, outputTimeFrame=11,output_last = True,\\\n",
    "                maxRelationNumber=20,differenceSize=2,inputRelationTensor=4,theGivenRange=0.08,customMaxMatrixIndex=maxMatrixIndex):\n",
    "        \"\"\"\n",
    "        cell_size is the size of cell_state.\n",
    "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
    "        the input of LSTM  structure was the output of 'fromRelationToEffectNet' module, so that \n",
    "        the effectOutputTensorSize has the same number as input_size. \n",
    "        The dataset for this mode, DatasetV4, only return tensors with position values without velocity and accelerate values.\n",
    "        \n",
    "        \"\"\"\n",
    "        super(differenceLSTMModel, self).__init__()\n",
    "        self.maxMatrixIndex=customMaxMatrixIndex\n",
    "        self.theGivenRange=theGivenRange\n",
    "        self.maxRelationNumber=maxRelationNumber\n",
    "        \n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "#         self.positionTuple=(0,1,6,7)\n",
    "#         self.velocityTuple=(2,3,8,9)\n",
    "#         self.acclerateTuple=(4,5,10,11)\n",
    "        \n",
    "        #effect representive vector computation lstm\n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        #object modifying lstm\n",
    "        self.cell_size2 = cell_size_2\n",
    "        self.hidden_size2 = hidden_size_2\n",
    "        self.fl2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.il2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.ol2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.Cl2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        \n",
    "        self.outputTimeFrame=outputTimeFrame\n",
    "\n",
    "        self.fromRelationToEffectNet=fromRelationToEffectNetworkPositionOnly(effectOutputTensorSize=input_size)\n",
    "        self.objectModifyNet=objectModifyNetwork(inputSize=hidden_size+6,outputSize=6)\n",
    "        self.effectComputationWithDiscountParameterNet=fromRelationToEffectNetworkWithDiscountParameter(\\\n",
    "            maxRelationNumber=self.maxRelationNumber,effectOutputTensorSize=input_size,customMaxMatrixIndex=self.maxMatrixIndex) #edit tag\n",
    "        #the two variable in output size represent the difference of x and y positions\n",
    "        self.hiddenStateToDifferenceTensorNet=\\\n",
    "        hiddenStateToDifferenceNetwork(inputSize=hidden_size,outputSize=2)\n",
    "        \n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, inputEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputEffectTensor, Hidden_State), 2)\n",
    "#         print('in step,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl(combined))\n",
    "        i = F.sigmoid(self.il(combined))\n",
    "        o = F.sigmoid(self.ol(combined))\n",
    "        C = F.tanh(self.Cl(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def step2(self, inputPreEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputPreEffectTensor, Hidden_State), 2)\n",
    "#         print('in step2,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl2(combined))\n",
    "        i = F.sigmoid(self.il2(combined))\n",
    "        o = F.sigmoid(self.ol2(combined))\n",
    "        C = F.tanh(self.Cl2(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def forward(self, inputs,discountParameterTensor,vehicleGraph):\n",
    "        '''\n",
    "        Args(only for notes):\n",
    "            inputs:relation tensors\n",
    "        Returns:\n",
    "            allDifferenceTensor,allGraphTensor\n",
    "        '''\n",
    "        batch_size = inputs.size(0)\n",
    "        time_step = inputs.size(1)\n",
    "        Hidden_State, Cell_State,Hidden_State_2,Cell_State_2 = self.initHidden(batch_size)\n",
    "        \n",
    "        if self.output_last:\n",
    "            #effect computation\n",
    "            for i in range(time_step):\n",
    "                logging.debug(fromAllToStr('inputs[:,i,:,:].shape',inputs[:,i,:,:].shape))\n",
    "                logging.debug(fromAllToStr('discountParameterTensor[:,i,:].shape',discountParameterTensor[:,i,:].shape))\n",
    "                logging.debug(fromAllToStr('self.effectComputationWithDiscountParameterNet',self.effectComputationWithDiscountParameterNet))\n",
    "                logging.info(fromAllToStr('is effectComputationWithDiscountParameterNet in cuda:',next(self.effectComputationWithDiscountParameterNet.parameters()).is_cuda))\n",
    "                logging.info(fromAllToStr('inputs.device:',inputs.device))\n",
    "                logging.info(fromAllToStr('discoutParameterTensor.device',discountParameterTensor.device))\n",
    "                effects=self.effectComputationWithDiscountParameterNet\\\n",
    "                (inputs[:,i,:,:],discountParameterTensor[:,i,:])\n",
    "                logging.debug(fromAllToStr('effects.shape',effects.shape))\n",
    "                logging.debug(fromAllToStr('Hidden_State.shape',Hidden_State.shape))\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(effects), Hidden_State, Cell_State) \n",
    "                #the property of each vehicle\n",
    "                \n",
    "            Hidden_State_2=Hidden_State\n",
    "            Cell_State_2=Cell_State\n",
    "            \n",
    "            #the lstm process below take as inputs the previous object states and output the next predicted states\n",
    "            #applying function permute to deal with the dimension inconsistency\n",
    "            logging.debug(fromAllToStr('in seq2seq, inputs shape:',inputs.shape))\n",
    "            preVehicleProperty=vehicleGraph[:,-1,:,:].squeeze()\n",
    "            \n",
    "            \n",
    "            #object properties computation\n",
    "            for i in range(self.outputTimeFrame):\n",
    "                #produce relation first\n",
    "                for relationComputingBatch in range(preVehicleProperty.shape[0]):\n",
    "                    logging.info(fromAllToStr())\n",
    "                    relationTensorInBatchComputing, discountParameterTensorInBatchComputing=\\\n",
    "                    computeRelationAndAllTheOtherTensorsWithDistance(preVehicleProperty[relationComputingBatch].unsqueeze(0),self.theGivenRange,\\\n",
    "                                                                    isInModel=True,maxRelationsNumber=self.maxRelationNumber,customMaxMatrixIndex=self.maxMatrixIndex)\n",
    "                    logging.debug(fromAllToStr('relationTensorInBatchComputing.shape: ',relationTensorInBatchComputing.shape))\n",
    "                    logging.debug(fromAllToStr('discountParameterTensorInBatchComputing.shape: ',discountParameterTensorInBatchComputing.shape))\n",
    "                    relationTensorInBatchComputing,discountParameterTensorInBatchComputing=\\\n",
    "                    relationTensorInBatchComputing.unsqueeze(0),discountParameterTensorInBatchComputing.unsqueeze(0)\n",
    "                    if relationComputingBatch==0:\n",
    "                        allRelationTensorOfPrevehicle,allDiscountParameterTensor=\\\n",
    "                        relationTensorInBatchComputing,discountParameterTensorInBatchComputing\n",
    "                        logging.debug(fromAllToStr('relationComputingBatch=0 allRelationTensorOfPrevehicle.shape:',allRelationTensorOfPrevehicle.shape))\n",
    "                        logging.debug(fromAllToStr('relationComputingBatch=0 allDiscountParameterTensor.shape:',allDiscountParameterTensor.shape))\n",
    "                    else:\n",
    "                        allRelationTensorOfPrevehicle,allDiscountParameterTensor=\\\n",
    "                        torch.cat((allRelationTensorOfPrevehicle,relationTensorInBatchComputing),0),\\\n",
    "                        torch.cat((allDiscountParameterTensor,discountParameterTensorInBatchComputing),0)\n",
    "                        logging.debug(fromAllToStr('else allRelationTensorOfPrevehicle.shape:',allRelationTensorOfPrevehicle.shape))\n",
    "                        logging.debug(fromAllToStr('else allDiscountParameterTensor.shape:',allDiscountParameterTensor.shape))\n",
    "                #then compute effect using relations and discount parameters\n",
    "                logging.debug(fromAllToStr('allRelationTensorOfPrevehicle.shape:',allRelationTensorOfPrevehicle.shape))\n",
    "                logging.debug(fromAllToStr('allDiscountParameterTensor.shape:',allDiscountParameterTensor.shape))\n",
    "                combinedEffect=self.effectComputationWithDiscountParameterNet\\\n",
    "                (allRelationTensorOfPrevehicle[:,0,:,:],allDiscountParameterTensor[:,0,:])\n",
    "#                 if i==0:\n",
    "#                     #add dimension timestep, which in dimension 1\n",
    "#                     outputVehicleProperties=preVehicleProperty.unsqueeze(1) \n",
    "#                 else:\n",
    "#                     outputVehicleProperties=torch.cat((outputVehicleProperties,preVehicleProperty.unsqueeze(1)),1)\n",
    "                #don't need to compute new effect vector after the prediction of the last time step\n",
    "                if i<self.outputTimeFrame:\n",
    "                    Hidden_State_2,Cell_State_2=self.step2(combinedEffect,Hidden_State_2,Cell_State_2)\n",
    "                    differenceTensor=self.hiddenStateToDifferenceTensorNet(Hidden_State_2)\n",
    "                    logging.debug(fromAllToStr('differenceTensor.shape:',differenceTensor.shape))\n",
    "                    logging.debug(fromAllToStr('preVehicleProperty.shape:',preVehicleProperty.shape))\n",
    "                    newGraph=preVehicleProperty+differenceTensor.permute(0,2,1)\n",
    "                    #tensors of difference and predicted graph should be stored as the output of the model\n",
    "                    if i==0:\n",
    "                        #the reason for we do not unsqueeze dimension 0 but 1 is that 0 is the dimension of batch, and 1 represent time step\n",
    "                        allDifferenceTensor=differenceTensor.unsqueeze(1)\n",
    "                        allGraphTensor=newGraph.unsqueeze(1)\n",
    "                    else:\n",
    "                        #see above why we cat dimension 1 but not 0\n",
    "                        allDifferenceTensor=torch.cat((allDifferenceTensor,differenceTensor.unsqueeze(1)),1)\n",
    "                        allGraphTensor=torch.cat((allGraphTensor,newGraph.unsqueeze(1)),1)\n",
    "                if i==self.outputTimeFrame:\n",
    "                    break\n",
    "#             print(inputs.shape)\n",
    "#             vehicleProperty=inputs[:,-1,self.tupleForEachVehicle,0:6].squeeze()\n",
    "#             objectAndFinalEffect=torch.cat((vehicleProperty,Hidden_State),2)\n",
    "#             outputState=self.objectModifyNet(objectAndFinalEffect)\n",
    "            return allDifferenceTensor,allGraphTensor\n",
    "        else:\n",
    "            outputs = None\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:,:]), Hidden_State, Cell_State)  \n",
    "                if outputs is None:\n",
    "                    outputs = Hidden_State.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "            return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "#         use_gpu = torch.cuda.is_available()\n",
    "        if useGpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size,self.maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Cell_State = Variable(torch.zeros(batch_size,self.maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Hidden_State_2=Variable(torch.zeros(batch_size,self.maxMatrixIndex,self.hidden_size2).cuda())\n",
    "            Cell_State_2=Variable(torch.zeros(batch_size,self.maxMatrixIndex,self.hidden_size2).cuda())\n",
    "            return Hidden_State, Cell_State,Hidden_State_2,Cell_State_2\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.maxMatrixIndex,self.hidden_size))\n",
    "            Cell_State = Variable(torch.zeros(batch_size, self.maxMatrixIndex,self.hidden_size))\n",
    "            Hidden_State_2=Variable(torch.zeros(batch_size,self.maxMatrixIndex,self.hidden_size2))\n",
    "            Cell_State_2=Variable(torch.zeros(batch_size,self.maxMatrixIndex,self.hidden_size2))\n",
    "            return Hidden_State, Cell_State,Hidden_State_2,Cell_State_2\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class elementWeightedLoss(nn.Module):\n",
    "    def __init__(self,lambdaDistance,lambdaVelocity,lambdaAccelerate):\n",
    "        super(elementWeightedLoss,self).__init__()\n",
    "        self.lambdaDistance,self.lambdaVelocity,self.lambdaAccelerate=\\\n",
    "        lambdaDistance,lambdaVelocity,lambdaAccelerate\n",
    "        \n",
    "    def forward(self,output,label):\n",
    "        '''\n",
    "        output and label dimension: [batch, timestep, vehiclenum, properties]\n",
    "        properties dimension:(distancex,distancey,velocityx,velocityy,acceleratex,acceleratey)\n",
    "        '''\n",
    "        distanceLoss=torch.mean(torch.pow(output[:,:,:,0:2]-label[:,:,:,0:2],2))\n",
    "        velocityLoss=torch.mean(torch.pow(output[:,:,:,2:4]-label[:,:,:,2:4],2))\n",
    "        accelerateLoss=torch.mean(torch.pow(output[:,:,:,4:6]-label[:,:,:,4:6],2))\n",
    "        return torch.mul(distanceLoss,self.lambdaDistance)+torch.mul(velocityLoss,self.lambdaVelocity)+\\\n",
    "                torch.mul(accelerateLoss,self.lambdaAccelerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# seperation of model and testing part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing differenceLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runDifferenceLSTMModel:\n",
    "    differenceLSTMNet=differenceLSTMModel(outputTimeFrame=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runDifferenceLSTMModel:\n",
    "    trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n",
    "    datasetV4Instance=tensorsDatasetV4MultiThread(trajectoryFileList,numberOfTensorsEachBatch=3,lableTensorEachBatch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runDifferenceLSTMModel:\n",
    "    datasetV4Loader=DataLoader(datasetV4Instance,batch_size=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "allCombineTensorTrain, allCombineTensorValid,\\\n",
    "combinedRelationTensors, combinedDiscountParameterTensors,differenceLabels=item\n",
    "logging.debug(fromAllToStr(all))\n",
    "print(differenceLSTMNet)\n",
    "differenceLSTMNet(combinedRelationTensors,combinedDiscountParameterTensors,allCombineTensorTrain) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runDifferenceLSTMModel:\n",
    "    for items in iter(datasetV4Loader):\n",
    "        allCombineTensorTrain, allCombineTensorValid,\\\n",
    "        combinedRelationTensors, combinedDiscountParameterTensors,differenceLabels=items\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(fromAllToStr(combinedRelationTensors.shape,combinedDiscountParameterTensors.shape,allCombineTensorTrain.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeCombinedRelationTensors=torch.rand((2,3,200,4))\n",
    "fakeCombinedDiscoundParameterTensors=torch.rand((2,3,200))\n",
    "fakeAllCombineTensorTrain=torch.rand((2,3,2,10))\n",
    "fakeDifferenceLSTMNet=differenceLSTMModel(outputTimeFrame=2,customMaxMatrixIndex=10) \n",
    "fakeResult=fakeDifferenceLSTMNet(fakeCombinedRelationTensors,fakeCombinedDiscoundParameterTensors,fakeAllCombineTensorTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differenceLSTMNet(combinedRelationTensors,combinedDiscountParameterTensors,allCombineTensorTrain) #edit tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fakeDifferenceLSTMNet=differenceLSTMModel(outputTimeFrame=2,customMaxMatrixIndex=10) \n",
    "\n",
    "with SummaryWriter(comment='differenceNet') as w:\n",
    "    w.add_graph(fakeDifferenceLSTMNet,(fakeCombinedRelationTensors,fakeCombinedDiscoundParameterTensors,fakeAllCombineTensorTrain,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runDifferenceLSTMModel:\n",
    "    if not isTest:\n",
    "        learningRate=1e-3\n",
    "        MSELoss=nn.MSELoss()\n",
    "        lambdaWholeNet=lambda epoch: 0.5**(epoch//30)\n",
    "        optim=torch.optim.RMSprop([{'params':differenceLSTMNet.parameters(),'initial_lr':learningRate}],lr=learningRate)\n",
    "        lrSchedule=torch.optim.lr_scheduler.LambdaLR(optim,lambdaWholeNet,last_epoch=10)\n",
    "        if useGpu:\n",
    "            differenceLSTMNet.cuda()\n",
    "        differenceLSTMNet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eachItem in result:\n",
    "    logging.debug(fromAllToStr(eachItem.shape))\n",
    "logging.debug(fromAllToStr('allCombinedTensorValid.shape:',allCombineTensorValid.shape))\n",
    "logging.debug(fromAllToStr('differenceLabel.shape:',differenceLabels.shape))\n",
    "logging.debug(fromAllToStr('allCombineTensorTrain.shape:',allCombineTensorTrain.shape))\n",
    "logging.debug(fromAllToStr('allCombineTensorValid.shape:',allCombineTensorValid.shape))\n",
    "logging.debug(fromAllToStr('combinedRelationTensors.shape:',combinedRelationTensors.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputDifferences,outputGraphs=result\n",
    "loss1=MSELoss(outputDifferences,differenceLabels)\n",
    "loss2=MSELoss(outputGraphs,allCombineTensorValid)\n",
    "lossAll=loss1+loss2\n",
    "optim.zero_grad()\n",
    "lossAll.backward()\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.zero_grad()\n",
    "if runDifferenceLSTMModel:\n",
    "    differenceLSTMNet.cuda()\n",
    "    losses1=[]\n",
    "    losses2=[]\n",
    "    lossesAll=[]\n",
    "    for items in iter(datasetV4Loader):\n",
    "        logging.info('data loading finished')\n",
    "        # differenceLSTMNet=differenceLSTMModel(outputTimeFrame=10) \n",
    "        allCombineTensorTrain, allCombineTensorValid,\\\n",
    "        combinedRelationTensors, combinedDiscountParameterTensors,differenceLabels=items\n",
    "        if useGpu:\n",
    "            combinedRelationTensors=Variable(combinedRelationTensors.cuda())\n",
    "            combinedDiscountParameterTensors=Variable(combinedDiscountParameterTensors.cuda())\n",
    "            allCombineTensorTrain=Variable(allCombineTensorTrain.cuda())\n",
    "            allCombineTensorValid=Variable(allCombineTensorValid.cuda())\n",
    "            differenceLabels=Variable(differenceLabels.cuda())\n",
    "        logging.info(fromAllToStr('combinedRelationTensors.device:',combinedRelationTensors.device))\n",
    "        logging.info('forward start')\n",
    "        optim.zero_grad()\n",
    "        result=differenceLSTMNet(combinedRelationTensors,combinedDiscountParameterTensors,allCombineTensorTrain) #edit tag\n",
    "        logging.info('forward finished')\n",
    "        outputDifferences,outputGraphs=result\n",
    "        loss1=MSELoss(outputDifferences,differenceLabels)\n",
    "        loss2=MSELoss(outputGraphs,allCombineTensorValid)\n",
    "        lossAll=loss1+loss2\n",
    "        lossesAll.append(lossAll)\n",
    "        losses1.append(loss1)\n",
    "        losses2.append(loss2)\n",
    "        logging.info(fromAllToStr('loss1:',loss1,'\\nloss2:',loss2,'\\nlossAll:',lossAll))\n",
    "        logging.info('backward started')\n",
    "        lossAll.backward()\n",
    "        logging.info('backward finished')\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(differenceLSTMNet)\n",
    "next(differenceLSTMNet.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(fromAllToStr(loss1,loss2,lossAll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eachItem in items:\n",
    "    logging.debug(fromAllToStr(eachItem.shape))\n",
    "logging.debug(fromAllToStr('differenceLabels.shape',differenceLabels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST RELATION WITHIN A GIVEN RANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasetV4Instance=tensorsDatasetV4(trajectoryFileList,lableTensorEachBatch=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasetV4Loader=DataLoader(datasetV4Instance,batch_size=5,num_workers=4,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "itemList=datasetV4Instance.__getitem__(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for item in itemList:\n",
    "    logging.debug(fromAllToStr(item.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combineTensorTrain,combineTensorValid,combinedRelationTensors,combinedDiscountParameterTensors,\\\n",
    "differenceLabels=itemList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logging.debug(fromAllToStr('combnedRelationTensors.shape',combinedRelationTensors.shape))\n",
    "for i in range(combinedRelationTensors.shape[0]):\n",
    "    logging.debug(fromAllToStr('combinedRelationTensor, batch ', i, ':',combinedRelationTensors[i,0:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logging.debug(fromAllToStr(\"relationInAllTimeStep.shape:\",relationInAllTimeStep.shape))\n",
    "logging.debug(fromAllToStr(\"discountInAllTimeStep,shape:\",discountInAllTimeStep.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorA=torch.randn(10,10,100)\n",
    "tensorB=torch.randn(10,10,1)\n",
    "logging.debug(fromAllToStr('tensorA',tensorA))\n",
    "logging.debug(fromAllToStr('tensorB',tensorB))\n",
    "tensorAmB=torch.mul(tensorA,tensorB)\n",
    "logging.debug(fromAllToStr('tensorAmB',tensorAmB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loaderIter=iter(datasetV3Loader)\n",
    "toTestDifferenceItem, toTestDifferenceLabel=loaderIter.__next__()\n",
    "logging.debug(fromAllToStr(\"toTestDifferenceItem[0]:\",toTestDifferenceItem[0][0][0]))\n",
    "differenceSeries=differenceBetweenTwoFrameForBatch(toTestDifferenceItem)\n",
    "logging.debug(fromAllToStr('differenceSeries.shape:',differenceSeries.shape,'toTestDifferenceItem.shape:',\\\n",
    "                          toTestDifferenceItem.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logging.info('test')\n",
    "logTensor=torch.zeros(3,3)\n",
    "logging.debug(str(logTensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testTensor=torch.rand((6))\n",
    "logging.debug(testTensor)\n",
    "logging.debug(testTensor.expand(3,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test seq2seq relation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test code\n",
    "if runSeq2SeqRelationModel:\n",
    "    outputTimeFrame=5\n",
    "    RelationLSTMSeq2SeqModel=RelationLSTMSeq2Seq(outputTimeFrame=outputTimeFrame)\n",
    "    datasetV3Instance=tensorsDatasetV3(trajectoryFileList,lableTensorEachBatch=outputTimeFrame)\n",
    "    datasetV3Loader=DataLoader(datasetV3Instance,batch_size=2)\n",
    "    V3iter=iter(datasetV3Loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test code\n",
    "if runSeq2SeqRelationModel:\n",
    "    RelationLSTMSeq2SeqModel=RelationLSTMSeq2Seq(outputTimeFrame=outputTimeFrame)\n",
    "    inputs, labels=V3iter.__next__()\n",
    "    inputs=fromObjectsToRelationPairsBatchAndTimestepVersion(inputs)\n",
    "    outputs=RelationLSTMSeq2SeqModel(inputs)\n",
    "    print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model and dataset\n",
    "if runSeq2SeqRelationModel:\n",
    "    epochs=500\n",
    "    itersInEachEpoch=100\n",
    "    outputTimeFrame=500\n",
    "    relationLSTMSeq2SeqModel=RelationLSTMSeq2SeqPositionOnly(outputTimeFrame=outputTimeFrame)\n",
    "    datasetV3Instance=tensorsDatasetV3(trajectoryFileList,lableTensorEachBatch=outputTimeFrame)\n",
    "    datasetV3Loader=DataLoader(datasetV3Instance,batch_size=8,num_workers=4)\n",
    "    V3iter=iter(datasetV3Loader)\n",
    "    if useGpu:\n",
    "        relationLSTMSeq2SeqModel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runSeq2SeqRelationModel:\n",
    "    if not isTest:\n",
    "        learningRate=1e-3\n",
    "        lossFn=elementWeightedLoss(100,0.1,0.1)\n",
    "        lambdaWholeNet=lambda epoch: 0.5**(epoch//30)\n",
    "        optim=torch.optim.RMSprop([{'params':relationLSTMSeq2SeqModel.parameters(),'initial_lr':learningRate}],lr=learningRate)\n",
    "        lrSchedule=torch.optim.lr_scheduler.LambdaLR(optim,lambdaWholeNet,last_epoch=10)\n",
    "        relationLSTMSeq2SeqModel.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training seq2seqRelation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runSeq2SeqRelationModel:\n",
    "    if not isTest:\n",
    "        lossCurve=[]\n",
    "        for epoch in range(epochs):\n",
    "            for iteration in range(itersInEachEpoch):\n",
    "                try:\n",
    "                    inputs, labels=V3iter.__next__()\n",
    "                except StopIteration:\n",
    "                    V3iter=iter(datasetV3Loader)\n",
    "                    inputs, labels=V3iter.__next__()\n",
    "                inputs=fromObjectsToRelationPairsBatchAndTimestepVersion(inputs)\n",
    "                labels=labels.permute(0,1,3,2)\n",
    "                if useGpu:\n",
    "                    inputs=Variable(inputs.cuda())\n",
    "                    labels=Variable(labels.cuda())\n",
    "                outputs=relationLSTMSeq2SeqModel(inputs)\n",
    "                logging.debug(fromAllToStr('compare the shape of outputs and labels:',outputs.shape, labels.shape))\n",
    "                loss=lossFn(outputs,labels)\n",
    "                lossCurve.append(loss)\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                logging.info(fromAllToStr('loss in opoch ',epoch,',iteration',iteration,':',loss))\n",
    "                if(iteration%10==0):\n",
    "                    plt.plot(lossCurve)\n",
    "                    plt.savefig(fromAllToStr('.\\positionOnly\\lossCurve epoch ',epoch+1000000, ' iteration ',iteration+10000000,'.jpg'))\n",
    "            lrSchedule.step()\n",
    "            if epoch%2==0:\n",
    "                torch.save(relationLSTMSeq2SeqModel.state_dict(),\\\n",
    "                           '.\\positionOnly\\relationLSTMSeq2SeqMode_in_epoch_weightedLossAndPositionOnly_'+str(epoch+10000)+'.pt')\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing seq2seqRelation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetV3Loader=DataLoader(datasetV3Instance,batch_size=2,num_workers=4)\n",
    "V3iter=iter(datasetV3Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runSeq2SeqRelationModel:\n",
    "    if isTest:\n",
    "        relationLSTMSeq2SeqModel.eval()\n",
    "        with torch.no_grad():\n",
    "            relationLSTMSeq2SeqModel.load_state_dict(torch.load('relationLSTMSeq2SeqMode_in_epoch_weightedLossAndPositionOnly_10008.pt'))\n",
    "    #         inputs,labels=V3iter.__next__()\n",
    "            itemIndex=5838\n",
    "            inputs,labels=datasetV3Instance.__getitem__(itemIndex)\n",
    "            inputs=inputs.unsqueeze(0)\n",
    "            labels=labels.unsqueeze(0)\n",
    "            for ii in range(4):\n",
    "                newInput, newLabel=datasetV3Instance.__getitem__(itemIndex+ii)\n",
    "                newInput=newInput.unsqueeze(0)\n",
    "                newLabel=newLabel.unsqueeze(0)\n",
    "                inputs=torch.cat((inputs,newInput),0)\n",
    "                labels=torch.cat((labels,newLabel),0)\n",
    "\n",
    "            print(inputs.shape,labels.shape)\n",
    "\n",
    "            inputs=fromObjectsToRelationPairsBatchAndTimestepVersion(inputs)\n",
    "            print(inputs.shape)\n",
    "            labels=labels.permute(0,1,3,2)\n",
    "            outputs=relationLSTMSeq2SeqModel(inputs)\n",
    "            print(outputs.shape)\n",
    "            normalizationDict=datasetV3Instance.getNormalizationDict()\n",
    "            for i in range(labels.shape[1]):\n",
    "                resultImage=visualizeTensorData(outputs[0,i,:,0],outputs[0,i,:,1],normalizationDict=normalizationDict)\n",
    "                fileName='./predictWithRelationSeq2Seq/'+str(i)+'.png'\n",
    "                cv2.imwrite(fileName,resultImage)\n",
    "                resultImage=visualizeTensorData(labels[0,i,:,0],labels[0,i,:,1],normalizationDict=normalizationDict)\n",
    "                fileName='./predictWithRelationSeq2Seq/'+'l'+str(i)+'.png'\n",
    "                cv2.imwrite(fileName,resultImage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runSeq2SeqRelationModel:\n",
    "    if isTest:\n",
    "        vehicleListHalf=[]\n",
    "        for i in range(0,250,2):\n",
    "            vehicleListHalf.append(i)\n",
    "        trajectoryImage=visualizeTrajectory(outputs,normalizationDict=normalizationDict,radius=3,vehicleList=vehicleListHalf)\n",
    "        cv2.imwrite('trajectoryImage.jpg',trajectoryImage[0]) \n",
    "        differenceEachVehicleEachFrame,differenceEachVehicleAllFrame,averageDifferenceAllVehicleEachFrame,\\\n",
    "        averageDifferenceAllVehicleAllFrame=numericalEvaluation(outputs,labels) \n",
    "        logging.debug(fromAllToStr(averageDifferenceAllVehicleAllFrame.shape))\n",
    "        logging.debug(fromAllToStr(averageDifferenceAllVehicleAllFrame))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test lstm relation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('start test lstm relation model')\n",
    "if runRelationLSTM:\n",
    "    datasetV3Instance=tensorsDatasetV3(trajectoryFileList)\n",
    "    datasetV3Loader=DataLoader(datasetV3Instance,batch_size=2)\n",
    "    V3iter=iter(datasetV3Loader)\n",
    "    relationLSTMInstance=RelationLSTM()\n",
    "    if useGpu:\n",
    "        relationLSTMInstance.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "def calculateXandN(x,n):\n",
    "    return (1+x)/(n+1+2*x)\n",
    "ns=[]\n",
    "xs=[]\n",
    "for i in range(0,100):\n",
    "    xs.append(i)\n",
    "for i in range(0,10):\n",
    "    ns.append(i)\n",
    "lines=[]\n",
    "for i in range(20):\n",
    "    lines.append([])\n",
    "for x in xs:\n",
    "    for n in ns:\n",
    "        lines[n].append(calculateXandN(x,n))\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(241)\n",
    "plt.plot(xs,lines[0],'b')\n",
    "plt.plot(xs,lines[1],'g')\n",
    "plt.plot(xs,lines[2],'k')\n",
    "plt.plot(xs,lines[3],'y')\n",
    "plt.plot(xs,lines[4],'m')\n",
    "plt.plot(xs,lines[5],'-')\n",
    "plt.subplot(242)\n",
    "plt.plot(ns,lines[0],'b')\n",
    "plt.subplot(243)\n",
    "plt.plot(ns,lines[1],'b')\n",
    "plt.subplot(244)\n",
    "plt.plot(ns,lines[2],'b')\n",
    "plt.subplot(245)\n",
    "plt.plot(ns,lines[3],'b')\n",
    "plt.subplot(246)\n",
    "plt.plot(ns,lines[4],'b')\n",
    "plt.subplot(247)\n",
    "plt.plot(ns,lines[5],'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training relation lstm\n",
    "if runRelationLSTM:\n",
    "    if not isTest:\n",
    "        learningRate=1e-3\n",
    "        MSELoss=nn.MSELoss()\n",
    "        lambdaWholeNet=lambda epoch: 0.5**(epoch//30)\n",
    "        optim=torch.optim.RMSprop([{'params':relationLSTMInstance.parameters(),'initial_lr':learningRate}],lr=learningRate)\n",
    "        lrSchedule=torch.optim.lr_scheduler.LambdaLR(optim,lambdaWholeNet,last_epoch=10)\n",
    "        relationLSTMInstance.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runRelationLSTM:\n",
    "    #training relation lstm\n",
    "    if not runOnG814:\n",
    "        %matplotlib inline\n",
    "    from IPython.display import clear_output\n",
    "    normalizationDict=datasetV3Instance.getNormalizationDict()\n",
    "    optim.zero_grad()\n",
    "    if not isTest:  \n",
    "        losses=[]\n",
    "        iterInEpoch=50\n",
    "        for epoch in range(5):\n",
    "            print(epoch)\n",
    "            i=0\n",
    "            for inputs,label in V3iter:\n",
    "                i=i+1\n",
    "                if i>iterInEpoch*(epoch+1):\n",
    "                    break\n",
    "        #         print(i)\n",
    "                inputs=fromObjectsToRelationPairsBatchAndTimestepVersion(inputs)\n",
    "                print('label.shape',label.shape)\n",
    "                label=label[:,0,:,:].squeeze()\n",
    "                label=label.permute(0,2,1)\n",
    "#                 label=label[:,tupleForEachVehicle,0:6]\n",
    "                if useGpu:\n",
    "                    inputs=Variable(inputs.cuda())\n",
    "                    label=Variable(label.cuda())\n",
    "                output=relationLSTMInstance(inputs)\n",
    "\n",
    "        #         print(output[0,0:10,:],secondObjects[0,0:10,:])\n",
    "                print('output.shape',output.shape)\n",
    "                loss=MSELoss(output,label)\n",
    "                if i<5:\n",
    "                    print('epoch ',epoch, ' i', i,' loss',loss)\n",
    "        #         print(loss)\n",
    "                losses.append(loss.item())\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "    #             outputView=output.reshape((7,6,250)).cpu()\n",
    "    #             for j in range(10):\n",
    "    #                 inputs=inputs.cpu()\n",
    "    #                 resultImage=visualizeTensorData(inputs[0,j,0,:],inputs[0,j,1,:],normalizationDict=normalizationDict)\n",
    "    #                 fileName='./predictWithRelationLSTM/'+str((epoch+1)*10000000+i*100000+j)+'.png'\n",
    "    #                 cv2.imwrite(fileName,resultImage)\n",
    "    #             resultImage=visualizeTensorData(outputView[0,0,:],outputView[0,1,:],normalizationDict=normalizationDict)\n",
    "    #             fileName='./predictWithLSTMOnly/'+str((epoch+1)*10000000+i*100000+50)+'.png' #the predicted image is named with string which last two number is 50(because j < 50)\n",
    "    #             cv2.imwrite(fileName,resultImage)\n",
    "            lrSchedule.step()\n",
    "        plt.figure(figsize=(30,30))\n",
    "        plt.plot(losses)\n",
    "        plt.savefig('./losses.png')\n",
    "        torch.save(relationLSTMInstance.state_dict(),'./relationLSTM.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test relation-object model over a period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runObjectRelationNet:\n",
    "    normalizationDict=datasetV2.getNormalizationDict()\n",
    "\n",
    "    datas=[]\n",
    "    for i in range(0,6):\n",
    "        break\n",
    "        datas.append([])\n",
    "    for ii in range(0,2000):\n",
    "        break\n",
    "        theInput,second=datasetV2.__getitem__(ii)\n",
    "        theInput=theInput.unsqueeze(0)\n",
    "        second=second.unsqueeze(0)\n",
    "        #         print(i)\n",
    "        #         theInput,second=item\n",
    "        if useGpu:\n",
    "            theInput=Variable(theInput.cuda())\n",
    "            second=Variable(second.cuda())\n",
    "        inputObjects=theInput[:,tupleForEachVehicle,0:6]\n",
    "        secondObjects=second[:,tupleForEachVehicle,0:6]\n",
    "        for i in range(0,6):\n",
    "            datas[i].append(inputObjects[0,0,i])\n",
    "    #     print('inputObjects.shape',inputObjects.shape)\n",
    "    #     resultImage=visualizeTensorData(inputObjects[0,:,0].cpu(),inputObjects[0,:,1].cpu(),normalizationDict=normalizationDict)\n",
    "    #     fileName='./resultImage/'+str(ii)+'.png'\n",
    "    #     cv2.imwrite(fileName,resultImage)\n",
    "    timeStamp=int(time.time())\n",
    "    dirName='resultImage'+str(timeStamp)\n",
    "    os.mkdir(dirName)\n",
    "    if isTest:\n",
    "        with torch.no_grad():\n",
    "            wholeNet.eval()\n",
    "            theInput,second=datasetV2.__getitem__(5000)\n",
    "            theInput=theInput.unsqueeze(0)\n",
    "            second=second.unsqueeze(0)\n",
    "        #         print(i)\n",
    "    #         theInput,second=item\n",
    "            if useGpu:\n",
    "                theInput=Variable(theInput.cuda())\n",
    "                second=Variable(second.cuda())\n",
    "            inputObjects=theInput[:,tupleForEachVehicle,0:6]\n",
    "            secondObjects=second[:,tupleForEachVehicle,0:6]\n",
    "            print('inputObjects.shape',inputObjects.shape)\n",
    "            resultImage=visualizeTensorData(inputObjects[0,:,0].cpu(),inputObjects[0,:,1].cpu(),normalizationDict=normalizationDict)\n",
    "            fileName='./resultImage/'+'0000000000000000'+'.png'\n",
    "            cv2.imwrite(fileName,resultImage)\n",
    "            stepInput=fromObjectsToRelationPairs(inputObjects[0].permute(1,0)).unsqueeze(0)\n",
    "        #             output=wholeNet(theInput)\n",
    "            print(stepInput.shape)\n",
    "        #             print(output.shape)\n",
    "            #predict step by step\n",
    "            for step in range(500):\n",
    "                output=wholeNet(stepInput)\n",
    "                print('step: ',step)\n",
    "                for ii in range(output.shape[1]):\n",
    "                    print(output[0,ii])\n",
    "    #             break\n",
    "    #             for j in range(10):\n",
    "    #                 print(stepInput[:,tupleForEachVehicle,0:6][0,j,:])\n",
    "    #                 print(output[0,j,:])\n",
    "    #                 print()\n",
    "                stepInput=fromObjectsToRelationPairs(output[0].permute(1,0)).unsqueeze(0)\n",
    "    #             print('outputShape',output.shape)\n",
    "    #             print('outputshape[0]',output[0].shape)\n",
    "                resultImage=visualizeTensorData(output[0,:,0].cpu(),output[0,:,1].cpu(),normalizationDict=normalizationDict)\n",
    "\n",
    "                import os\n",
    "                fileName='./'+dirName+'/'+str(1000000+step)+'.png'\n",
    "                cv2.imwrite(fileName,resultImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training and testing process for 'fromRelationToObjectnetwork'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runObjectRelationNet:\n",
    "    #generate a tuple in which each element is the index of a vehicle\n",
    "    #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "    listForEachVehicle=[]\n",
    "    for i in range(maxMatrixIndex):\n",
    "        listForEachVehicle.append(i*(maxMatrixIndex-1))\n",
    "    tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "\n",
    "    dataloaderV2=DataLoader(datasetV2,batch_size=1,shuffle=True)\n",
    "\n",
    "\n",
    "    wholeNet=fromRelationToObjectNetwork()\n",
    "    if isTest:\n",
    "        wholeNet.load_state_dict(torch.load(modelPath))\n",
    "\n",
    "\n",
    "    if useGpu:\n",
    "        wholeNet.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runObjectRelationNet:\n",
    "    if not isTest:\n",
    "        learningRate=1e-3\n",
    "        MSELoss=nn.MSELoss()\n",
    "        lambdaWholeNet=lambda epoch: 0.5**(epoch//30)\n",
    "        optim=torch.optim.RMSprop([{'params':wholeNet.parameters(),'initial_lr':learningRate}],lr=learningRate)\n",
    "        lrSchedule=torch.optim.lr_scheduler.LambdaLR(optim,lambdaWholeNet,last_epoch=10)\n",
    "        wholeNet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if runObjectRelationNet:\n",
    "    if not runOnG814:\n",
    "        %matplotlib inline\n",
    "    from IPython.display import clear_output\n",
    "    if not isTest:  \n",
    "        losses=[]\n",
    "        iterInEpoch=50\n",
    "        for epoch in range(300):\n",
    "            print(epoch)\n",
    "            for i,item in enumerate(dataloaderV2):\n",
    "                if i>iterInEpoch:\n",
    "                    break\n",
    "        #         print(i)\n",
    "                theInput,second=item\n",
    "                if useGpu:\n",
    "                    theInput=Variable(theInput.cuda())\n",
    "                    second=Variable(second.cuda())\n",
    "                secondObjects=second[:,tupleForEachVehicle,0:6]\n",
    "\n",
    "                output=wholeNet(theInput)\n",
    "        #         print(output[0,0:10,:],secondObjects[0,0:10,:])\n",
    "                loss=MSELoss(output,secondObjects)\n",
    "                if i<5:\n",
    "                    print('epoch ',epoch, ' i', i,' loss',loss)\n",
    "        #         print(loss)\n",
    "                losses.append(loss.item())\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            lrSchedule.step()\n",
    "        plt.figure(figsize=(30,30))\n",
    "        plt.plot(losses)\n",
    "        plt.savefig('./losses.png')\n",
    "        torch.save(wholeNet.state_dict(),'./wholeNet_300epoch_50perEpoch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training simple LSTM module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runLSTM:\n",
    "    #lstm version \n",
    "    if not isTest:\n",
    "        learningRate=1e-3\n",
    "        MSELoss=nn.MSELoss()\n",
    "        lambdaWholeNet=lambda epoch: 0.5**(epoch//30)\n",
    "        optim=torch.optim.RMSprop([{'params':lstmModel.parameters(),'initial_lr':learningRate}],lr=learningRate)\n",
    "        lrSchedule=torch.optim.lr_scheduler.LambdaLR(optim,lambdaWholeNet,last_epoch=10)\n",
    "        lstmModel.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if runLSTM:\n",
    "    #lstm version\n",
    "    if not runOnG814:\n",
    "        %matplotlib inline\n",
    "    from IPython.display import clear_output\n",
    "    normalizationDict=datasetV3.getNormalizationDict()\n",
    "    if not isTest:  \n",
    "        losses=[]\n",
    "        iterInEpoch=50\n",
    "        for epoch in range(5):\n",
    "            print(epoch)\n",
    "            i=0\n",
    "            for inputs,label in iterV3:\n",
    "                i=i+1\n",
    "                if i>iterInEpoch*(epoch+1):\n",
    "                    break\n",
    "        #         print(i)\n",
    "                if useGpu:\n",
    "                    inputs=Variable(inputs.cuda())\n",
    "                    label=Variable(label.cuda())\n",
    "                output=lstmModel(inputs.reshape((inputs.shape[0],inputs.shape[1],-1)))\n",
    "\n",
    "        #         print(output[0,0:10,:],secondObjects[0,0:10,:])\n",
    "                loss=MSELoss(output,label.squeeze().reshape((label.shape[0],-1)))\n",
    "                if i<5:\n",
    "                    print('epoch ',epoch, ' i', i,' loss',loss)\n",
    "        #         print(loss)\n",
    "                losses.append(loss.item())\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                outputView=output.reshape((7,6,250)).cpu()\n",
    "                for j in range(10):\n",
    "                    inputs=inputs.cpu()\n",
    "                    resultImage=visualizeTensorData(inputs[0,j,0,:],inputs[0,j,1,:],normalizationDict=normalizationDict)\n",
    "                    fileName='./predictWithLSTMOnly/'+str((epoch+1)*10000000+i*100000+j)+'.png'\n",
    "                    cv2.imwrite(fileName,resultImage)\n",
    "                resultImage=visualizeTensorData(outputView[0,0,:],outputView[0,1,:],normalizationDict=normalizationDict)\n",
    "                fileName='./predictWithLSTMOnly/'+str((epoch+1)*10000000+i*100000+50)+'.png' #the predicted image is named with string which last two number is 50(because j < 50)\n",
    "                cv2.imwrite(fileName,resultImage)\n",
    "            lrSchedule.step()\n",
    "        plt.figure(figsize=(30,30))\n",
    "        plt.plot(losses)\n",
    "        plt.savefig('./losses.png')\n",
    "        torch.save(wholeNet.state_dict(),'./wholeNet_300epoch_50perEpoch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#draw properties of a single tensor over time\n",
    "#output results step by step\n",
    "plt.subplot(321)\n",
    "plt.plot(datas[0])\n",
    "plt.subplot(322)\n",
    "plt.plot(datas[1])\n",
    "plt.subplot(323)\n",
    "plt.plot(datas[2])\n",
    "plt.subplot(324)\n",
    "plt.plot(datas[3])\n",
    "plt.subplot(325)\n",
    "plt.plot(datas[4])\n",
    "plt.subplot(326)\n",
    "plt.plot(datas[5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
