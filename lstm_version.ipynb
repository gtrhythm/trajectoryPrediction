{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variable\n",
    "doNormalization=True\n",
    "useGpu=True\n",
    "runOnG814=False\n",
    "isTest=True\n",
    "modelPath='/home/wangyuchen/wholeNet_300epoch_50perEpoch.pt'\n",
    "maxMatrixIndex=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def getValueByLable(lableList,valueList):\n",
    "    \"\"\"\n",
    "    For instance, given a lable list ['Local_X','Local_Y'] and a value list [2.0, 24.0, 437.0, 1118846981300.0, 16.254, \n",
    "    79.349, 6451167.199, 1873312.382, 14.5, 4.9, 2.0, 39.14, -5.73, 2.0, 0.0, 13.0, 0.0, 0.0] which values sorted by the \n",
    "    order of allLableList below, the function return a value Dict {'Local_X':16.254, 'Local_Y':79.349}\n",
    "    Args:\n",
    "        lableList: the list of lables you've required, such as['Vehicle_ID', 'Total_Frames','Global_Time']\n",
    "        valueList: the list contains all legally value, sorted by:['Vehicle_ID', 'Frame_ID','Total_Frames','Global_Time','Local_X','Local_Y','Global_X','Global_Y',\\\n",
    "                      'v_Length','v_Width','v_Class','v_Vel','v_Acc','Lane_ID','Preceding','Following','Space_Headway',\\\n",
    "                      'Time_Headway']\n",
    "    Returns: \n",
    "        value dict of the input lables\n",
    "    For instance, given a lable list ['Local_X','Local_Y'] and a value list [2.0, 24.0, 437.0, 1118846981300.0, 16.254, \n",
    "    79.349, 6451167.199, 1873312.382, 14.5, 4.9, 2.0, 39.14, -5.73, 2.0, 0.0, 13.0, 0.0, 0.0] which values sorted by the \n",
    "    order of allLableList above, the function return a value List [16.254, 79.349]\n",
    "\n",
    "    \"\"\"\n",
    "    allLableList=['Vehicle_ID', 'Frame_ID','Total_Frames','Global_Time','Local_X','Local_Y','Global_X','Global_Y',\\\n",
    "                  'v_Length','v_Width','v_Class','v_Vel','v_Acc','Lane_ID','Preceding','Following','Space_Headway',\\\n",
    "                  'Time_Headway']\n",
    "    valueDictReturn={}\n",
    "    for lableItem in lableList:\n",
    "        valueDictReturn[lableItem]=valueList[allLableList.index(lableItem)]\n",
    "    return valueDictReturn\n",
    "\n",
    "def rearrangeDataByGlobalTime(allValueLists):\n",
    "    '''\n",
    "    Args:\n",
    "        allValueLists: all values have been read from a txt file which have already been converted to a list\n",
    "    Returns:\n",
    "        dict have been arranged by global time. One single global time generally contains several value lists.\n",
    "    '''\n",
    "    valueDict={}\n",
    "    for valueList in allValueLists:\n",
    "        dictKey=getValueByLable(['Global_Time'],valueList)['Global_Time']\n",
    "        if dictKey in valueDict:\n",
    "            # if dictKey already there, then add valueList to the list of the key\n",
    "            valueDict[dictKey].append(valueList)\n",
    "        else:\n",
    "            #else, create a list and append valueList on it\n",
    "            valueDict[dictKey]=[valueList]\n",
    "    return valueDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def readFirstFrame(matrixIndexAndVehicleIDRecordDictParam, valueLists):\n",
    "    \"\"\"\n",
    "    To generate the first set of tensors from the first frame\n",
    "    Args:\n",
    "        matrixIndexAndVehicleIDRecordDictParam: just as its name\n",
    "        valueLists: a list consists of all valuelist at one time\n",
    "    Returns:\n",
    "        several tensors arranged by: positionTensor, speedTensor, accTensor, angleTensor,newVehicleList(type:list)\n",
    "    \n",
    "    \"\"\"\n",
    "    maxMatrixIndex=matrixIndexAndVehicleIDRecordDictParam.keys().__len__()-1\n",
    "    #tensors initialize\n",
    "    positionTensor=torch.zeros(2,maxMatrixIndex)\n",
    "    speedTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    accTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    angleTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    newVehicleIDList=[]\n",
    "    curMatrixIndex=0\n",
    "    matrixIndexAndVehicleIDRecordDictParam['time']=getValueByLable([\"Global_Time\"],valueLists[0])['Global_Time']\n",
    "    #fill out all tensors\n",
    "    for eachValueList in valueLists:\n",
    "        #get values from eachValueList, generate dict\n",
    "        returnedEachValueDict=getValueByLable(['Vehicle_ID','Local_X','Local_Y','v_Vel','v_Acc'],eachValueList)\n",
    "        #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "        #angle Tensor assignment is not neeed for the initial value of each element in it is already zero\n",
    "        positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "        speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "        accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "        #then handle the record matrix\n",
    "        matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['Vehicle_ID']=returnedEachValueDict['Vehicle_ID']\n",
    "        matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['refresh']=0\n",
    "        newVehicleIDList.append(copy.deepcopy(returnedEachValueDict['Vehicle_ID']))\n",
    "        curMatrixIndex=curMatrixIndex+1\n",
    "    return positionTensor,speedTensor,accTensor,angleTensor,newVehicleIDList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMatrixIndexByVehicleID(matrixIndexAndVehicleIDRecordDictParam, vehicle_ID):\n",
    "    for i in range(0, len(matrixIndexAndVehicleIDRecordDictParam)-1):\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']==vehicle_ID:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def findEmptyMatrixIndex(matrixIndexAndVehicleIDRecordDictParam):\n",
    "    for i in range(0, len(matrixIndexAndVehicleIDRecordDictParam)-1):\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']==-1:\n",
    "            #Vehicle_ID=-1 when there is no existed vehicle ID bounding to the index\n",
    "            return i\n",
    "    raise Exception(\"NO EMPTY ELEMENT IN MATRIX\")\n",
    "\n",
    "def readGeneralFrame(matrixIndexAndVehicleIDRecordDictParam, valueLists, prePositionTensor):\n",
    "    \"\"\"\n",
    "    To generate the first set of tensors from the general frame that have a preceding one\n",
    "    Args:\n",
    "        matrixIndexAndVehicleIDRecordDictParam: just as its name\n",
    "        valueLists: a list consists of all valuelist at one time\n",
    "        prePositionTensor: positionTensor from the preceding frame, which is used to calculate angle tensor\n",
    "    Returns:\n",
    "        everal tensors arranged by: positionTensor, speedTensor, accTensor, angleTensor,newVehicleList(type:list),\n",
    "        vanishedVehicleList(type:list)\n",
    "    \n",
    "    \"\"\"\n",
    "    #tensors initialize\n",
    "    maxMatrixIndex=matrixIndexAndVehicleIDRecordDictParam.keys().__len__()-1\n",
    "    positionTensor=torch.zeros(2,maxMatrixIndex)\n",
    "    speedTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    accTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    angleTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    newVehicleIDList=[]\n",
    "    vanishedVehicleList=[]\n",
    "    curMatrixIndex=0\n",
    "    matrixIndexAndVehicleIDRecordDictParam['time']=getValueByLable([\"Global_Time\"],valueLists[0])['Global_Time']\n",
    "    #fill out all tensors\n",
    "    for eachValueList in valueLists:\n",
    "        #get values from eachValueList, generate dict\n",
    "        returnedEachValueDict=getValueByLable(['Vehicle_ID','Local_X','Local_Y','v_Vel','v_Acc'],eachValueList)\n",
    "        indexOfVehicle=findMatrixIndexByVehicleID(matrixIndexAndVehicleIDRecordDictParam,returnedEachValueDict['Vehicle_ID'])\n",
    "        if indexOfVehicle!=-1:\n",
    "        #index exist then the vehicle already existed in the preceded frame\n",
    "            matrixIndexAndVehicleIDRecordDictParam[indexOfVehicle]['refresh']=1\n",
    "            curMatrixIndex=indexOfVehicle\n",
    "            #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "            positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "            speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "            accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "            angleTensor[:,curMatrixIndex]=math.atan2(positionTensor[0,curMatrixIndex]-\\\n",
    "                                                     prePositionTensor[0,curMatrixIndex],\\\n",
    "                                                    positionTensor[1,curMatrixIndex]-prePositionTensor[1,curMatrixIndex])\n",
    "        else:\n",
    "            pass #ignore new vehicleID\n",
    "        #a new vehicle ID\n",
    "#             newVehicleIDList.append(copy.deepcopy(returnedEachValueDict['Vehicle_ID']))\n",
    "#             curMatrixIndex=findEmptyMatrixIndex(matrixIndexAndVehicleIDRecordDictParam)\n",
    "#             matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['Vehicle_ID']=copy.deepcopy(returnedEachValueDict['Vehicle_ID'])\n",
    "#             matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['refresh']=1\n",
    "#             #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "#             positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "#             speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "#             accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "#             angleTensor[:,curMatrixIndex]=math.atan2(positionTensor[0,curMatrixIndex]-\\\n",
    "#                                                      prePositionTensor[0,curMatrixIndex],\\\n",
    "#                                                     positionTensor[1,curMatrixIndex]-prePositionTensor[1,curMatrixIndex])\n",
    "    for i in range(0,maxMatrixIndex):\n",
    "    #find vanished vehicle and remove from dict\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['refresh']==0:\n",
    "            #if refresh=0 then the corresponding vehicle ID was not found in this frame\n",
    "            vanishedVehicleList.append(copy.deepcopy(matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']))\n",
    "            matrixIndexAndVehicleIDRecordDictParam[i]['refresh']=-1\n",
    "            matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']=-1\n",
    "    \n",
    "    for i in range(0,maxMatrixIndex):\n",
    "    #set all refrshed which equivalent to 1 to 0 to prepare for the next frame\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['refresh']==1:\n",
    "                #if refresh=0 then the corresponding vehicle ID was not found in this frame\n",
    "                matrixIndexAndVehicleIDRecordDictParam[i]['refresh']=0\n",
    "\n",
    "    return positionTensor,speedTensor,accTensor,angleTensor,newVehicleIDList,vanishedVehicleList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as utils\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromDirGenerateDict(trajectoryDir):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        valueDict: the key is global time, and the value of each key contain SEVERAL LIST of properties, \n",
    "                   each list consist of all property of a single vehicle at one time.\n",
    "    \"\"\"\n",
    "    trajectoryDataFile=open(trajectoryDir)\n",
    "    count=0\n",
    "    allLineList=[]\n",
    "    count=0\n",
    "    for count,line in enumerate(trajectoryDataFile):\n",
    "        #read a single line, remove space and enter\n",
    "        lineList=line.split(' ')\n",
    "        try:\n",
    "            while True:\n",
    "                lineList.remove('')\n",
    "        except:\n",
    "            try:\n",
    "                lineList.remove('\\n')\n",
    "            except:\n",
    "                pass\n",
    "            pass\n",
    "        for i in range(0,lineList.__len__()):\n",
    "            # convert string to float\n",
    "            lineList[i]=float(lineList[i])\n",
    "        allLineList.append(lineList)\n",
    "    valueDict=rearrangeDataByGlobalTime(allLineList)\n",
    "    return valueDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxAndMinValueFromValueDict(valueDict,lableList):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        valueDict: each key in dict is global time, the value of each key is a list of all value at one time\n",
    "        lableList: lables from which you want to get the max and min value. the type of each value in the list \n",
    "                    is str.\n",
    "    Returns:\n",
    "        a dict, which has keys keys from the input lable list and the value of each key is a dict which formed\n",
    "        as 'max':value, 'min':value\n",
    "    \"\"\"\n",
    "    maxAndMinDict={}\n",
    "    keys=list(valueDict.keys())\n",
    "    for lable in lableList:\n",
    "        max=0\n",
    "        min=0 #speed,  positon are all from 0 to max, accelerate from - to +\n",
    "        for eachKey in keys:\n",
    "            valueLists=valueDict[eachKey]\n",
    "            for valueList in valueLists:\n",
    "                value=getValueByLable([lable],valueList)[lable]\n",
    "                if value>max:\n",
    "                    max=value\n",
    "                if value<min:\n",
    "                    min=value\n",
    "        maxAndMinDict[lable]={'max':max,'min':min}\n",
    "    return maxAndMinDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test function of finding the max and min value: block 1, get valueDict for saving time from file reaidng\n",
    "# valueDict=fromDirGenerateDict(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test function of finding the max and min value: block 1, get valueDict for saving time from file reaidng\n",
    "# getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Acc','v_Vel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valueDict=fromDirGenerateDict(1)\n",
    "# theKey=list(valueDict.keys())[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "def visualizeData(valueVisualize, maxLength=1000,maxWidth=100,blocksize=10):\n",
    "    \"\"\"\n",
    "    visualize a frame on an white image\n",
    "    Args:\n",
    "        valueVisualize: a list of values, each item in the list can be obtained by function \n",
    "        getValueByLable\n",
    "    Returns:\n",
    "        the image of the input frame\n",
    "    \"\"\"\n",
    "    image=np.ones((maxLength,maxWidth,3),dtype=np.int8)\n",
    "    image=image*255\n",
    "#     figure=plt.figure(figsize=(10,50))\n",
    "#     axe=figure.add_subplot(1,1,1)\n",
    "    \n",
    "    for item in valueVisualize:\n",
    "        infoList=getValueByLable(['Vehicle_ID','Local_X','Local_Y'],item)\n",
    "        vehicleID=infoList['Vehicle_ID']\n",
    "        x=int(infoList['Local_X'])\n",
    "        y=int(infoList['Local_Y'])\n",
    "        colorR=int((vehicleID+100)%255)\n",
    "        colorG=int((vehicleID+150)%255)\n",
    "        colorB=int((vehicleID+200)%255)\n",
    "        cv2.circle(image,(x,y),int(blocksize/2),(colorB,colorG,colorR),-1) #\n",
    "#     axe.imshow(image)\n",
    "    return image\n",
    "# visualizeData(valueDict[theKey])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save visulized images\n",
    "# for key in list(valueDict.keys())[1:10000]:\n",
    "#     image=visualizeData(valueDict[key])\n",
    "#     cv2.imwrite('visualizeFolder/image'+str(key)+'.png',image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensorsDataset(Dataset):\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=100,lableTensorEachBatch=2):\n",
    "        if(numberOfTensorsEachBatch<5):\n",
    "            raise Exception(\"THE NUMBER OF TENSORS IN EACH BATCH IS TOO SMALL\")\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the ture index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                 speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                 accTensor.mul(angleCosTensor)),0)\n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "            if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "            elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "            else:\n",
    "                allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "        return allCombineTensorTrain,allCombineTensorValid\n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class tensorsDatasetV2(Dataset):\n",
    "    \"\"\"\n",
    "    dataset for relation model\n",
    "    \"\"\"\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=1,lableTensorEachBatch=1):\n",
    "        if(numberOfTensorsEachBatch!=1 or lableTensorEachBatch!=1):\n",
    "            raise Exception(\"BOTH TRAIN AND VALID TENSOR NUMBERS SHOULD BE ONE!\")\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        if doNormalization:\n",
    "            self.positionXMax=0\n",
    "            self.positionXMin=999999\n",
    "            self.positionYMax=0\n",
    "            self.positionYMin=99999\n",
    "            self.speedMax=-100\n",
    "            self.speedMin=999999\n",
    "            self.accMax=-100\n",
    "            self.accMin=9999\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            if doNormalization:\n",
    "                #get the max and min value for normalization\n",
    "                maxAndMinDict=getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Vel','v_Acc'])\n",
    "                #position X\n",
    "                if self.positionXMax<maxAndMinDict['Local_X']['max']:\n",
    "                    self.positionXMax=maxAndMinDict['Local_X']['max']\n",
    "                if self.positionXMin>maxAndMinDict['Local_X']['min']:\n",
    "                    self.positionXMin=maxAndMinDict['Local_X']['min']\n",
    "                #position Y\n",
    "                if self.positionYMax<maxAndMinDict['Local_Y']['max']:\n",
    "                    self.positionYMax=maxAndMinDict['Local_Y']['max']\n",
    "                if self.positionYMin>maxAndMinDict['Local_Y']['min']:\n",
    "                    self.positionYMin=maxAndMinDict['Local_Y']['min']\n",
    "                #speed\n",
    "                if self.speedMax<maxAndMinDict['v_Vel']['max']:\n",
    "                    self.speedMax=maxAndMinDict['v_Vel']['max']\n",
    "                if self.speedMin>maxAndMinDict['v_Vel']['min']:\n",
    "                    self.speedMin=maxAndMinDict['v_Vel']['min']\n",
    "                #acc\n",
    "                if self.accMax<maxAndMinDict['v_Acc']['max']:\n",
    "                    self.accMax=maxAndMinDict['v_Acc']['max']\n",
    "                if self.accMin>maxAndMinDict['v_Acc']['min']:\n",
    "                    self.accMin=maxAndMinDict['v_Acc']['min']\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def getNormalizationDict(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            a dict:{'positionXMax':self.positionXMax,'positonYMax':self.self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "        '''\n",
    "        if not doNormalization:\n",
    "            raise Exception('NORMALIZATION IS NOT APPLIED')\n",
    "        return {'positionXMax':self.positionXMax,'positionYMax':self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':self.speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "    \n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the ture index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        #first frame normalization\n",
    "        if doNormalization:\n",
    "#             print('before nomalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                     torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "            speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "            accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "#             print('after normalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        else:\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        #generate relation tensor for all vehicle pairs\n",
    "        print('in getitem, combinedTensor shape: ',combinedTensor.shape)\n",
    "        relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "        relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "        print('in getitem, relation tensorleft shape:',relationTensorLeft.shape)\n",
    "        print('in getitem, relationtensorright shape',relationTensorRight.shape)\n",
    "#         print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "        for i in range(1,combinedTensor.shape[1]):\n",
    "            relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                          combinedTensor[:,i].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "            relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                           torch.transpose(torch.cat((combinedTensor[:,:i],combinedTensor[:,i+1:]),1),0,1)),0)\n",
    "#         print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "        combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1)  \n",
    "        firstCombinedRelationTensor=combinedRelationTensor\n",
    "        \n",
    "        \n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            if doNormalization:\n",
    "                positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                         torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "                speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "                accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                         speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            else:\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            #generate relation tensor for all vehicle pairs\n",
    "            relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "            relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "#             print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "            for j in range(1,combinedTensor.shape[1]):\n",
    "                relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                              combinedTensor[:,j].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "                relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                               torch.transpose(torch.cat((combinedTensor[:,:j],combinedTensor[:,j+1:]),1),0,1)),0)\n",
    "#             print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "            combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1)  \n",
    "            secondRelationTensor=combinedRelationTensor\n",
    "            #since we only need two tensors, which is input and output tensor respectively, we could return\n",
    "            #the two tensors in the first loop\n",
    "            #(ok I admit that the true reason is that I am lazy)\n",
    "            return firstCombinedRelationTensor,secondRelationTensor\n",
    "#             if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "#                 allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "#             elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "#                 allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "#             else:\n",
    "#                 allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "        return allCombineTensorTrain,allCombineTensorValid\n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n",
    "#run on 2080 in g814\n",
    "if runOnG814:\n",
    "    trajectoryFileList=['/home/wangyuchen/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetV2=tensorsDatasetV2(trajectoryFileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in getitem, combinedTensor shape:  torch.Size([6, 250])\n",
      "in getitem, relation tensorleft shape: torch.Size([249, 6])\n",
      "in getitem, relationtensorright shape torch.Size([249, 6])\n",
      "torch.Size([62250, 12]) torch.Size([62250, 12])\n"
     ]
    }
   ],
   "source": [
    "dataIter=iter(datasetV2)\n",
    "first,second=dataIter.__next__()\n",
    "print(first.shape, second.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in getitem, combinedTensor shape:  torch.Size([6, 250])\n",
      "in getitem, relation tensorleft shape: torch.Size([249, 6])\n",
      "in getitem, relationtensorright shape torch.Size([249, 6])\n",
      "in getitem, combinedTensor shape:  torch.Size([6, 250])\n",
      "in getitem, relation tensorleft shape: torch.Size([249, 6])\n",
      "in getitem, relationtensorright shape torch.Size([249, 6])\n",
      "in getitem, combinedTensor shape:  torch.Size([6, 250])\n",
      "in getitem, relation tensorleft shape: torch.Size([249, 6])\n",
      "in getitem, relationtensorright shape torch.Size([249, 6])\n",
      "in getitem, combinedTensor shape:  torch.Size([6, 250])\n",
      "in getitem, relation tensorleft shape: torch.Size([249, 6])\n",
      "in getitem, relationtensorright shape torch.Size([249, 6])\n",
      "0\n",
      "torch.Size([4, 62250, 12]) torch.Size([4, 62250, 12])\n",
      "tensor([[0.4119, 0.9362, 0.0000, 0.6054, 0.0000, 0.5772],\n",
      "        [0.4119, 0.9362, 0.0000, 0.6054, 0.0000, 0.5772],\n",
      "        [0.4119, 0.9362, 0.0000, 0.6054, 0.0000, 0.5772],\n",
      "        [0.4119, 0.9362, 0.0000, 0.6054, 0.0000, 0.5772],\n",
      "        [0.4119, 0.9362, 0.0000, 0.6054, 0.0000, 0.5772]])\n",
      "tensor([[0.6169, 0.9184, 0.0000, 0.5038, 0.0000, 0.4978],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000],\n",
      "        [0.4119, 0.9362, 0.0000, 0.6054, 0.0000, 0.5772],\n",
      "        [0.5709, 0.8808, 0.0000, 0.5118, 0.0000, 0.5000],\n",
      "        [0.6169, 0.9184, 0.0000, 0.5038, 0.0000, 0.4978]])\n",
      "in getitem, combinedTensor shape:  torch.Size([6, 250])\n",
      "in getitem, relation tensorleft shape: torch.Size([249, 6])\n",
      "in getitem, relationtensorright shape torch.Size([249, 6])\n",
      "in getitem, combinedTensor shape:  torch.Size([6, 250])\n",
      "in getitem, relation tensorleft shape: torch.Size([249, 6])\n",
      "in getitem, relationtensorright shape torch.Size([249, 6])\n",
      "in getitem, combinedTensor shape:  torch.Size([6, 250])\n",
      "in getitem, relation tensorleft shape: torch.Size([249, 6])\n",
      "in getitem, relationtensorright shape torch.Size([249, 6])\n",
      "in getitem, combinedTensor shape:  torch.Size([6, 250])\n",
      "in getitem, relation tensorleft shape: torch.Size([249, 6])\n",
      "in getitem, relationtensorright shape torch.Size([249, 6])\n"
     ]
    }
   ],
   "source": [
    "maxMatrixIndex=250\n",
    "dataloaderV2=DataLoader(datasetV2,batch_size=4,shuffle=True)\n",
    "for i,item in enumerate(dataloaderV2):\n",
    "    if(i>0):\n",
    "        break\n",
    "    print(i)\n",
    "    first,second=item\n",
    "    print(first.shape,second.shape)\n",
    "    print(first[0,:5,:6])\n",
    "    print(first[0,(2,245,246,247,248,249,250,251),6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def generateAdjacencyMatrix(batchedPositionTensor,lambdaX,lambdaY,omegaX,omegaY,m):\n",
    "    \"\"\"\n",
    "    Using batched position tensor generate batched adjacency matrix\n",
    "    Args:\n",
    "        batchedPositionTensor: a batch of position tensor, which size in (batch, timeSequence,2,vehicles), the \n",
    "        value 2 in dim=2 is the position of x and y. \n",
    "        lambda1,lambda2,omega1,omega2,m are parameters of the function. m<1\n",
    "        see detail in my notebook\n",
    "    Returns:\n",
    "        a batch of adjacency matrix\n",
    "    Example:\n",
    "        if given a batch of combined tensor, named theTensor, which size as below:\n",
    "            (4,100,6,250)\n",
    "        which means 4 batches, 100 time step, 6 dimension which respectively of positonx, positony, velocityx, \n",
    "        velocityy, accx,accy.\n",
    "        then we apply the function in such way:\n",
    "        generateAdjacencyMatrix(theTensor(:,:,0:1,:))\n",
    "    \"\"\"\n",
    "    print(batchedPositionTensor.size())\n",
    "    sizeOfEachMatrix=batchedPositionTensor[0,0,0,:].size()[0]\n",
    "    print(sizeOfEachMatrix)\n",
    "    for batchI in range(batchedPositionTensor.size()[0]): #revolve each batch\n",
    "#         print('batchI',batchI)\n",
    "        timeStepsMatrixList=[]\n",
    "        for timeStepI in range(batchedPositionTensor.size()[1]):#revolve each time step\n",
    "#             print('timeStepI:',timeStepI)\n",
    "#             adjacencyMatrix=np.zeros((sizeOfEachMatrix,sizeOfEachMatrix))\n",
    "            adjacencyList=[]\n",
    "            tempPositionList=batchedPositionTensor[batchI,timeStepI,:,:].numpy().tolist()\n",
    "#             start=time.time()\n",
    "            for i in range(sizeOfEachMatrix):\n",
    "                tempLineList=[]\n",
    "                for j in range(sizeOfEachMatrix):\n",
    "#                     adjacencyMatrix[i,j]=1\n",
    "                    if (tempPositionList[1][i]*tempPositionList[1][j]==0):\n",
    "                        toZero=0\n",
    "                    else:\n",
    "                        toZero=1\n",
    "                        \n",
    "                    #calculate original element with linear function\n",
    "#                     tempLineList.append((1-abs(tempPositionList[1][i]-tempPositionList[1][j]))*\\\n",
    "#                         (1-abs(tempPositionList[0][i]-tempPositionList[0][j]))*toZero)\n",
    "                    \n",
    "                    #calculate original element with exponential function\n",
    "                    element=(omegaY/(math.exp(lambdaY*(abs(tempPositionList[1][j]-tempPositionList[1][i])))))*\\\n",
    "                    (omegaX/(math.exp(lambdaX*(abs(tempPositionList[0][j]-tempPositionList[0][i])))))*toZero\n",
    "                    tempLineList.append(element)\n",
    "#                     adjacencyMatrix[i,j]=(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])*\\\n",
    "#                         (batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])\n",
    "#                     (omegaY/math.exp(lambdaX*abs(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])))*\\\n",
    "#                     (omegaX/math.exp(lambdaY*abs(batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])))\n",
    "                    \n",
    "                    #calculate original element with expenential\n",
    "#                     adjacencyMatrix[i,j]=\n",
    "#                     (omegaY/math.exp(lambdaX*abs(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])))*\\\n",
    "#                     (omegaX/math.exp(lambdaY*abs(batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])))\n",
    "                    if(tempPositionList[1][j]-tempPositionList[1][i]<0):\n",
    "                        #if i follows j, then multiple m, m<1\n",
    "                        tempLineList[j]=tempLineList[j]*m\n",
    "                adjacencyList.append(tempLineList)\n",
    "            \n",
    "#             end=time.time()\n",
    "#             print(end-start)\n",
    "            adjacencyMatrix=torch.tensor(adjacencyList).unsqueeze(0)\n",
    "            if timeStepI==0:\n",
    "                matrixSequenceInTimeStepDim=adjacencyMatrix\n",
    "            else:\n",
    "                matrixSequenceInTimeStepDim=\\\n",
    "                torch.cat((matrixSequenceInTimeStepDim,adjacencyMatrix),0)\n",
    "        matrixSequenceInTimeStepDim=matrixSequenceInTimeStepDim.unsqueeze(0)\n",
    "        if batchI==0:\n",
    "            matrixSequenceInBatchDim=matrixSequenceInTimeStepDim\n",
    "        else:\n",
    "            matrixSequenceInBatchDim=torch.cat((matrixSequenceInBatchDim,matrixSequenceInTimeStepDim),0)            \n",
    "    return matrixSequenceInBatchDim\n",
    "\n",
    "def tensorNormalization(inputTensor,minValue,maxValue):\n",
    "    inputTensor.div_(maxValue)\n",
    "    \n",
    "def batchNormalizationForCombinedTensor(inputBatchedTensor,minX,maxX,minY,maxY,minV,maxV,minA,maxA):\n",
    "    tensorNormalization(inputBatchedTensor[:,:,0,:],minX,maxX)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,1,:],minY,maxY)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,2:4,:],minV,maxV)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,4:6,:],minA,maxA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    __init__(self, input_size, cell_size, hidden_size, output_last = True)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, cell_size, hidden_size, output_last = True):\n",
    "        \"\"\"\n",
    "        cell_size is the size of cell_state.\n",
    "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, input, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((input, Hidden_State), 1)\n",
    "        f = F.sigmoid(self.fl(combined))\n",
    "        i = F.sigmoid(self.il(combined))\n",
    "        o = F.sigmoid(self.ol(combined))\n",
    "        C = F.tanh(self.Cl(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        time_step = inputs.size(1)\n",
    "        Hidden_State, Cell_State = self.initHidden(batch_size)\n",
    "        \n",
    "        if self.output_last:\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:]), Hidden_State, Cell_State)  \n",
    "            return Hidden_State\n",
    "        else:\n",
    "            outputs = None\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:]), Hidden_State, Cell_State)  \n",
    "                if outputs is None:\n",
    "                    outputs = Hidden_State.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "            return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        if use_gpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            return Hidden_State, Cell_State\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            return Hidden_State, Cell_State\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# maxMatrixIndex=250\n",
    "# trajectorDataSet=tensorsDataset(trajectoryFileList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataLoader=DataLoader(trajectorDataSet,batch_size=1,shuffle=True,num_workers=4)\n",
    "# for i,data in enumerate(dataLoader):\n",
    "#     print('11111')\n",
    "#     if(i>10):\n",
    "#         break\n",
    "#     print(data[0].shape)\n",
    "#     print(data[0][0,:,1,1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeTensorData(xTensor,yTensor, maxLength=2500,maxWidth=100,blocksize=10,normalizationDict=None):\n",
    "    \"\"\"\n",
    "    visualize a frame on an white image\n",
    "    Args:\n",
    "        valueVisualize: a list of values, each item in the list can be obtained by function \n",
    "        getValueByLable\n",
    "    Returns:\n",
    "        the image of the input frame\n",
    "    \"\"\"\n",
    "    image=np.ones((maxLength,maxWidth,3),dtype=np.int8)\n",
    "    image=image*255\n",
    "#     figure=plt.figure(figsize=(10,50))\n",
    "#     axe=figure.add_subplot(1,1,1)\n",
    "    xLength=xTensor.shape[0] #the length of y is equivalent to x's\n",
    "    \n",
    "    if doNormalization:\n",
    "        originalXTensor=torch.zeros(xLength)\n",
    "        originalYTensor=torch.zeros(xLength) #originalX and Y tensor share the same length\n",
    "        originalXTensor=torch.add(\\\n",
    "                                  torch.mul(xTensor,normalizationDict['positionXMax']-normalizationDict['positionXMin']),\\\n",
    "                                  torch.add(originalXTensor,normalizationDict['positionXMin'])\n",
    "                                 )\n",
    "        originalYTensor=torch.add(\\\n",
    "                                  torch.mul(xTensor,normalizationDict['positionYMax']-normalizationDict['positionYMin']),\\\n",
    "                                  torch.add(originalYTensor,normalizationDict['positionYMin'])\n",
    "                                 )\n",
    "        for i in range(xLength):\n",
    "            x=int(originalXTensor[i])\n",
    "            y=int(originalYTensor[i])\n",
    "            colorR=int((i*17+29)%255)\n",
    "            colorG=int((i*9++93)%255)\n",
    "            colorB=int((i*13+111)%255)\n",
    "            cv2.circle(image,(x,y),int(blocksize/2),(colorB,colorG,colorR),-1) #\n",
    "    #     axe.imshow(image)\n",
    "        return image\n",
    "        \n",
    "    \n",
    "    \n",
    "    for i in range(xLength):\n",
    "        x=int(xTensor[i])\n",
    "        y=int(yTensor[i])\n",
    "        colorR=int((i*17+29)%255)\n",
    "        colorG=int((i*9++93)%255)\n",
    "        colorB=int((i*13+111)%255)\n",
    "        cv2.circle(image,(x,y),int(blocksize/2),(colorB,colorG,colorR),-1) #\n",
    "#     axe.imshow(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code fragment below is used to visualize tensor data\n",
    "# dataLoader=DataLoader(trajectorDataSet,batch_size=1,shuffle=True,num_workers=4)\n",
    "# for dataI,data in enumerate(dataLoader):\n",
    "#     if(dataI>10):\n",
    "#         break\n",
    "#     for i in range(int(data[0][0,:,0,0].shape[0])):\n",
    "#         tensorImage=visualizeTensorData(data[0][0,i,0,:],data[0][0,i,1,:],2500,100,10) \n",
    "#         fileName=str(100000+dataI)+'_'+str(100000+i)+'.png'\n",
    "#         cv2.imwrite('./tensorVisualizeFolder/'+fileName,tensorImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process objects to generate relation tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30):\n",
    "        super(relationNetwork,self).__init__()\n",
    "        self.layer1=nn.Linear(inputSize,30)\n",
    "        self.layer2=nn.Linear(30,50)\n",
    "        self.layer3=nn.Linear(50,70)\n",
    "        self.layer4=nn.Linear(70,50)\n",
    "        self.layer5=nn.Linear(50,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.ReLU(self.layer5(x4))\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class effectNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process objects to generate relation tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30):\n",
    "        super(effectNetwork,self).__init__()\n",
    "        self.layer1=nn.Linear(inputSize,30)\n",
    "        self.layer2=nn.Linear(30,50)\n",
    "        self.layer3=nn.Linear(50,70)\n",
    "        self.layer4=nn.Linear(70,50)\n",
    "        self.layer5=nn.Linear(50,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.ReLU(self.layer5(x4))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class effectCombinationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process objects to generate relation tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30):\n",
    "        super(effectCombinationNetwork,self).__init__()\n",
    "        self.layer1=nn.Linear(inputSize,30)\n",
    "        self.layer2=nn.Linear(30,50)\n",
    "        self.layer3=nn.Linear(50,70)\n",
    "        self.layer4=nn.Linear(70,50)\n",
    "        self.layer5=nn.Linear(50,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.ReLU(self.layer5(x4))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectModifyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process objects to generate relation tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30):\n",
    "        super(objectModifyNetwork,self).__init__()\n",
    "        self.layer1=nn.Linear(inputSize,30)\n",
    "        self.layer2=nn.Linear(30,50)\n",
    "        self.layer3=nn.Linear(50,70)\n",
    "        self.layer4=nn.Linear(70,50)\n",
    "        self.layer5=nn.Linear(50,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.layer5(x4)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in getitem, combinedTensor shape:  torch.Size([6, 250])\n",
      "in getitem, relation tensorleft shape: torch.Size([249, 6])\n",
      "in getitem, relationtensorright shape torch.Size([249, 6])\n",
      "0\n",
      "torch.Size([1, 62250, 40])\n",
      "torch.Size([1, 62250, 44])\n",
      "in getitem, combinedTensor shape:  torch.Size([6, 250])\n",
      "in getitem, relation tensorleft shape: torch.Size([249, 6])\n",
      "in getitem, relationtensorright shape torch.Size([249, 6])\n"
     ]
    }
   ],
   "source": [
    "#this block build for relation network testing\n",
    "#delete later if needed\n",
    "relationTensorSize=40\n",
    "positionTuple=(0,1,6,7)\n",
    "velocityTuple=(2,3,8,9)\n",
    "acclerateTuple=(4,5,10,11)\n",
    "positionRelationNet=relationNetwork(outputSize=relationTensorSize)\n",
    "velocityRelationNet=relationNetwork(outputSize=relationTensorSize)\n",
    "accelerateRelationNet=relationNetwork(outputSize=relationTensorSize)\n",
    "\n",
    "maxMatrixIndex=250\n",
    "\n",
    "#load test data in network testing block\n",
    "dataloaderV2=DataLoader(datasetV2,batch_size=1,shuffle=True)\n",
    "for i,item in enumerate(dataloaderV2):\n",
    "    if(i>0):\n",
    "        break\n",
    "    print(i)\n",
    "    first,second=item\n",
    "#     print(first.shape,second.shape)\n",
    "#     print(first[0,:5,:6])\n",
    "#     print(first[0,(2,245,246,247,248,249,250,251),6:])\n",
    "    #from frame to position, velocity, accelerate\n",
    "    positionRelationTensors=positionRelationNet(first[:,:,positionTuple])\n",
    "    velocityRelationTensors=velocityRelationNet(first[:,:,velocityTuple])\n",
    "    accelerateRelationTensors=accelerateRelationNet(first[:,:,acclerateTuple])\n",
    "    print(positionRelationTensors.shape)\n",
    "    objectsAndRelationTensors=torch.cat((first[:,:,positionTuple],positionRelationTensors),2)\n",
    "    print(objectsAndRelationTensors.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in getitem, combinedTensor shape:  torch.Size([6, 250])\n",
      "in getitem, relation tensorleft shape: torch.Size([249, 6])\n",
      "in getitem, relationtensorright shape torch.Size([249, 6])\n",
      "torch.Size([1, 62250, 12])\n",
      "torch.Size([1, 250, 60])\n",
      "torch.Size([1, 250, 20])\n",
      "torch.Size([1, 250, 26])\n"
     ]
    }
   ],
   "source": [
    "objectAndTensorSize=relationTensorSize+4 \n",
    "effectOutputTensorSize=20\n",
    "#the number 4 is is the size of positon(or velocity or accelerate) pairs,\n",
    "#such as (positonxObject1,positonyObject1,positionxObject2,positionyObject2)\n",
    "positionEffectNet=effectNetwork(inputSize=objectAndTensorSize,outputSize=effectOutputTensorSize)\n",
    "velocityEffectNet=effectNetwork(inputSize=objectAndTensorSize,outputSize=effectOutputTensorSize)\n",
    "accelerateEffectNet=effectNetwork(inputSize=objectAndTensorSize,outputSize=effectOutputTensorSize)\n",
    "first,second=next(iter(dataloaderV2))\n",
    "print(first.shape)\n",
    "\n",
    "#relation computation\n",
    "positionRelationTensors=positionRelationNet(first[:,:,positionTuple])\n",
    "velocityRelationTensors=velocityRelationNet(first[:,:,velocityTuple])\n",
    "accelerateRelationTensors=accelerateRelationNet(first[:,:,acclerateTuple])\n",
    "                                                      \n",
    "objectsAndPositionRelationTensors=torch.cat((first[:,:,positionTuple],positionRelationTensors),2)\n",
    "objectsAndVelocityRelationTensors=torch.cat((first[:,:,positionTuple],velocityRelationTensors),2)\n",
    "objectsAndAccelerateRelationTensors=torch.cat((first[:,:,positionTuple],accelerateRelationTensors),2)\n",
    "\n",
    "#effect computation\n",
    "positionEffectTensors=positionEffectNet(objectsAndPositionRelationTensors)\n",
    "velocityEffectTensors=velocityEffectNet(objectsAndVelocityRelationTensors)\n",
    "accelerateEffectTensors=accelerateEffectNet(objectsAndAccelerateRelationTensors)\n",
    "\n",
    "#effect combination type 1\n",
    "#tensor summation\n",
    "batchSize=positionRelationTensors.shape[0]\n",
    "positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "for i in range(maxMatrixIndex):\n",
    "    positionEffectSummation[:,i,:]=torch.sum(positionEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "    velocityEffectSummation[:,i,:]=torch.sum(velocityEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "    accelerateEffectSummation[:,i,:]=torch.sum(accelerateEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "    \n",
    "#effect combination net initial\n",
    "#combined tensors length\n",
    "combinedTensorSize=20\n",
    "effectCombinationNet=effectCombinationNetwork(inputSize=effectOutputTensorSize*3,outputSize=combinedTensorSize)\n",
    "\n",
    "#combine tensors and process the combined one\n",
    "combinedEffectTensors=torch.cat((positionEffectSummation,velocityEffectSummation,accelerateEffectSummation),2)\n",
    "print(combinedEffectTensors.shape)\n",
    "processedCombinedEffectTensors=effectCombinationNet(combinedEffectTensors)\n",
    "print(processedCombinedEffectTensors.shape)\n",
    "\n",
    "#generate a tuple in which each element is the index of a vehicle\n",
    "#the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "#this part has been put into init function of Module class\n",
    "listForEachVehicle=[]\n",
    "for i in range(maxMatrixIndex):\n",
    "    listForEachVehicle.append(i*(maxMatrixIndex-1))\n",
    "tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "\n",
    "#the property of each vehicle\n",
    "vehicleProperty=first[:,tupleForEachVehicle,0:6]\n",
    "\n",
    "objectAndFinalEffect=torch.cat((vehicleProperty,processedCombinedEffectTensors),2)\n",
    "print(objectAndFinalEffect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 250, 6])\n"
     ]
    }
   ],
   "source": [
    "#initialize the final network to generate new objects properties\n",
    "objectModifyNet=objectModifyNetwork(inputSize=combinedTensorSize+6,outputSize=6)\n",
    "finalObjectState=objectModifyNet(objectAndFinalEffect)\n",
    "print(finalObjectState.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from relation to new object network\n",
    "class fromRelationToObjectNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(fromRelationToObjectNetwork,self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        \n",
    "        #position relation network initialize\n",
    "        self.relationTensorSize=40\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "        self.velocityTuple=(2,3,8,9)\n",
    "        self.acclerateTuple=(4,5,10,11)\n",
    "        self.positionRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        self.velocityRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        self.accelerateRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        \n",
    "        #effect network initialize\n",
    "        self.objectAndTensorSize=self.relationTensorSize+4 \n",
    "        self.effectOutputTensorSize=20\n",
    "        #the number 4 is is the size of positon(or velocity or accelerate) pairs,\n",
    "        #such as (positonxObject1,positonyObject1,positionxObject2,positionyObject2)\n",
    "        self.positionEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        self.velocityEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        self.accelerateEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        \n",
    "        #effect combination net initialize\n",
    "        #combined tensors length\n",
    "        self.combinedTensorSize=20\n",
    "        self.effectCombinationNet=\\\n",
    "        effectCombinationNetwork(inputSize=self.effectOutputTensorSize*3,outputSize=self.combinedTensorSize)\n",
    "        \n",
    "        #initialize the final network to generate new objects properties\n",
    "        self.objectModifyNet=objectModifyNetwork(inputSize=self.combinedTensorSize+6,outputSize=6)\n",
    "        \n",
    "    def forward(self,inputObjectsPairs):\n",
    "        #relation computation\n",
    "        positionRelationTensors=self.positionRelationNet(inputObjectsPairs[:,:,self.positionTuple])\n",
    "        velocityRelationTensors=self.velocityRelationNet(inputObjectsPairs[:,:,self.velocityTuple])\n",
    "        accelerateRelationTensors=self.accelerateRelationNet(inputObjectsPairs[:,:,self.acclerateTuple])\n",
    "\n",
    "        objectsAndPositionRelationTensors=torch.cat((inputObjectsPairs[:,:,self.positionTuple],positionRelationTensors),2)\n",
    "        objectsAndVelocityRelationTensors=torch.cat((inputObjectsPairs[:,:,self.velocityTuple],velocityRelationTensors),2)\n",
    "        objectsAndAccelerateRelationTensors=torch.cat((inputObjectsPairs[:,:,self.acclerateTuple],accelerateRelationTensors),2)\n",
    "\n",
    "        #effect computation\n",
    "        positionEffectTensors=self.positionEffectNet(objectsAndPositionRelationTensors)\n",
    "        velocityEffectTensors=self.velocityEffectNet(objectsAndVelocityRelationTensors)\n",
    "        accelerateEffectTensors=self.accelerateEffectNet(objectsAndAccelerateRelationTensors)\n",
    "\n",
    "        \n",
    "        #effect combination type 1\n",
    "        #tensor summation\n",
    "        batchSize=positionRelationTensors.shape[0]\n",
    "        if useGpu==True:\n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "            velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "            accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "        else: \n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "            velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "            accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "        for i in range(maxMatrixIndex):\n",
    "            positionEffectSummation[:,i,:]=torch.sum(positionEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "            velocityEffectSummation[:,i,:]=torch.sum(velocityEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "            accelerateEffectSummation[:,i,:]=torch.sum(accelerateEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "        combinedEffectTensors=torch.cat((positionEffectSummation,velocityEffectSummation,accelerateEffectSummation),2)\n",
    "        processedCombinedEffectTensors=self.effectCombinationNet(combinedEffectTensors)\n",
    "        \n",
    "        #the property of each vehicle\n",
    "        vehicleProperty=inputObjectsPairs[:,self.tupleForEachVehicle,0:6]\n",
    "        objectAndFinalEffect=torch.cat((vehicleProperty,processedCombinedEffectTensors),2)\n",
    "        \n",
    "        #compute final state\n",
    "        finalObjectState=self.objectModifyNet(objectAndFinalEffect)\n",
    "        return finalObjectState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxMatrixIndex=250\n",
    "#generate a tuple in which each element is the index of a vehicle\n",
    "#the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "listForEachVehicle=[]\n",
    "for i in range(maxMatrixIndex):\n",
    "    listForEachVehicle.append(i*(maxMatrixIndex-1))\n",
    "tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "\n",
    "dataloaderV2=DataLoader(datasetV2,batch_size=1,shuffle=True)\n",
    "\n",
    "\n",
    "wholeNet=fromRelationToObjectNetwork()\n",
    "if isTest:\n",
    "    wholeNet.load_state_dict(torch.load(modelPath))\n",
    "    \n",
    "\n",
    "if useGpu:\n",
    "    wholeNet.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isTest:\n",
    "    learningRate=1e-3\n",
    "    MSELoss=nn.MSELoss()\n",
    "    lambdaWholeNet=lambda epoch: 0.5**(epoch//30)\n",
    "    optim=torch.optim.RMSprop([{'params':wholeNet.parameters(),'initial_lr':learningRate}],lr=learningRate)\n",
    "    lrSchedule=torch.optim.lr_scheduler.LambdaLR(optim,lambdaWholeNet,last_epoch=10)\n",
    "    wholeNet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "if not isTest:  \n",
    "    losses=[]\n",
    "    iterInEpoch=50\n",
    "    for epoch in range(300):\n",
    "        print(epoch)\n",
    "        for i,item in enumerate(dataloaderV2):\n",
    "            if i>iterInEpoch:\n",
    "                break\n",
    "    #         print(i)\n",
    "            theInput,second=item\n",
    "            if useGpu:\n",
    "                theInput=Variable(theInput.cuda())\n",
    "                second=Variable(second.cuda())\n",
    "            secondObjects=second[:,tupleForEachVehicle,0:6]\n",
    "\n",
    "            output=wholeNet(theInput)\n",
    "    #         print(output[0,0:10,:],secondObjects[0,0:10,:])\n",
    "            loss=MSELoss(output,secondObjects)\n",
    "            if i<5:\n",
    "                print('epoch ',epoch, ' i', i,' loss',loss)\n",
    "    #         print(loss)\n",
    "            losses.append(loss.item())\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        lrSchedule.step()\n",
    "    plt.figure(figsize=(30,30))\n",
    "    plt.plot(losses)\n",
    "    plt.savefig('./losses.png')\n",
    "    torch.save(wholeNet.state_dict(),'./wholeNet_300epoch_50perEpoch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in getitem, combinedTensor shape:  torch.Size([6, 250])\n",
      "in getitem, relation tensorleft shape: torch.Size([249, 6])\n",
      "in getitem, relationtensorright shape torch.Size([249, 6])\n",
      "torch.Size([1, 250, 6])\n",
      "torch.Size([1, 62250, 12])\n",
      "step:  0\n",
      "step:  1\n",
      "step:  2\n",
      "step:  3\n",
      "step:  4\n",
      "step:  5\n",
      "step:  6\n",
      "step:  7\n",
      "step:  8\n",
      "step:  9\n",
      "step:  10\n",
      "step:  11\n",
      "step:  12\n",
      "step:  13\n",
      "step:  14\n",
      "step:  15\n",
      "step:  16\n",
      "step:  17\n",
      "step:  18\n",
      "step:  19\n",
      "step:  20\n",
      "step:  21\n",
      "step:  22\n",
      "step:  23\n",
      "step:  24\n",
      "step:  25\n",
      "step:  26\n",
      "step:  27\n",
      "step:  28\n",
      "step:  29\n",
      "step:  30\n",
      "step:  31\n",
      "step:  32\n",
      "step:  33\n",
      "step:  34\n",
      "step:  35\n",
      "step:  36\n",
      "step:  37\n",
      "step:  38\n",
      "step:  39\n",
      "step:  40\n",
      "step:  41\n",
      "step:  42\n",
      "step:  43\n",
      "step:  44\n",
      "step:  45\n",
      "step:  46\n",
      "step:  47\n",
      "step:  48\n",
      "step:  49\n",
      "step:  50\n",
      "step:  51\n",
      "step:  52\n",
      "step:  53\n",
      "step:  54\n",
      "step:  55\n",
      "step:  56\n",
      "step:  57\n",
      "step:  58\n",
      "step:  59\n",
      "step:  60\n",
      "step:  61\n",
      "step:  62\n",
      "step:  63\n",
      "step:  64\n",
      "step:  65\n",
      "step:  66\n",
      "step:  67\n",
      "step:  68\n",
      "step:  69\n",
      "step:  70\n",
      "step:  71\n",
      "step:  72\n",
      "step:  73\n",
      "step:  74\n",
      "step:  75\n",
      "step:  76\n",
      "step:  77\n",
      "step:  78\n",
      "step:  79\n",
      "step:  80\n",
      "step:  81\n",
      "step:  82\n",
      "step:  83\n",
      "step:  84\n",
      "step:  85\n",
      "step:  86\n",
      "step:  87\n",
      "step:  88\n",
      "step:  89\n",
      "step:  90\n",
      "step:  91\n",
      "step:  92\n",
      "step:  93\n",
      "step:  94\n",
      "step:  95\n",
      "step:  96\n",
      "step:  97\n",
      "step:  98\n",
      "step:  99\n",
      "step:  100\n",
      "step:  101\n",
      "step:  102\n",
      "step:  103\n",
      "step:  104\n",
      "step:  105\n",
      "step:  106\n",
      "step:  107\n",
      "step:  108\n",
      "step:  109\n",
      "step:  110\n",
      "step:  111\n",
      "step:  112\n",
      "step:  113\n",
      "step:  114\n",
      "step:  115\n",
      "step:  116\n",
      "step:  117\n",
      "step:  118\n",
      "step:  119\n",
      "step:  120\n",
      "step:  121\n",
      "step:  122\n",
      "step:  123\n",
      "step:  124\n",
      "step:  125\n",
      "step:  126\n",
      "step:  127\n",
      "step:  128\n",
      "step:  129\n",
      "step:  130\n",
      "step:  131\n",
      "step:  132\n",
      "step:  133\n",
      "step:  134\n",
      "step:  135\n",
      "step:  136\n",
      "step:  137\n",
      "step:  138\n",
      "step:  139\n",
      "step:  140\n",
      "step:  141\n",
      "step:  142\n",
      "step:  143\n",
      "step:  144\n",
      "step:  145\n",
      "step:  146\n",
      "step:  147\n",
      "step:  148\n",
      "step:  149\n",
      "step:  150\n",
      "step:  151\n",
      "step:  152\n",
      "step:  153\n",
      "step:  154\n",
      "step:  155\n",
      "step:  156\n",
      "step:  157\n",
      "step:  158\n",
      "step:  159\n",
      "step:  160\n",
      "step:  161\n",
      "step:  162\n",
      "step:  163\n",
      "step:  164\n",
      "step:  165\n",
      "step:  166\n",
      "step:  167\n",
      "step:  168\n",
      "step:  169\n",
      "step:  170\n",
      "step:  171\n",
      "step:  172\n",
      "step:  173\n",
      "step:  174\n",
      "step:  175\n",
      "step:  176\n",
      "step:  177\n",
      "step:  178\n",
      "step:  179\n",
      "step:  180\n",
      "step:  181\n",
      "step:  182\n",
      "step:  183\n",
      "step:  184\n",
      "step:  185\n",
      "step:  186\n",
      "step:  187\n",
      "step:  188\n",
      "step:  189\n",
      "step:  190\n",
      "step:  191\n",
      "step:  192\n",
      "step:  193\n",
      "step:  194\n",
      "step:  195\n",
      "step:  196\n",
      "step:  197\n",
      "step:  198\n",
      "step:  199\n",
      "step:  200\n",
      "step:  201\n",
      "step:  202\n",
      "step:  203\n",
      "step:  204\n",
      "step:  205\n",
      "step:  206\n",
      "step:  207\n",
      "step:  208\n",
      "step:  209\n",
      "step:  210\n",
      "step:  211\n",
      "step:  212\n",
      "step:  213\n",
      "step:  214\n",
      "step:  215\n",
      "step:  216\n",
      "step:  217\n",
      "step:  218\n",
      "step:  219\n",
      "step:  220\n",
      "step:  221\n",
      "step:  222\n",
      "step:  223\n",
      "step:  224\n",
      "step:  225\n",
      "step:  226\n",
      "step:  227\n",
      "step:  228\n",
      "step:  229\n",
      "step:  230\n",
      "step:  231\n",
      "step:  232\n",
      "step:  233\n",
      "step:  234\n",
      "step:  235\n",
      "step:  236\n",
      "step:  237\n",
      "step:  238\n",
      "step:  239\n",
      "step:  240\n",
      "step:  241\n",
      "step:  242\n",
      "step:  243\n",
      "step:  244\n",
      "step:  245\n",
      "step:  246\n",
      "step:  247\n",
      "step:  248\n",
      "step:  249\n",
      "step:  250\n",
      "step:  251\n",
      "step:  252\n",
      "step:  253\n",
      "step:  254\n",
      "step:  255\n",
      "step:  256\n",
      "step:  257\n",
      "step:  258\n",
      "step:  259\n",
      "step:  260\n",
      "step:  261\n",
      "step:  262\n",
      "step:  263\n",
      "step:  264\n",
      "step:  265\n",
      "step:  266\n",
      "step:  267\n",
      "step:  268\n",
      "step:  269\n",
      "step:  270\n",
      "step:  271\n",
      "step:  272\n",
      "step:  273\n",
      "step:  274\n",
      "step:  275\n",
      "step:  276\n",
      "step:  277\n",
      "step:  278\n",
      "step:  279\n",
      "step:  280\n",
      "step:  281\n",
      "step:  282\n",
      "step:  283\n",
      "step:  284\n",
      "step:  285\n",
      "step:  286\n",
      "step:  287\n",
      "step:  288\n",
      "step:  289\n",
      "step:  290\n",
      "step:  291\n",
      "step:  292\n",
      "step:  293\n",
      "step:  294\n",
      "step:  295\n",
      "step:  296\n",
      "step:  297\n",
      "step:  298\n",
      "step:  299\n",
      "step:  300\n",
      "step:  301\n",
      "step:  302\n",
      "step:  303\n",
      "step:  304\n",
      "step:  305\n",
      "step:  306\n",
      "step:  307\n",
      "step:  308\n",
      "step:  309\n",
      "step:  310\n",
      "step:  311\n",
      "step:  312\n",
      "step:  313\n",
      "step:  314\n",
      "step:  315\n",
      "step:  316\n",
      "step:  317\n",
      "step:  318\n",
      "step:  319\n",
      "step:  320\n",
      "step:  321\n",
      "step:  322\n",
      "step:  323\n",
      "step:  324\n",
      "step:  325\n",
      "step:  326\n",
      "step:  327\n",
      "step:  328\n",
      "step:  329\n",
      "step:  330\n",
      "step:  331\n",
      "step:  332\n",
      "step:  333\n",
      "step:  334\n",
      "step:  335\n",
      "step:  336\n",
      "step:  337\n",
      "step:  338\n",
      "step:  339\n",
      "step:  340\n",
      "step:  341\n",
      "step:  342\n",
      "step:  343\n",
      "step:  344\n",
      "step:  345\n",
      "step:  346\n",
      "step:  347\n",
      "step:  348\n",
      "step:  349\n",
      "step:  350\n",
      "step:  351\n",
      "step:  352\n",
      "step:  353\n",
      "step:  354\n",
      "step:  355\n",
      "step:  356\n",
      "step:  357\n",
      "step:  358\n",
      "step:  359\n",
      "step:  360\n",
      "step:  361\n",
      "step:  362\n",
      "step:  363\n",
      "step:  364\n",
      "step:  365\n",
      "step:  366\n",
      "step:  367\n",
      "step:  368\n",
      "step:  369\n",
      "step:  370\n",
      "step:  371\n",
      "step:  372\n",
      "step:  373\n",
      "step:  374\n",
      "step:  375\n",
      "step:  376\n",
      "step:  377\n",
      "step:  378\n",
      "step:  379\n",
      "step:  380\n",
      "step:  381\n",
      "step:  382\n",
      "step:  383\n",
      "step:  384\n",
      "step:  385\n",
      "step:  386\n",
      "step:  387\n",
      "step:  388\n",
      "step:  389\n",
      "step:  390\n",
      "step:  391\n",
      "step:  392\n",
      "step:  393\n",
      "step:  394\n",
      "step:  395\n",
      "step:  396\n",
      "step:  397\n",
      "step:  398\n",
      "step:  399\n",
      "step:  400\n",
      "step:  401\n",
      "step:  402\n",
      "step:  403\n",
      "step:  404\n",
      "step:  405\n",
      "step:  406\n",
      "step:  407\n",
      "step:  408\n",
      "step:  409\n",
      "step:  410\n",
      "step:  411\n",
      "step:  412\n",
      "step:  413\n",
      "step:  414\n",
      "step:  415\n",
      "step:  416\n",
      "step:  417\n",
      "step:  418\n",
      "step:  419\n",
      "step:  420\n",
      "step:  421\n",
      "step:  422\n",
      "step:  423\n",
      "step:  424\n",
      "step:  425\n",
      "step:  426\n",
      "step:  427\n",
      "step:  428\n",
      "step:  429\n",
      "step:  430\n",
      "step:  431\n",
      "step:  432\n",
      "step:  433\n",
      "step:  434\n",
      "step:  435\n",
      "step:  436\n",
      "step:  437\n",
      "step:  438\n",
      "step:  439\n",
      "step:  440\n",
      "step:  441\n",
      "step:  442\n",
      "step:  443\n",
      "step:  444\n",
      "step:  445\n",
      "step:  446\n",
      "step:  447\n",
      "step:  448\n",
      "step:  449\n",
      "step:  450\n",
      "step:  451\n",
      "step:  452\n",
      "step:  453\n",
      "step:  454\n",
      "step:  455\n",
      "step:  456\n",
      "step:  457\n",
      "step:  458\n",
      "step:  459\n",
      "step:  460\n",
      "step:  461\n",
      "step:  462\n",
      "step:  463\n",
      "step:  464\n",
      "step:  465\n",
      "step:  466\n",
      "step:  467\n",
      "step:  468\n",
      "step:  469\n",
      "step:  470\n",
      "step:  471\n",
      "step:  472\n",
      "step:  473\n",
      "step:  474\n",
      "step:  475\n",
      "step:  476\n",
      "step:  477\n",
      "step:  478\n",
      "step:  479\n",
      "step:  480\n",
      "step:  481\n",
      "step:  482\n",
      "step:  483\n",
      "step:  484\n",
      "step:  485\n",
      "step:  486\n",
      "step:  487\n",
      "step:  488\n",
      "step:  489\n",
      "step:  490\n",
      "step:  491\n",
      "step:  492\n",
      "step:  493\n",
      "step:  494\n",
      "step:  495\n",
      "step:  496\n",
      "step:  497\n",
      "step:  498\n",
      "step:  499\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def fromObjectsToRelationPairs(combinedTensor):\n",
    "    '''\n",
    "    Args:\n",
    "        The input tensor should already be transposed if it is generated from the network's output\n",
    "    Returns:\n",
    "        Relation pairs\n",
    "    '''\n",
    "    #generate relation tensor for all vehicle pairs\n",
    "    relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "    relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "#         print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "    for i in range(1,combinedTensor.shape[1]):\n",
    "        relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                      combinedTensor[:,i].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "        relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                       torch.transpose(torch.cat((combinedTensor[:,:i],combinedTensor[:,i+1:]),1),0,1)),0)\n",
    "#         print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "    combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1) \n",
    "    return combinedRelationTensor\n",
    "\n",
    "normalizationDict=datasetV2.getNormalizationDict()\n",
    "if isTest:\n",
    "    with torch.no_grad():\n",
    "        wholeNet.eval()\n",
    "        theInput,second=datasetV2.__getitem__(random.randint(1000,10000))\n",
    "        theInput=theInput.unsqueeze(0)\n",
    "        second=second.unsqueeze(0)\n",
    "    #         print(i)\n",
    "#         theInput,second=item\n",
    "        if useGpu:\n",
    "            theInput=Variable(theInput.cuda())\n",
    "            second=Variable(second.cuda())\n",
    "        inputObjects=theInput[:,tupleForEachVehicle,0:6]\n",
    "        secondObjects=second[:,tupleForEachVehicle,0:6]\n",
    "        print(inputObjects.shape)\n",
    "        stepInput=fromObjectsToRelationPairs(inputObjects[0].permute(1,0)).unsqueeze(0)\n",
    "    #             output=wholeNet(theInput)\n",
    "        print(stepInput.shape)\n",
    "    #             print(output.shape)\n",
    "        #predict step by step\n",
    "        for step in range(500):\n",
    "            output=wholeNet(stepInput)\n",
    "            print('step: ',step)\n",
    "#             for j in range(10):\n",
    "#                 print(stepInput[:,tupleForEachVehicle,0:6][0,j,:])\n",
    "#                 print(output[0,j,:])\n",
    "#                 print()\n",
    "            stepInput=fromObjectsToRelationPairs(output[0].permute(1,0)).unsqueeze(0)\n",
    "            resultImage=visualizeTensorData(output[0][0].cpu(),output[0][1].cpu(),normalizationDict=normalizationDict)\n",
    "            timeStamp=int(time.time())\n",
    "            fileName='./resultImage/'+str(timeStamp)+str('_')+str(step)+'.png'\n",
    "            cv2.imwrite(fileName,resultImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output results step by step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch] *",
   "language": "python",
   "name": "conda-env-.conda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
