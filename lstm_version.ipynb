{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#global variable\n",
    "doNormalization=True\n",
    "useGpu=False\n",
    "runOnG814=False\n",
    "isTest=False\n",
    "modelPath='/home/wangyuchen/wholeNet_300epoch_50perEpoch.pt'\n",
    "maxMatrixIndex=250\n",
    "runRelationLSTM=False\n",
    "runObjectRelationNet=False\n",
    "runSeq2SeqRelationModel=True\n",
    "runLSTM=False\n",
    "runDifferenceLSTMModel=False\n",
    "\n",
    "maxRelationsNumberGlobal=20 # the maximum nubmer of relation in the \"relation within the given range\" version \n",
    "\n",
    "if useGpu:\n",
    "    import os\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromAllToStr(*args):\n",
    "    returnedStr=str()\n",
    "    for eachItem in args:\n",
    "        returnedStr=returnedStr+str(eachItem)\n",
    "    return returnedStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='logging.txt',level=logging.DEBUG, format='%(asctime)s -%(lineno)d - %(funcName)s - %(levelname)s - %(message)s',)\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s -%(lineno)d - %(funcName)s - %(levelname)s - %(message)s')\n",
    "console.setFormatter(formatter)\n",
    "logging.getLogger('').addHandler(console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a tuple in which each element is the index of a vehicle\n",
    "#the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "listForEachVehicle=[]\n",
    "for i in range(maxMatrixIndex):\n",
    "    listForEachVehicle.append(i*(maxMatrixIndex-1))\n",
    "tupleForEachVehicle=tuple(listForEachVehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def getValueByLable(lableList,valueList):\n",
    "    \"\"\"\n",
    "    For instance, given a lable list ['Local_X','Local_Y'] and a value list [2.0, 24.0, 437.0, 1118846981300.0, 16.254, \n",
    "    79.349, 6451167.199, 1873312.382, 14.5, 4.9, 2.0, 39.14, -5.73, 2.0, 0.0, 13.0, 0.0, 0.0] which values sorted by the \n",
    "    order of allLableList below, the function return a value Dict {'Local_X':16.254, 'Local_Y':79.349}\n",
    "    Args:\n",
    "        lableList: the list of lables you've required, such as['Vehicle_ID', 'Total_Frames','Global_Time']\n",
    "        valueList: the list contains all legally value, sorted by:['Vehicle_ID', 'Frame_ID','Total_Frames','Global_Time','Local_X','Local_Y','Global_X','Global_Y',\\\n",
    "                      'v_Length','v_Width','v_Class','v_Vel','v_Acc','Lane_ID','Preceding','Following','Space_Headway',\\\n",
    "                      'Time_Headway']\n",
    "    Returns: \n",
    "        value dict of the input lables\n",
    "    For instance, given a lable list ['Local_X','Local_Y'] and a value list [2.0, 24.0, 437.0, 1118846981300.0, 16.254, \n",
    "    79.349, 6451167.199, 1873312.382, 14.5, 4.9, 2.0, 39.14, -5.73, 2.0, 0.0, 13.0, 0.0, 0.0] which values sorted by the \n",
    "    order of allLableList above, the function return a value List [16.254, 79.349]\n",
    "\n",
    "    \"\"\"\n",
    "    allLableList=['Vehicle_ID', 'Frame_ID','Total_Frames','Global_Time','Local_X','Local_Y','Global_X','Global_Y',\\\n",
    "                  'v_Length','v_Width','v_Class','v_Vel','v_Acc','Lane_ID','Preceding','Following','Space_Headway',\\\n",
    "                  'Time_Headway']\n",
    "    valueDictReturn={}\n",
    "    for lableItem in lableList:\n",
    "        valueDictReturn[lableItem]=valueList[allLableList.index(lableItem)]\n",
    "    return valueDictReturn\n",
    "\n",
    "def rearrangeDataByGlobalTime(allValueLists):\n",
    "    '''\n",
    "    Args:\n",
    "        allValueLists: all values have been read from a txt file which have already been converted to a list\n",
    "    Returns:\n",
    "        dict have been arranged by global time. One single global time generally contains several value lists.\n",
    "    '''\n",
    "    valueDict={}\n",
    "    for valueList in allValueLists:\n",
    "        dictKey=getValueByLable(['Global_Time'],valueList)['Global_Time']\n",
    "        if dictKey in valueDict:\n",
    "            # if dictKey already there, then add valueList to the list of the key\n",
    "            valueDict[dictKey].append(valueList)\n",
    "        else:\n",
    "            #else, create a list and append valueList on it\n",
    "            valueDict[dictKey]=[valueList]\n",
    "    return valueDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def readFirstFrame(matrixIndexAndVehicleIDRecordDictParam, valueLists):\n",
    "    \"\"\"\n",
    "    To generate the first set of tensors from the first frame\n",
    "    Args:\n",
    "        matrixIndexAndVehicleIDRecordDictParam: just as its name\n",
    "        valueLists: a list consists of all valuelist at one time\n",
    "    Returns:\n",
    "        several tensors arranged by: positionTensor, speedTensor, accTensor, angleTensor,newVehicleList(type:list)\n",
    "    \n",
    "    \"\"\"\n",
    "    maxMatrixIndex=matrixIndexAndVehicleIDRecordDictParam.keys().__len__()-1\n",
    "    #tensors initialize\n",
    "    positionTensor=torch.zeros(2,maxMatrixIndex)\n",
    "    speedTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    accTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    angleTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    newVehicleIDList=[]\n",
    "    curMatrixIndex=0\n",
    "    matrixIndexAndVehicleIDRecordDictParam['time']=getValueByLable([\"Global_Time\"],valueLists[0])['Global_Time']\n",
    "    #fill out all tensors\n",
    "    for eachValueList in valueLists:\n",
    "        #get values from eachValueList, generate dict\n",
    "        returnedEachValueDict=getValueByLable(['Vehicle_ID','Local_X','Local_Y','v_Vel','v_Acc'],eachValueList)\n",
    "        #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "        #angle Tensor assignment is not neeed for the initial value of each element in it is already zero\n",
    "        positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "        speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "        accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "        #then handle the record matrix\n",
    "        matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['Vehicle_ID']=returnedEachValueDict['Vehicle_ID']\n",
    "        matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['refresh']=0\n",
    "        newVehicleIDList.append(copy.deepcopy(returnedEachValueDict['Vehicle_ID']))\n",
    "        curMatrixIndex=curMatrixIndex+1\n",
    "    return positionTensor,speedTensor,accTensor,angleTensor,newVehicleIDList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMatrixIndexByVehicleID(matrixIndexAndVehicleIDRecordDictParam, vehicle_ID):\n",
    "    for i in range(0, len(matrixIndexAndVehicleIDRecordDictParam)-1):\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']==vehicle_ID:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def findEmptyMatrixIndex(matrixIndexAndVehicleIDRecordDictParam):\n",
    "    for i in range(0, len(matrixIndexAndVehicleIDRecordDictParam)-1):\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']==-1:\n",
    "            #Vehicle_ID=-1 when there is no existed vehicle ID bounding to the index\n",
    "            return i\n",
    "    raise Exception(\"NO EMPTY ELEMENT IN MATRIX\")\n",
    "\n",
    "def readGeneralFrame(matrixIndexAndVehicleIDRecordDictParam, valueLists, prePositionTensor):\n",
    "    \"\"\"\n",
    "    To generate the first set of tensors from the general frame that have a preceding one.\n",
    "    In this version, we ignore the new vehicle appeared among a serial of frame.\n",
    "    Args:\n",
    "        matrixIndexAndVehicleIDRecordDictParam: just as its name\n",
    "        valueLists: a list consists of all valuelist at one time\n",
    "        prePositionTensor: positionTensor from the preceding frame, which is used to calculate angle tensor\n",
    "    Returns:\n",
    "        everal tensors arranged by: positionTensor, speedTensor, accTensor, angleTensor,newVehicleList(type:list),\n",
    "        vanishedVehicleList(type:list)\n",
    "    \n",
    "    \"\"\"\n",
    "    #tensors initialize\n",
    "    maxMatrixIndex=matrixIndexAndVehicleIDRecordDictParam.keys().__len__()-1\n",
    "    positionTensor=torch.zeros(2,maxMatrixIndex)\n",
    "    speedTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    accTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    angleTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    newVehicleIDList=[]\n",
    "    vanishedVehicleList=[]\n",
    "    curMatrixIndex=0\n",
    "    matrixIndexAndVehicleIDRecordDictParam['time']=getValueByLable([\"Global_Time\"],valueLists[0])['Global_Time']\n",
    "    #fill out all tensors\n",
    "    for eachValueList in valueLists:\n",
    "        #get values from eachValueList, generate dict\n",
    "        returnedEachValueDict=getValueByLable(['Vehicle_ID','Local_X','Local_Y','v_Vel','v_Acc'],eachValueList)\n",
    "        indexOfVehicle=findMatrixIndexByVehicleID(matrixIndexAndVehicleIDRecordDictParam,returnedEachValueDict['Vehicle_ID'])\n",
    "        if indexOfVehicle!=-1:\n",
    "        #if index exist then the vehicle already existed in the preceded frame\n",
    "            matrixIndexAndVehicleIDRecordDictParam[indexOfVehicle]['refresh']=1\n",
    "            curMatrixIndex=indexOfVehicle\n",
    "            #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "            positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "            speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "            accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "            angleTensor[:,curMatrixIndex]=math.atan2(positionTensor[0,curMatrixIndex]-\\\n",
    "                                                     prePositionTensor[0,curMatrixIndex],\\\n",
    "                                                    positionTensor[1,curMatrixIndex]-prePositionTensor[1,curMatrixIndex])\n",
    "        else:\n",
    "            pass #ignore new vehicleID\n",
    "        #a new vehicle ID\n",
    "#             newVehicleIDList.append(copy.deepcopy(returnedEachValueDict['Vehicle_ID']))\n",
    "#             curMatrixIndex=findEmptyMatrixIndex(matrixIndexAndVehicleIDRecordDictParam)\n",
    "#             matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['Vehicle_ID']=copy.deepcopy(returnedEachValueDict['Vehicle_ID'])\n",
    "#             matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['refresh']=1\n",
    "#             #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "#             positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "#             speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "#             accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "#             angleTensor[:,curMatrixIndex]=math.atan2(positionTensor[0,curMatrixIndex]-\\\n",
    "#                                                      prePositionTensor[0,curMatrixIndex],\\\n",
    "#                                                     positionTensor[1,curMatrixIndex]-prePositionTensor[1,curMatrixIndex])\n",
    "    for i in range(0,maxMatrixIndex):\n",
    "    #find vanished vehicle and remove from dict\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['refresh']==0:\n",
    "            #if refresh=0 then the corresponding vehicle ID was not found in this frame\n",
    "            vanishedVehicleList.append(copy.deepcopy(matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']))\n",
    "            matrixIndexAndVehicleIDRecordDictParam[i]['refresh']=-1\n",
    "            matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']=-1\n",
    "    \n",
    "    for i in range(0,maxMatrixIndex):\n",
    "    #set all refrshed which equivalent to 1 to 0 to prepare for the next frame\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['refresh']==1:\n",
    "                #if refresh=0 then the corresponding vehicle ID was not found in this frame\n",
    "                matrixIndexAndVehicleIDRecordDictParam[i]['refresh']=0\n",
    "\n",
    "    return positionTensor,speedTensor,accTensor,angleTensor,newVehicleIDList,vanishedVehicleList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-18 21:59:52,022 -276 - wrapper - DEBUG - $HOME=/home/wangyuchen\n",
      "2020-08-18 21:59:52,024 -276 - wrapper - DEBUG - CONFIGDIR=/home/wangyuchen/.config/matplotlib\n",
      "2020-08-18 21:59:52,025 -276 - wrapper - DEBUG - matplotlib data path: /home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/matplotlib/mpl-data\n",
      "2020-08-18 21:59:52,030 -1007 - rc_params_from_file - DEBUG - loaded rc file /home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/matplotlib/mpl-data/matplotlibrc\n",
      "2020-08-18 21:59:52,033 -1644 - <module> - DEBUG - matplotlib version 3.1.3\n",
      "2020-08-18 21:59:52,034 -1645 - <module> - DEBUG - interactive is False\n",
      "2020-08-18 21:59:52,035 -1646 - <module> - DEBUG - platform is linux\n",
      "2020-08-18 21:59:52,036 -1647 - <module> - DEBUG - loaded modules: ['builtins', 'sys', '_frozen_importlib', '_imp', '_warnings', '_thread', '_weakref', '_frozen_importlib_external', '_io', 'marshal', 'posix', 'zipimport', 'encodings', 'codecs', '_codecs', 'encodings.aliases', 'encodings.utf_8', '_signal', '__main__', 'encodings.latin_1', 'io', 'abc', '_weakrefset', '_bootlocale', '_locale', 'site', 'os', 'errno', 'stat', '_stat', 'posixpath', 'genericpath', 'os.path', '_collections_abc', '_sitebuiltins', 'sysconfig', '_sysconfigdata_m_linux_x86_64-linux-gnu', 'types', 'functools', '_functools', 'collections', 'operator', '_operator', 'keyword', 'heapq', '_heapq', 'itertools', 'reprlib', '_collections', 'weakref', 'collections.abc', 'importlib', 'importlib._bootstrap', 'importlib._bootstrap_external', 'warnings', 'importlib.util', 'importlib.abc', 'importlib.machinery', 'contextlib', 'mpl_toolkits', 'runpy', 'pkgutil', 'ipykernel', 'ipykernel._version', 'ipykernel.connect', '__future__', 'json', 'json.decoder', 're', 'enum', 'sre_compile', '_sre', 'sre_parse', 'sre_constants', 'copyreg', 'json.scanner', '_json', 'json.encoder', 'subprocess', 'time', 'signal', '_posixsubprocess', 'select', 'selectors', 'math', 'threading', 'traceback', 'linecache', 'tokenize', 'token', 'IPython', 'IPython.core', 'IPython.core.getipython', 'IPython.core.release', 'IPython.core.application', 'atexit', 'copy', 'glob', 'fnmatch', 'logging', 'string', '_string', 'shutil', 'zlib', 'bz2', '_compression', '_bz2', 'lzma', '_lzma', 'pwd', 'grp', 'traitlets', 'traitlets.traitlets', 'inspect', 'ast', '_ast', 'dis', 'opcode', '_opcode', 'six', 'struct', '_struct', 'traitlets.utils', 'traitlets.utils.getargspec', 'traitlets.utils.importstring', 'ipython_genutils', 'ipython_genutils._version', 'ipython_genutils.py3compat', 'ipython_genutils.encoding', 'locale', 'platform', 'traitlets.utils.sentinel', 'traitlets.utils.bunch', 'traitlets._version', 'traitlets.config', 'traitlets.config.application', 'decorator', 'traitlets.config.configurable', 'traitlets.config.loader', 'argparse', 'textwrap', 'gettext', 'ipython_genutils.path', 'random', 'hashlib', '_hashlib', '_blake2', '_sha3', 'bisect', '_bisect', '_random', 'ipython_genutils.text', 'ipython_genutils.importstring', 'IPython.core.crashhandler', 'pprint', 'IPython.core.ultratb', 'pydoc', 'urllib', 'urllib.parse', 'IPython.core.debugger', 'bdb', 'IPython.utils', 'IPython.utils.PyColorize', 'IPython.utils.coloransi', 'IPython.utils.ipstruct', 'IPython.utils.colorable', 'pygments', 'pygments.util', 'IPython.utils.py3compat', 'IPython.utils.encoding', 'IPython.core.excolors', 'IPython.testing', 'IPython.testing.skipdoctest', 'pdb', 'cmd', 'code', 'codeop', 'IPython.core.display_trap', 'IPython.utils.path', 'IPython.utils.process', 'IPython.utils._process_posix', 'pexpect', 'pexpect.exceptions', 'pexpect.utils', 'pexpect.expect', 'pexpect.pty_spawn', 'pty', 'tty', 'termios', 'ptyprocess', 'ptyprocess.ptyprocess', 'fcntl', 'resource', 'ptyprocess.util', 'pexpect.spawnbase', 'pexpect.run', 'IPython.utils._process_common', 'shlex', 'IPython.utils.decorators', 'IPython.utils.data', 'IPython.utils.terminal', 'IPython.utils.sysinfo', 'IPython.utils._sysinfo', 'IPython.core.profiledir', 'IPython.paths', 'tempfile', 'IPython.utils.importstring', 'IPython.terminal', 'IPython.terminal.embed', 'IPython.core.compilerop', 'IPython.core.magic_arguments', 'IPython.core.error', 'IPython.utils.text', 'pathlib', 'ntpath', 'IPython.core.magic', 'getopt', 'IPython.core.oinspect', 'typing', 'typing.io', 'typing.re', 'IPython.core.page', 'IPython.core.display', 'binascii', 'mimetypes', 'IPython.lib', 'IPython.lib.security', 'getpass', 'IPython.lib.pretty', 'datetime', '_datetime', 'IPython.utils.openpy', 'IPython.utils.dir2', 'IPython.utils.wildcard', 'pygments.lexers', 'pygments.lexers._mapping', 'pygments.modeline', 'pygments.plugin', 'pygments.lexers.python', 'pygments.lexer', 'pygments.filter', 'pygments.filters', 'pygments.token', 'pygments.regexopt', 'pygments.unistring', 'pygments.formatters', 'pygments.formatters._mapping', 'pygments.formatters.html', 'pygments.formatter', 'pygments.styles', 'IPython.core.inputtransformer2', 'IPython.core.interactiveshell', 'pickleshare', 'pickle', '_compat_pickle', '_pickle', 'IPython.core.prefilter', 'IPython.core.autocall', 'IPython.core.macro', 'IPython.core.splitinput', 'IPython.core.alias', 'IPython.core.builtin_trap', 'IPython.core.events', 'backcall', 'backcall.backcall', 'IPython.core.displayhook', 'IPython.core.displaypub', 'IPython.core.extensions', 'IPython.core.formatters', 'IPython.utils.sentinel', 'IPython.core.history', 'sqlite3', 'sqlite3.dbapi2', '_sqlite3', 'IPython.core.logger', 'IPython.core.payload', 'IPython.core.usage', 'IPython.display', 'IPython.lib.display', 'html', 'html.entities', 'IPython.utils.io', 'IPython.utils.capture', 'IPython.utils.strdispatch', 'IPython.core.hooks', 'IPython.utils.syspathcontext', 'IPython.utils.tempdir', 'IPython.utils.contexts', 'IPython.core.async_helpers', 'IPython.terminal.interactiveshell', 'asyncio', 'asyncio.base_events', 'concurrent', 'concurrent.futures', 'concurrent.futures._base', 'concurrent.futures.process', 'queue', 'multiprocessing', 'multiprocessing.context', 'multiprocessing.process', 'multiprocessing.reduction', 'socket', '_socket', 'array', '__mp_main__', 'multiprocessing.connection', '_multiprocessing', 'multiprocessing.util', 'concurrent.futures.thread', 'asyncio.compat', 'asyncio.coroutines', 'asyncio.constants', 'asyncio.events', 'asyncio.base_futures', 'asyncio.log', 'asyncio.futures', 'asyncio.base_tasks', '_asyncio', 'asyncio.tasks', 'asyncio.locks', 'asyncio.protocols', 'asyncio.queues', 'asyncio.streams', 'asyncio.subprocess', 'asyncio.transports', 'asyncio.unix_events', 'asyncio.base_subprocess', 'asyncio.selector_events', 'ssl', 'ipaddress', '_ssl', 'base64', 'asyncio.sslproto', 'prompt_toolkit', 'prompt_toolkit.application', 'prompt_toolkit.application.application', 'prompt_toolkit.buffer', 'prompt_toolkit.application.current', 'prompt_toolkit.eventloop', 'prompt_toolkit.eventloop.async_generator', 'prompt_toolkit.eventloop.utils', 'prompt_toolkit.eventloop.dummy_contextvars', 'prompt_toolkit.eventloop.inputhook', 'prompt_toolkit.utils', 'wcwidth', 'wcwidth.wcwidth', 'wcwidth.table_wide', 'wcwidth.table_zero', 'prompt_toolkit.application.run_in_terminal', 'prompt_toolkit.eventloop.async_context_manager', 'prompt_toolkit.auto_suggest', 'prompt_toolkit.document', 'prompt_toolkit.clipboard', 'prompt_toolkit.clipboard.base', 'prompt_toolkit.selection', 'prompt_toolkit.clipboard.in_memory', 'prompt_toolkit.filters', 'prompt_toolkit.filters.app', 'prompt_toolkit.cache', 'prompt_toolkit.enums', 'prompt_toolkit.filters.base', 'prompt_toolkit.filters.cli', 'prompt_toolkit.filters.utils', 'prompt_toolkit.completion', 'prompt_toolkit.completion.base', 'prompt_toolkit.formatted_text', 'prompt_toolkit.formatted_text.ansi', 'prompt_toolkit.output', 'prompt_toolkit.output.base', 'prompt_toolkit.data_structures', 'prompt_toolkit.styles', 'prompt_toolkit.styles.base', 'prompt_toolkit.styles.defaults', 'prompt_toolkit.styles.named_colors', 'prompt_toolkit.styles.style', 'prompt_toolkit.styles.pygments', 'prompt_toolkit.styles.style_transformation', 'colorsys', 'prompt_toolkit.output.color_depth', 'prompt_toolkit.output.defaults', 'prompt_toolkit.patch_stdout', 'prompt_toolkit.output.vt100', 'prompt_toolkit.formatted_text.base', 'prompt_toolkit.mouse_events', 'prompt_toolkit.formatted_text.html', 'xml', 'xml.dom', 'xml.dom.domreg', 'xml.dom.minidom', 'xml.dom.minicompat', 'xml.dom.xmlbuilder', 'xml.dom.NodeFilter', 'prompt_toolkit.formatted_text.pygments', 'prompt_toolkit.formatted_text.utils', 'prompt_toolkit.completion.filesystem', 'prompt_toolkit.completion.fuzzy_completer', 'prompt_toolkit.completion.word_completer', 'prompt_toolkit.completion.nested', 'prompt_toolkit.history', 'prompt_toolkit.search', 'prompt_toolkit.key_binding', 'prompt_toolkit.key_binding.key_bindings', 'prompt_toolkit.keys', 'prompt_toolkit.key_binding.key_processor', 'prompt_toolkit.key_binding.vi_state', 'prompt_toolkit.validation', 'prompt_toolkit.input', 'prompt_toolkit.input.base', 'prompt_toolkit.input.defaults', 'prompt_toolkit.input.typeahead', 'prompt_toolkit.key_binding.bindings', 'prompt_toolkit.key_binding.bindings.page_navigation', 'prompt_toolkit.key_binding.bindings.scroll', 'prompt_toolkit.key_binding.defaults', 'prompt_toolkit.key_binding.bindings.basic', 'prompt_toolkit.key_binding.bindings.named_commands', 'prompt_toolkit.layout', 'prompt_toolkit.layout.containers', 'prompt_toolkit.layout.controls', 'prompt_toolkit.lexers', 'prompt_toolkit.lexers.base', 'prompt_toolkit.lexers.pygments', 'prompt_toolkit.layout.processors', 'prompt_toolkit.layout.utils', 'prompt_toolkit.layout.dimension', 'prompt_toolkit.layout.margins', 'prompt_toolkit.layout.mouse_handlers', 'prompt_toolkit.layout.screen', 'prompt_toolkit.layout.layout', 'prompt_toolkit.layout.menus', 'prompt_toolkit.key_binding.bindings.completion', 'prompt_toolkit.key_binding.bindings.cpr', 'prompt_toolkit.key_binding.bindings.emacs', 'prompt_toolkit.key_binding.bindings.mouse', 'prompt_toolkit.key_binding.bindings.vi', 'prompt_toolkit.input.vt100_parser', 'prompt_toolkit.input.ansi_escape_sequences', 'prompt_toolkit.key_binding.digraphs', 'prompt_toolkit.key_binding.emacs_state', 'prompt_toolkit.layout.dummy', 'prompt_toolkit.renderer', 'prompt_toolkit.application.dummy', 'prompt_toolkit.shortcuts', 'prompt_toolkit.shortcuts.dialogs', 'prompt_toolkit.key_binding.bindings.focus', 'prompt_toolkit.widgets', 'prompt_toolkit.widgets.base', 'prompt_toolkit.widgets.toolbars', 'prompt_toolkit.widgets.dialogs', 'prompt_toolkit.widgets.menus', 'prompt_toolkit.shortcuts.progress_bar', 'prompt_toolkit.shortcuts.progress_bar.base', 'prompt_toolkit.shortcuts.progress_bar.formatters', 'prompt_toolkit.shortcuts.prompt', 'prompt_toolkit.key_binding.bindings.auto_suggest', 'prompt_toolkit.key_binding.bindings.open_in_editor', 'prompt_toolkit.shortcuts.utils', 'pygments.style', 'IPython.terminal.debugger', 'IPython.core.completer', 'unicodedata', 'IPython.core.latex_symbols', 'IPython.utils.generics', 'jedi', 'jedi.api', 'parso', 'parso.parser', 'parso.tree', 'parso._compatibility', 'parso.utils', 'parso.pgen2', 'parso.pgen2.generator', 'parso.pgen2.grammar_parser', 'parso.python', 'parso.python.tokenize', 'parso.python.token', 'parso.grammar', 'parso.python.diff', 'difflib', 'parso.python.parser', 'parso.python.tree', 'parso.python.prefix', 'parso.cache', 'gc', 'parso.python.errors', 'parso.normalizer', 'parso.python.pep8', 'parso.file_io', 'jedi._compatibility', 'jedi.file_io', 'jedi.parser_utils', 'jedi.debug', 'jedi.settings', 'jedi.cache', 'jedi.api.classes', 'jedi.inference', 'jedi.inference.imports', 'jedi.inference.sys_path', 'jedi.inference.cache', 'jedi.inference.base_value', 'jedi.common', 'jedi.common.value', 'jedi.inference.helpers', 'jedi.inference.utils', 'jedi.common.utils', 'jedi.inference.compiled', 'jedi.inference.compiled.value', 'jedi.inference.filters', 'jedi.inference.flow_analysis', 'jedi.inference.recursion', 'jedi.inference.names', 'jedi.inference.docstrings', 'jedi.inference.lazy_value', 'jedi.plugins', 'jedi.inference.compiled.access', 'jedi.inference.compiled.getattr_static', 'jedi.inference.signature', 'jedi.inference.context', 'jedi.inference.analysis', 'jedi.inference.gradual', 'jedi.inference.gradual.typeshed', 'jedi.inference.gradual.stub_value', 'jedi.inference.value', 'jedi.inference.value.module', 'jedi.inference.value.klass', 'jedi.inference.arguments', 'jedi.inference.value.iterable', 'jedi.inference.value.dynamic_arrays', 'jedi.inference.value.function', 'jedi.inference.parser_cache', 'jedi.inference.gradual.generics', 'jedi.inference.value.instance', 'jedi.inference.gradual.typing', 'jedi.inference.gradual.base', 'jedi.inference.gradual.type_var', 'jedi.inference.syntax_tree', 'jedi.inference.gradual.annotation', 'jedi.inference.param', 'jedi.inference.value.decorator', 'jedi.inference.gradual.conversion', 'jedi.api.keywords', 'pydoc_data', 'pydoc_data.topics', 'jedi.api.completion_cache', 'jedi.api.helpers', 'jedi.api.interpreter', 'jedi.inference.compiled.mixed', 'jedi.api.completion', 'jedi.api.strings', 'jedi.api.file_name', 'jedi.api.environment', 'filecmp', 'jedi.inference.compiled.subprocess', 'jedi.inference.compiled.subprocess.functions', 'jedi.api.exceptions', 'jedi.api.project', 'jedi.inference.references', 'jedi.inference.gradual.utils', 'jedi.plugins.registry', 'jedi.plugins.stdlib', 'jedi.plugins.flask', 'jedi.plugins.pytest', 'IPython.terminal.ptutils', 'IPython.terminal.shortcuts', 'IPython.terminal.magics', 'IPython.lib.clipboard', 'IPython.terminal.pt_inputhooks', 'IPython.terminal.prompts', 'IPython.terminal.ipapp', 'IPython.core.magics', 'IPython.core.magics.auto', 'IPython.core.magics.basic', 'IPython.core.magics.code', 'urllib.request', 'email', 'http', 'http.client', 'email.parser', 'email.feedparser', 'email.errors', 'email._policybase', 'email.header', 'email.quoprimime', 'email.base64mime', 'email.charset', 'email.encoders', 'quopri', 'email.utils', 'email._parseaddr', 'calendar', 'email.message', 'uu', 'email._encoded_words', 'email.iterators', 'urllib.error', 'urllib.response', 'IPython.core.magics.config', 'IPython.core.magics.display', 'IPython.core.magics.execution', 'timeit', 'cProfile', '_lsprof', 'profile', 'optparse', 'pstats', 'IPython.utils.module_paths', 'IPython.utils.timing', 'IPython.core.magics.extension', 'IPython.core.magics.history', 'IPython.core.magics.logging', 'IPython.core.magics.namespace', 'IPython.core.magics.osm', 'IPython.core.magics.packaging', 'IPython.core.magics.pylab', 'IPython.core.pylabtools', 'IPython.core.magics.script', 'IPython.lib.backgroundjobs', 'IPython.core.shellapp', 'IPython.extensions', 'IPython.extensions.storemagic', 'IPython.utils.frame', 'jupyter_client', 'jupyter_client._version', 'jupyter_client.connect', 'zmq', 'ctypes', '_ctypes', 'ctypes._endian', 'zmq.backend', 'zmq.backend.select', 'zmq.backend.cython', 'zmq.backend.cython.constants', 'cython_runtime', 'zmq.backend.cython.error', '_cython_0_29_14', 'zmq.backend.cython.message', 'zmq.error', 'zmq.backend.cython.context', 'zmq.backend.cython.socket', 'zmq.backend.cython.utils', 'zmq.backend.cython._poll', 'zmq.backend.cython._version', 'zmq.backend.cython._device', 'zmq.backend.cython._proxy_steerable', 'zmq.sugar', 'zmq.sugar.constants', 'zmq.utils', 'zmq.utils.constant_names', 'zmq.sugar.context', 'zmq.sugar.attrsettr', 'zmq.sugar.socket', 'zmq.sugar.poll', 'zmq.utils.jsonapi', 'zmq.utils.strtypes', 'zmq.sugar.frame', 'zmq.sugar.tracker', 'zmq.sugar.version', 'zmq.sugar.stopwatch', 'jupyter_client.localinterfaces', 'jupyter_core', 'jupyter_core.version', 'jupyter_core.paths', 'jupyter_client.launcher', 'traitlets.log', 'jupyter_client.client', 'jupyter_client.channels', 'jupyter_client.channelsabc', 'jupyter_client.clientabc', 'jupyter_client.manager', 'jupyter_client.kernelspec', 'jupyter_client.managerabc', 'jupyter_client.blocking', 'jupyter_client.blocking.client', 'jupyter_client.blocking.channels', 'jupyter_client.multikernelmanager', 'uuid', 'ctypes.util', 'ipykernel.kernelapp', 'tornado', 'tornado.ioloop', 'numbers', 'tornado.concurrent', 'tornado.log', 'logging.handlers', 'tornado.escape', 'tornado.util', 'tornado.speedups', 'curses', '_curses', 'zmq.eventloop', 'zmq.eventloop.ioloop', 'tornado.platform', 'tornado.platform.asyncio', 'tornado.gen', 'zmq.eventloop.zmqstream', 'ipykernel.iostream', 'imp', 'jupyter_client.session', 'hmac', 'jupyter_client.jsonutil', 'dateutil', 'dateutil._version', 'dateutil.parser', 'dateutil.parser._parser', 'decimal', '_decimal', 'dateutil.relativedelta', 'dateutil._common', 'dateutil.tz', 'dateutil.tz.tz', 'six.moves', 'dateutil.tz._common', 'dateutil.tz._factories', 'dateutil.parser.isoparser', '_strptime', 'jupyter_client.adapter', 'ipykernel.heartbeat', 'ipykernel.ipkernel', 'IPython.utils.tokenutil', 'ipykernel.comm', 'ipykernel.comm.manager', 'ipykernel.comm.comm', 'ipykernel.kernelbase', 'tornado.queues', 'tornado.locks', 'ipykernel.jsonutil', 'ipykernel.zmqshell', 'IPython.core.payloadpage', 'ipykernel.displayhook', 'ipykernel.eventloops', 'distutils', 'distutils.version', 'ipykernel.parentpoller', 'faulthandler', 'ipykernel.datapub', 'ipykernel.serialize', 'ipykernel.pickleutil', 'ipykernel.codeutil', 'IPython.core.completerlib', 'storemagic', 'jedi.inference.finder', 'numpy', 'numpy._globals', 'numpy.__config__', 'numpy.version', 'numpy._distributor_init', 'mkl', 'mkl._mklinit', 'mkl._py_mkl_service', 'numpy.core', 'numpy.core.multiarray', 'numpy.core.overrides', 'numpy.core._multiarray_umath', 'numpy.compat', 'numpy.compat._inspect', 'numpy.compat.py3k', 'numpy.core.umath', 'numpy.core.numerictypes', 'numpy.core._string_helpers', 'numpy.core._type_aliases', 'numpy.core._dtype', 'numpy.core.numeric', 'numpy.core.shape_base', 'numpy.core._asarray', 'numpy.core.fromnumeric', 'numpy.core._methods', 'numpy.core._exceptions', 'numpy.core._ufunc_config', 'numpy.core.arrayprint', 'numpy.core.defchararray', 'numpy.core.records', 'numpy.core.memmap', 'numpy.core.function_base', 'numpy.core.machar', 'numpy.core.getlimits', 'numpy.core.einsumfunc', 'numpy.core._add_newdocs', 'numpy.core._multiarray_tests', 'numpy.core._dtype_ctypes', 'numpy.core._internal', 'numpy._pytesttester', 'numpy.lib', 'numpy.lib.mixins', 'numpy.lib.scimath', 'numpy.lib.type_check', 'numpy.lib.ufunclike', 'numpy.lib.index_tricks', 'numpy.matrixlib', 'numpy.matrixlib.defmatrix', 'numpy.linalg', 'numpy.linalg.linalg', 'numpy.lib.twodim_base', 'numpy.linalg.lapack_lite', 'numpy.linalg._umath_linalg', 'numpy.lib.function_base', 'numpy.lib.histograms', 'numpy.lib.stride_tricks', 'numpy.lib.nanfunctions', 'numpy.lib.shape_base', 'numpy.lib.polynomial', 'numpy.lib.utils', 'numpy.lib.arraysetops', 'numpy.lib.npyio', 'numpy.lib.format', 'numpy.lib._datasource', 'numpy.lib._iotools', 'numpy.lib.financial', 'numpy.lib.arrayterator', 'numpy.lib.arraypad', 'numpy.lib._version', 'numpy.fft', 'numpy.fft._pocketfft', 'numpy.fft._pocketfft_internal', 'numpy.fft.helper', 'numpy.polynomial', 'numpy.polynomial.polynomial', 'numpy.polynomial.polyutils', 'numpy.polynomial._polybase', 'numpy.polynomial.chebyshev', 'numpy.polynomial.legendre', 'numpy.polynomial.hermite', 'numpy.polynomial.hermite_e', 'numpy.polynomial.laguerre', 'numpy.random', 'numpy.random._pickle', 'numpy.random.mtrand', 'numpy.random._bit_generator', 'numpy.random._common', 'secrets', 'numpy.random._bounded_integers', 'numpy.random._mt19937', 'numpy.random._philox', 'numpy.random._pcg64', 'numpy.random._sfc64', 'numpy.random._generator', 'numpy.ctypeslib', 'numpy.ma', 'numpy.ma.core', 'numpy.ma.extras', 'numpy.testing', 'unittest', 'unittest.result', 'unittest.util', 'unittest.case', 'unittest.suite', 'unittest.loader', 'unittest.main', 'unittest.runner', 'unittest.signals', 'numpy.testing._private', 'numpy.testing._private.utils', 'numpy.testing._private.decorators', 'numpy.testing._private.nosetester', 'torch', 'torch._utils', 'torch._utils_internal', 'torch.version', 'torch._six', 'torch._C._onnx', 'torch._C._jit_tree_views', 'torch._C.cpp', 'torch._C.cpp.nn', 'torch._C', 'torch.random', 'torch.serialization', 'tarfile', 'torch._tensor_str', 'torch.tensor', 'torch._namedtensor_internals', 'torch.utils', 'torch.utils.throughput_benchmark', 'torch.utils.hooks', 'torch.storage', 'torch.cuda', 'torch.cuda._utils', 'torch.cuda.memory', 'torch.cuda.random', 'torch.cuda.sparse', 'torch.cuda.profiler', 'torch.cuda.nvtx', 'torch.cuda.streams', 'torch.sparse', 'torch.functional', 'torch.nn', 'torch.nn.modules', 'torch.nn.modules.module', 'torch.nn.parameter', 'torch.nn.modules.linear', 'torch.nn.functional', 'torch.nn._reduction', 'torch.nn.modules.utils', 'torch.nn.grad', 'torch.nn._VF', 'torch._jit_internal', 'torch.nn.init', 'torch.nn.modules.conv', 'torch.nn.modules.activation', 'torch.nn.modules.loss', 'torch.nn.modules.container', 'torch.nn.modules.pooling', 'torch.nn.modules.batchnorm', 'torch.nn.modules._functions', 'torch.autograd', 'torch.autograd.variable', 'torch.autograd.function', 'torch.autograd.gradcheck', 'torch.testing', 'torch.autograd.grad_mode', 'torch.autograd.anomaly_mode', 'torch.autograd.profiler', 'torch.nn.modules.instancenorm', 'torch.nn.modules.normalization', 'torch.nn.modules.dropout', 'torch.nn.modules.padding', 'torch.nn.modules.sparse', 'torch.nn.modules.rnn', 'torch.nn.utils', 'torch.nn.utils.rnn', 'torch.nn.utils.clip_grad', 'torch.nn.utils.weight_norm', 'torch.nn.utils.convert_parameters', 'torch.nn.utils.spectral_norm', 'torch.nn.utils.fusion', 'torch.nn.modules.pixelshuffle', 'torch.nn.modules.upsampling', 'torch.nn.modules.distance', 'torch.nn.modules.fold', 'torch.nn.modules.adaptive', 'torch.nn.modules.transformer', 'torch.nn.modules.flatten', 'torch.nn.parallel', 'torch.nn.parallel.parallel_apply', 'torch.nn.parallel.replicate', 'torch.cuda.comm', 'torch.cuda.nccl', 'torch.nn.parallel.data_parallel', 'torch.nn.parallel.scatter_gather', 'torch.nn.parallel._functions', 'torch.nn.parallel.distributed', 'torch.distributed', 'torch.distributed.distributed_c10d', 'torch.distributed.rendezvous', 'torch.nn.intrinsic', 'torch.nn.intrinsic.modules', 'torch.nn.intrinsic.modules.fused', 'torch.nn.quantized', 'torch.nn.quantized.modules', 'torch.nn.quantized.modules.activation', 'torch.nn.quantized.functional', 'torch.nn.quantized.modules.conv', 'torch.nn.intrinsic.qat', 'torch.nn.intrinsic.qat.modules', 'torch.nn.intrinsic.qat.modules.linear_relu', 'torch.nn.qat', 'torch.nn.qat.modules', 'torch.nn.qat.modules.linear', 'torch.nn.qat.modules.conv', 'torch.nn.intrinsic.qat.modules.conv_fused', 'torch._ops', 'torch.jit', 'torch.backends', 'torch.backends.cudnn', 'torch.jit.annotations', 'torch.jit._recursive', 'torch.jit.frontend', 'torch.nn.quantized.modules.utils', 'torch.nn.quantized.modules.linear', 'torch.nn.quantized.modules.functional_modules', 'torch.optim', 'torch.optim.adadelta', 'torch.optim.optimizer', 'torch.optim.adagrad', 'torch.optim.adam', 'torch.optim.adamw', 'torch.optim.sparse_adam', 'torch.optim.adamax', 'torch.optim.asgd', 'torch.optim.sgd', 'torch.optim.rprop', 'torch.optim.rmsprop', 'torch.optim.lbfgs', 'torch.optim.lr_scheduler', 'torch.multiprocessing', 'torch.multiprocessing.reductions', 'multiprocessing.resource_sharer', 'torch.multiprocessing.spawn', 'torch.utils.backcompat', 'torch.onnx', 'torch.hub', 'zipfile', 'torch.distributions', 'torch.distributions.bernoulli', 'torch.distributions.constraints', 'torch.distributions.exp_family', 'torch.distributions.distribution', 'torch.distributions.utils', 'torch.distributions.beta', 'torch.distributions.dirichlet', 'torch.distributions.binomial', 'torch.distributions.categorical', 'torch.distributions.cauchy', 'torch.distributions.chi2', 'torch.distributions.gamma', 'torch.distributions.constraint_registry', 'torch.distributions.transforms', 'torch.distributions.exponential', 'torch.distributions.fishersnedecor', 'torch.distributions.geometric', 'torch.distributions.gumbel', 'torch.distributions.uniform', 'torch.distributions.transformed_distribution', 'torch.distributions.half_cauchy', 'torch.distributions.half_normal', 'torch.distributions.normal', 'torch.distributions.independent', 'torch.distributions.kl', 'torch.distributions.laplace', 'torch.distributions.lowrank_multivariate_normal', 'torch.distributions.multivariate_normal', 'torch.distributions.one_hot_categorical', 'torch.distributions.pareto', 'torch.distributions.poisson', 'torch.distributions.log_normal', 'torch.distributions.logistic_normal', 'torch.distributions.multinomial', 'torch.distributions.negative_binomial', 'torch.distributions.relaxed_bernoulli', 'torch.distributions.relaxed_categorical', 'torch.distributions.studentT', 'torch.distributions.weibull', 'torch.backends.cuda', 'torch.backends.mkl', 'torch.backends.openmp', 'torch.backends.quantized', 'torch.quantization', 'torch.quantization.quantize', 'torch.quantization.default_mappings', 'torch.nn.intrinsic.quantized', 'torch.nn.intrinsic.quantized.modules', 'torch.nn.intrinsic.quantized.modules.linear_relu', 'torch.nn.intrinsic.quantized.modules.conv_relu', 'torch.nn.quantized.dynamic', 'torch.nn.quantized.dynamic.modules', 'torch.nn.quantized.dynamic.modules.linear', 'torch.nn.quantized.dynamic.modules.rnn', 'torch.quantization.stubs', 'torch.quantization.qconfig', 'torch.quantization.observer', 'torch.quantization.fake_quantize', 'torch.quantization.fuse_modules', 'torch.utils.data', 'torch.utils.data.sampler', 'torch.utils.data.distributed', 'torch.utils.data.dataset', 'torch.utils.data.dataloader', 'torch.utils.data._utils', 'torch.utils.data._utils.worker', 'torch.utils.data._utils.signal_handling', 'torch.utils.data._utils.pin_memory', 'torch.utils.data._utils.collate', 'torch.utils.data._utils.fetch', 'torch.__config__', 'torch.__future__', 'torch._torch_docs', 'torch._tensor_docs', 'torch._storage_docs', 'torch._classes', 'torch.quasirandom', 'pandas', 'pytz', 'pytz.exceptions', 'pytz.lazy', 'pytz.tzinfo', 'pytz.tzfile', 'pandas.compat', 'pandas.compat.numpy', 'pandas._libs', 'pandas._libs.tslibs', 'pandas._libs.tslibs.conversion', 'pandas._libs.tslibs.c_timestamp', 'pandas._libs.tslibs.nattype', 'pandas._libs.tslibs.np_datetime', 'pandas._libs.tslibs.timezones', 'pandas._libs.tslibs.tzconversion', 'pandas._libs.tslibs.timedeltas', 'pandas._libs.tslibs.offsets', 'pandas._libs.tslibs.ccalendar', 'pandas._config', 'pandas._config.config', 'pandas._config.dates', 'pandas._config.display', 'pandas._config.localization', 'pandas._libs.tslibs.strptime', 'pandas._libs.tslibs.fields', 'pandas._libs.tslibs.parsing', 'pandas._libs.tslibs.frequencies', 'pandas._libs.tslibs.period', 'pandas._libs.tslibs.timestamps', 'pandas._libs.tslibs.resolution', 'pandas._libs.hashtable', 'pandas._libs.missing', 'pandas._libs.ops_dispatch', 'pandas._libs.lib', 'fractions', 'pandas._libs.tslib', 'pandas.core', 'pandas.core.config_init', 'pandas.core.api', 'pandas.core.dtypes', 'pandas.core.dtypes.dtypes', 'pandas._libs.interval', 'pandas._libs.algos', 'pandas._typing', 'pandas.core.dtypes.base', 'pandas.errors', 'pandas.core.dtypes.generic', 'pandas.core.dtypes.inference', 'pandas.core.dtypes.missing', 'pandas.core.dtypes.common', 'pandas.core.algorithms', 'pandas.util', 'pandas.util._decorators', 'pandas._libs.properties', 'pandas.core.util', 'pandas.core.util.hashing', 'pandas._libs.hashing', 'pandas.core.dtypes.cast', 'pandas.util._validators', 'pandas.core.common', 'pandas.core.construction', 'pandas.core.indexers', 'pandas.core.arrays', 'pandas.core.arrays.base', 'pandas.compat.numpy.function', 'pandas.core.ops', 'pandas.core.ops.array_ops', 'pandas._libs.ops', 'pandas.core.ops.missing', 'pandas.core.ops.roperator', 'pandas.core.ops.dispatch', 'pandas.core.ops.invalid', 'pandas.core.ops.common', 'pandas.core.ops.docstrings', 'pandas.core.ops.mask_ops', 'pandas.core.ops.methods', 'pandas.core.missing', 'pandas.compat._optional', 'pandas.core.sorting', 'pandas.core.arrays.boolean', 'pandas.core.nanops', 'pandas.core.arrays.masked', 'pandas.core.arrays.categorical', 'pandas.core.accessor', 'pandas.core.base', 'pandas.io', 'pandas.io.formats', 'pandas.io.formats.console', 'pandas.core.arrays.datetimes', 'pandas.core.arrays.datetimelike', 'pandas.tseries', 'pandas.tseries.frequencies', 'pandas.tseries.offsets', 'dateutil.easter', 'pandas.core.arrays._ranges', 'pandas.core.arrays.integer', 'pandas.core.tools', 'pandas.core.tools.numeric', 'pandas.core.arrays.interval', 'pandas.core.indexes', 'pandas.core.indexes.base', 'pandas._libs.index', 'pandas._libs.join', 'pandas.core.dtypes.concat', 'pandas.core.indexes.frozen', 'pandas.io.formats.printing', 'pandas.core.strings', 'pandas.core.arrays.numpy_', 'pandas.core.arrays.period', 'pandas.core.arrays.sparse', 'pandas.core.arrays.sparse.accessor', 'pandas.core.arrays.sparse.array', 'pandas._libs.sparse', 'pandas.core.arrays.sparse.dtype', 'pandas.core.arrays.string_', 'pandas.core.arrays.timedeltas', 'pandas.core.groupby', 'pandas.core.groupby.generic', 'pandas.core.frame', 'pandas.core.generic', 'pandas.core.indexes.api', 'pandas.core.indexes.category', 'pandas.core.indexes.extension', 'pandas.core.indexes.datetimes', 'pandas.core.indexes.datetimelike', 'pandas.core.indexes.numeric', 'pandas.core.tools.timedeltas', 'pandas.core.tools.datetimes', 'pandas.arrays', 'pandas.core.indexes.interval', 'pandas.util._exceptions', 'pandas.core.indexes.multi', 'pandas.core.indexes.timedeltas', 'pandas.core.indexes.period', 'pandas.core.indexes.range', 'pandas.core.indexing', 'pandas._libs.indexing', 'pandas.core.internals', 'pandas.core.internals.blocks', 'pandas._libs.writers', 'pandas._libs.internals', 'pandas.core.internals.managers', 'pandas.core.internals.concat', 'pandas.io.formats.format', 'pandas.io.common', 'gzip', 'mmap', 'pandas.core.internals.construction', 'pandas.core.series', 'pandas._libs.reshape', 'pandas.core.indexes.accessors', 'pandas.plotting', 'pandas.plotting._core', 'pandas.plotting._misc', 'pandas.core.window', 'pandas.core.window.ewm', 'pandas._libs.window', 'pandas._libs.window.aggregations', 'pandas.core.window.common', 'pandas.core.groupby.base', 'pandas.core.window.rolling', 'pandas.core.window.indexers', 'pandas._libs.window.indexers', 'pandas.core.window.numba_', 'pandas.core.window.expanding', 'pandas.core.groupby.groupby', 'pandas._libs.groupby', 'pandas.core.groupby.ops', 'pandas._libs.reduction', 'pandas.core.groupby.grouper', 'pandas.core.groupby.categorical', 'pandas.tseries.api', 'pandas.core.computation', 'pandas.core.computation.api', 'pandas.core.computation.eval', 'pandas.core.computation.engines', 'pandas.core.computation.align', 'pandas.core.computation.common', 'pandas.core.computation.ops', 'pandas.core.computation.scope', 'pandas.compat.chainmap', 'pandas.core.computation.expr', 'pandas.core.computation.parsing', 'pandas.core.reshape', 'pandas.core.reshape.api', 'pandas.core.reshape.concat', 'pandas.core.reshape.melt', 'pandas.core.reshape.merge', 'pandas.core.reshape.pivot', 'pandas.core.reshape.util', 'pandas.core.reshape.reshape', 'pandas.core.reshape.tile', 'pandas.api', 'pandas.api.extensions', 'pandas.api.indexers', 'pandas.api.types', 'pandas.core.dtypes.api', 'pandas.util._print_versions', 'pandas.io.api', 'pandas.io.clipboards', 'pandas.io.excel', 'pandas.io.excel._base', 'pandas._libs.parsers', 'csv', '_csv', 'pandas.io.excel._util', 'pandas.io.parsers', 'pandas.io.date_converters', 'pandas.io.excel._odfreader', 'pandas.io.excel._openpyxl', 'pandas.io.excel._xlrd', 'pandas.io.excel._pyxlsb', 'pandas.io.excel._xlsxwriter', 'pandas._libs.json', 'pandas.io.excel._xlwt', 'pandas.io.feather_format', 'pandas.io.gbq', 'pandas.io.html', 'pandas.io.json', 'pandas.io.json._json', 'pandas.io.json._normalize', 'pandas.io.json._table_schema', 'pandas.io.orc', 'pandas.io.parquet', 'pandas.io.pickle', 'pandas.compat.pickle_compat', 'pandas.io.pytables', 'pandas.core.computation.pytables', 'pandas.io.sas', 'pandas.io.sas.sasreader', 'pandas.io.spss', 'pandas.io.sql', 'pandas.io.stata', 'pandas.util._tester', 'pandas.testing', 'pandas._testing', 'pandas._libs.testing', 'pandas._version', 'matplotlib', 'matplotlib.cbook', 'matplotlib.cbook.deprecation', 'matplotlib.rcsetup', 'matplotlib.fontconfig_pattern', 'pyparsing', 'matplotlib.colors', 'matplotlib._color_data', 'cycler', 'matplotlib._version', 'matplotlib.ft2font', 'kiwisolver']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-18 21:59:52,133 -276 - wrapper - DEBUG - CACHEDIR=/home/wangyuchen/.cache/matplotlib\n",
      "2020-08-18 21:59:52,136 -1360 - <module> - DEBUG - Using fontManager instance from /home/wangyuchen/.cache/matplotlib/fontlist-v310.json\n",
      "2020-08-18 21:59:52,256 -225 - switch_backend - DEBUG - Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "2020-08-18 21:59:52,262 -225 - switch_backend - DEBUG - Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "2020-08-18 21:59:52,265 -225 - switch_backend - DEBUG - Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as utils\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "if not runOnG814:\n",
    "    %matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromDirGenerateDict(trajectoryDir):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        valueDict: the key is global time, and the value of each key contain SEVERAL LIST of properties, \n",
    "                   each list consist of all property of a single vehicle at one time.\n",
    "    \"\"\"\n",
    "    trajectoryDataFile=open(trajectoryDir)\n",
    "    count=0\n",
    "    allLineList=[]\n",
    "    count=0\n",
    "    for count,line in enumerate(trajectoryDataFile):\n",
    "        #read a single line, remove space and enter\n",
    "        lineList=line.split(' ')\n",
    "        try:\n",
    "            while True:\n",
    "                lineList.remove('')\n",
    "        except:\n",
    "            try:\n",
    "                lineList.remove('\\n')\n",
    "            except:\n",
    "                pass\n",
    "            pass\n",
    "        for i in range(0,lineList.__len__()):\n",
    "            # convert string to float\n",
    "            lineList[i]=float(lineList[i])\n",
    "        allLineList.append(lineList)\n",
    "    valueDict=rearrangeDataByGlobalTime(allLineList)\n",
    "    return valueDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxAndMinValueFromValueDict(valueDict,lableList):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        valueDict: each key in dict is global time, the value of each key is a list of all value at one time\n",
    "        lableList: lables from which you want to get the max and min value. the type of each value in the list \n",
    "                    is str.\n",
    "    Returns:\n",
    "        a dict, which has keys keys from the input lable list and the value of each key is a dict which formed\n",
    "        as 'max':value, 'min':value\n",
    "    \"\"\"\n",
    "    maxAndMinDict={}\n",
    "    keys=list(valueDict.keys())\n",
    "    for lable in lableList:\n",
    "        max=0\n",
    "        min=0 #speed,  positon are all from 0 to max, accelerate from - to +\n",
    "        for eachKey in keys:\n",
    "            valueLists=valueDict[eachKey]\n",
    "            for valueList in valueLists:\n",
    "                value=getValueByLable([lable],valueList)[lable]\n",
    "                if value>max:\n",
    "                    max=value\n",
    "                if value<min:\n",
    "                    min=value\n",
    "        maxAndMinDict[lable]={'max':max,'min':min}\n",
    "    return maxAndMinDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test function of finding the max and min value: block 1, get valueDict for saving time from file reaidng\n",
    "# valueDict=fromDirGenerateDict(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test function of finding the max and min value: block 1, get valueDict for saving time from file reaidng\n",
    "# getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Acc','v_Vel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valueDict=fromDirGenerateDict(1)\n",
    "# theKey=list(valueDict.keys())[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeTensorData(xTensor,yTensor, maxLength=2500,maxWidth=100,blocksize=10,normalizationDict=False):\n",
    "    \"\"\"\n",
    "    visualize a frame on an white image\n",
    "    Args:\n",
    "        valueVisualize: a list of values, each item in the list can be obtained by function \n",
    "        getValueByLable\n",
    "    Returns:\n",
    "        the image of the input frame\n",
    "    \"\"\"\n",
    "    image=np.ones((maxLength,maxWidth,3),dtype=np.int8)\n",
    "    #set background to white\n",
    "    image=image*255\n",
    "#     figure=plt.figure(figsize=(10,50))\n",
    "#     axe=figure.add_subplot(1,1,1)\n",
    "    xLength=xTensor.shape[0] #the length of y is equivalent to x's\n",
    "#     print('length:',xLength)\n",
    "#     print('xTensor.shape',xTensor.shape)\n",
    "    if doNormalization&(normalizationDict is not False):\n",
    "        originalXTensor=torch.zeros(xLength)\n",
    "        originalYTensor=torch.zeros(xLength) #originalX and Y tensor share the same length\n",
    "        originalXTensor=torch.add(\\\n",
    "                                  torch.mul(xTensor,normalizationDict['positionXMax']-normalizationDict['positionXMin']),\\\n",
    "                                  torch.add(originalXTensor,normalizationDict['positionXMin'])\n",
    "                                 )\n",
    "        originalYTensor=torch.add(\\\n",
    "                                  torch.mul(yTensor,normalizationDict['positionYMax']-normalizationDict['positionYMin']),\\\n",
    "                                  torch.add(originalYTensor,normalizationDict['positionYMin'])\n",
    "                                 )\n",
    "        for i in range(xLength):\n",
    "            x=int(originalXTensor[i])\n",
    "            y=int(originalYTensor[i])\n",
    "            colorR=int((i*17+29)%255)\n",
    "            colorG=int((i*9++93)%255)\n",
    "            colorB=int((i*13+111)%255)\n",
    "            cv2.circle(image,(x,y),int(blocksize/2),(colorB,colorG,colorR),-1) #\n",
    "    #     axe.imshow(image)\n",
    "        return image\n",
    "        \n",
    "    \n",
    "    \n",
    "    for i in range(xLength):\n",
    "        x=int(xTensor[i])\n",
    "        y=int(yTensor[i])\n",
    "        colorR=int((i*17+29)%255)\n",
    "        colorG=int((i*9++93)%255)\n",
    "        colorB=int((i*13+111)%255)\n",
    "        cv2.circle(image,(x,y),int(blocksize/2),(colorB,colorG,colorR),-1) #\n",
    "#     axe.imshow(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeTrajectory(inputTensor, maxLength=2500, maxLength=100,normalizationDict=None, vehicleDict=None):\n",
    "    \"\"\"\n",
    "    to visualize the trajectory of all or selected vehicles\n",
    "    Args:\n",
    "        inputTensor: Tensor to be visualized. The dimension of the tensor is supposed to be (batch, timestep, vehicles, properties)\n",
    "        vehicleDict: If vehicleDict is not none, visualize all vehicles; if not, then only visualize vehicles of the given number.\n",
    "    Returns:\n",
    "        image: the visualized result.\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeTrajectoryDataHorizontal(xTensor,yTensor, maxLength=2500,maxWidth=100,blocksize=10,normalizationDict=False):\n",
    "    \"\"\"\n",
    "    visualize a frame on an white image\n",
    "    Args:\n",
    "        valueVisualize: a list of values, each item in the list can be obtained by function \n",
    "        getValueByLable\n",
    "    Returns:\n",
    "        the image of the input frame\n",
    "    \"\"\"\n",
    "    image=np.ones((maxWidth,maxLength,3),dtype=np.int8)\n",
    "    #set background to white\n",
    "    image=image*255\n",
    "#     figure=plt.figure(figsize=(10,50))\n",
    "#     axe=figure.add_subplot(1,1,1)\n",
    "    xLength=xTensor.shape[0] #the length of y is equivalent to x's\n",
    "#     print('length:',xLength)\n",
    "#     print('xTensor.shape',xTensor.shape)\n",
    "    if doNormalization&(normalizationDict is not False):\n",
    "        originalXTensor=torch.zeros(xLength)\n",
    "        originalYTensor=torch.zeros(xLength) #originalX and Y tensor share the same length\n",
    "        originalXTensor=torch.add(\\\n",
    "                                  torch.mul(xTensor,normalizationDict['positionXMax']-normalizationDict['positionXMin']),\\\n",
    "                                  torch.add(originalXTensor,normalizationDict['positionXMin'])\n",
    "                                 )\n",
    "        originalYTensor=torch.add(\\\n",
    "                                  torch.mul(yTensor,normalizationDict['positionYMax']-normalizationDict['positionYMin']),\\\n",
    "                                  torch.add(originalYTensor,normalizationDict['positionYMin'])\n",
    "                                 )\n",
    "        for i in range(xLength):\n",
    "            x=int(originalXTensor[i])\n",
    "            y=int(originalYTensor[i])\n",
    "            colorR=int((i*17+29)%255)\n",
    "            colorG=int((i*9++93)%255)\n",
    "            colorB=int((i*13+111)%255)\n",
    "            cv2.circle(image,(y,x),int(blocksize/2),(colorB,colorG,colorR),-1) #\n",
    "    #     axe.imshow(image)\n",
    "        return image\n",
    "        \n",
    "    \n",
    "    \n",
    "    for i in range(xLength):\n",
    "        x=int(xTensor[i])\n",
    "        y=int(yTensor[i])\n",
    "        colorR=int((i*17+29)%255)\n",
    "        colorG=int((i*9++93)%255)\n",
    "        colorB=int((i*13+111)%255)\n",
    "        cv2.circle(image,(y,x),int(blocksize/2),(colorB,colorG,colorR),-1) #\n",
    "#     axe.imshow(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-18 21:59:52,537 -225 - switch_backend - DEBUG - Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "if not runOnG814:\n",
    "    %matplotlib inline\n",
    "from IPython import display\n",
    "def visualizeData(valueVisualize, maxLength=1000,maxWidth=100,blocksize=10):\n",
    "    \"\"\"\n",
    "    visualize a frame on an white image\n",
    "    Args:\n",
    "        valueVisualize: a list of values, each item in the list can be obtained by function \n",
    "        getValueByLable\n",
    "    Returns:\n",
    "        the image of the input frame\n",
    "    \"\"\"\n",
    "    image=np.ones((maxLength,maxWidth,3),dtype=np.int8)\n",
    "    image=image*255\n",
    "#     figure=plt.figure(figsize=(10,50))\n",
    "#     axe=figure.add_subplot(1,1,1)\n",
    "    \n",
    "    for item in valueVisualize:\n",
    "        infoList=getValueByLable(['Vehicle_ID','Local_X','Local_Y'],item)\n",
    "        vehicleID=infoList['Vehicle_ID']\n",
    "        x=int(infoList['Local_X'])\n",
    "        y=int(infoList['Local_Y'])\n",
    "        colorR=int((vehicleID+100)%255)\n",
    "        colorG=int((vehicleID+150)%255)\n",
    "        colorB=int((vehicleID+200)%255)\n",
    "        cv2.circle(image,(x,y),int(blocksize/2),(colorB,colorG,colorR),-1) #\n",
    "#     axe.imshow(image)\n",
    "    return image\n",
    "# visualizeData(valueDict[theKey])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalEvaluation(resultTensor,labelTensor,normalizedDict=None,maxMatrixIndex=maxMatrixIndex):\n",
    "    \"\"\"\n",
    "    numericla evalution for models\n",
    "    REMOVE VELOCITY AND ACCELERATE FROM THE INPUT TENSOR IF THEY EXISTS IN THE INPUT TENSORS!!!\n",
    "    Args:\n",
    "        resultTensor: the predicted tensor of model, which dimension is (batch, timestep, vehicles, properties)\n",
    "        labelTensor: the label tensor from dataset, which dimension is (batch, timestep, vehicles, properties)\n",
    "        normalizedDict: the dict of normalization\n",
    "    Returns:\n",
    "        differenceEachVehicleEachFrame: the difference of each vehicle in each single frame\n",
    "        differenceEachVehicleAllFrame: the difference of each vehicle in all frame\n",
    "        averageDifferenceAllVehicleEachFrame: the average difference of all vehicle in each singel frame\n",
    "        averageDifferenceAllVehicleAllFrame: the average difference of all vehicle in over all frame\n",
    "    \"\"\"\n",
    "    differenceEachVehicleEachFrame=torch.abs(resultTensor-labelTensor)\n",
    "    differenceEachVehicleAllFrame=torch.sum(differenceEachVehicleEachFrame,dim=1,keepdim=True)\n",
    "    averageDifferenceAllVehicleEachFrame=torch.sum(differenceEachVehicleEachFrame,dim=2, keepdim=True)\n",
    "    averageDifferenceAllVehicleEachFrame=torch.div(averageDifferenceAllVehicleEachFrame,resultTensor.shape[2])\n",
    "    averageDifferenceAllVehicleAllFrame=torch.sum(averageDifferenceAllVehicleEachFrame,dim=1,keepdim=True)\n",
    "    return differenceEachVehicleEachFrame,differenceEachVehicleAllFrame,averageDifferenceAllVehicleEachFrame,\\\n",
    "            averageDifferenceAllVehicleAllFrame\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discountParameterByExponentialWithDistance(relationTensors, xfactor=1, yfactor=1, w1=1,w2=2,w3=1):\n",
    "    \"\"\"\n",
    "    to calculate a discount parameter matrix from the relations matrix of several vehicl pairs.\n",
    "    Args:\n",
    "        relationTensors: a vehicle pair is a \"relation\" in which two vehilce properties are of the same kind and order,\n",
    "        the relationTensors consists of many vehicle pairs. An extra dimension should be added to the a single vehicle pair\n",
    "        tensor if this function only take as a single vehicle pair. Generally, the relationTensors has to dimension:\n",
    "        the dimension of pairs, the properties of each pair.\n",
    "        xfactor,yfactor:  the weight of x part and y part\n",
    "        w1, w2,w3: facor in exponential operation, w1 and w2 relate to x and w3 relate to y\n",
    "    \"\"\"\n",
    "    vehiclePairDimension=relationTensors.shape[1] #the second dimension of relationTensors is the properties of each vehicle pair\n",
    "    relationsDimension=relationTensors.shape[0]\n",
    "    computationTensor=torch.zeros((relationsDimension,5)) #save the xDifference and yDifference for further computation\n",
    "    secondVehiclePropertyStartIndex=int((vehiclePairDimension)/2 )\n",
    "    logging.debug('secondVehiclePropertyStartIndex'+str(secondVehiclePropertyStartIndex))\n",
    "    \n",
    "    discountTensor=torch.zeros(relationsDimension)\n",
    "    for i in range(relationsDimension):\n",
    "        x1,y1,x2,y2=relationTensors[i][0],relationTensors[i][1],\\\n",
    "                    relationTensors[i][secondVehiclePropertyStartIndex],relationTensors[i][secondVehiclePropertyStartIndex+1]\n",
    "        if y2>=y1: #the second vehilce is in front of the first vehicle\n",
    "            yDifference=y2-y1\n",
    "            wy=w1\n",
    "        else: #the second vehicle is after the fist vehicle\n",
    "            yDifference=y1-y2\n",
    "            wy=w2\n",
    "        xDifference=abs(x1-x2)\n",
    "        wx=w3\n",
    "        computationTensor[i][0]=xDifference\n",
    "        computationTensor[i][1]=yDifference\n",
    "        computationTensor[i][2]=wx\n",
    "        computationTensor[i][3]=wy\n",
    "        if (x1==0 and y1==0)or(x2==0 and y2==0):\n",
    "            computationTensor[i][4]=0\n",
    "        else:\n",
    "            computationTensor[i][4]=1\n",
    "#         discountTensor[i]=(xfactor/math.exp(wx*(xDifference)))*(yfactor/math.exp(wx*(yDifference)))\n",
    "    logging.debug(fromAllToStr('computationTensor:\\n',computationTensor))\n",
    "    discountTensor=torch.mul(torch.mul((xfactor/torch.exp(torch.mul(computationTensor[:,0],computationTensor[:,2]))),\\\n",
    "                             (yfactor/torch.exp(torch.mul(computationTensor[:,1],computationTensor[:,3])))),\n",
    "                             computationTensor[:,4])\n",
    "    return discountTensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input: property tensors, target vehicle ID\n",
    "#output: relations, distances between target vehicle and other vehicles which inside the range\n",
    "def relationCalculateWithRange(propertyTensors, distanceRange, targetVehicleId, maxRelationsNumber=maxRelationsNumberGlobal):\n",
    "    \"\"\"\n",
    "    NOTICE:THE PROPERTIES AND DISTANCE RANGE SHOULD BOTH BE NORMALIZED OR UNNORMALIZED!!!\n",
    "    Args:\n",
    "        propertyTensors:property tensors of all vehicles, at position 0 and 1 are the x and y positon of the \n",
    "        corresponding vehicle\n",
    "        distanceRange: the distance on which we decide to take other vehicles into account\n",
    "        targetVehicleId: the center vehicle which we are going to calculate\n",
    "    \"\"\"\n",
    "    propertyTensorsCopy=copy.deepcopy(propertyTensors)\n",
    "    propertiesDimension=propertyTensorsCopy.shape[0]\n",
    "    reservedIndexes=[]\n",
    "    reservedIndexDistanceDict={}\n",
    "    for i in range(maxMatrixIndex):\n",
    "        reservedIndexes.append(i)\n",
    "    #remove vehicles which position are out-of-range\n",
    "    for i in range(propertiesDimension):\n",
    "        #simply remove the out-of-ranged vehicles by comparing y-axis before computing the distance.\n",
    "        #since most vehicles are out of the given range, this would reduce time cost\n",
    "        if abs(propertyTensorsCopy[i][1]-propertyTensors[targetVehicleId][1])>distanceRange:\n",
    "            reservedIndexes.remove(i)\n",
    "        #to compute if vehicle in the range\n",
    "        else:\n",
    "            distance=((propertyTensorsCopy[i][0]-propertyTensors[targetVehicleId][0])**2+\\\n",
    "                (propertyTensorsCopy[i][1]-propertyTensors[targetVehicleId][1])**2)**0.5\n",
    "            if distance>distanceRange:\n",
    "                reservedIndexes.remove(i)\n",
    "            else:\n",
    "                reservedIndexDistanceDict[i]=distance\n",
    "    #sort dict by value, not by key\n",
    "    sortedReservedIndexDistanceDict=sorted(reservedIndexDistanceDict.items(),key=lambda item:(item[1],item[0]))\n",
    "    #keep the top 'maxRelationsNumber' nearest vehicles in the generated relations\n",
    "    if(sortedReservedIndexDistanceDict.__len__()>maxRelationsNumber):\n",
    "        for i in range(maxRelationsNumber,sortedReservedIndexDistanceDict.__len__()):\n",
    "            reservedIndexes.remove(sortedReservedIndexDistanceDict[i][0])\n",
    "    #the final properties tensor is:\n",
    "    finalPropertiesTensor=propertyTensorsCopy[reservedIndexes]\n",
    "    if(sortedReservedIndexDistanceDict.__len__()<maxRelationsNumber):\n",
    "        #make sure the length of all relation are equanl to the value of maxRelationNumber\n",
    "        finalPropertiesTensor=torch.cat((finalPropertiesTensor,\\\n",
    "                                         torch.zeros((maxRelationsNumber-finalPropertiesTensor.shape[0],finalPropertiesTensor.shape[1]))))\n",
    "    \n",
    "#     logging.debug('targetVehicleId'+str(targetVehicleId))\n",
    "#     logging.debug('reservedIndexes.__len__()'+str(reservedIndexes.__len__()))\n",
    "#     logging.debug('propertyTensors[targetVehicleId].shape[0]'+str(propertyTensors[targetVehicleId].shape[0]))\n",
    "    expandedTargetVehicleTensor=propertyTensors[targetVehicleId].expand(maxRelationsNumber,propertyTensors[targetVehicleId].shape[0])\n",
    "    relationTensor=torch.cat((expandedTargetVehicleTensor,finalPropertiesTensor),1)\n",
    "#     logging.debug('expandedTargetVehicleTensor:'+str(expandedTargetVehicleTensor))\n",
    "#     logging.debug('relationTensor:'+str(relationTensor))\n",
    "    return relationTensor\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeRelationAndAllTheOtherTensorsWithDistance(inputFrameTensor,theGivenRange,maxRelationsNumber=20):\n",
    "    '''\n",
    "        to generate relation tensors, discount parameter tensor with relations and the relation quantity tensor\n",
    "    of each vehicle\n",
    "    Args:\n",
    "        inputFrameTensor: vehicle properties graph, which dimension are (timestep, property, vehicle).\n",
    "        To illustrate the meaning of dimensions, supposing we have a inputFrameTensor which \n",
    "        timestep is 10, all vehicle have 6 properties and there are 250 vehicles, then the dimension of the \n",
    "        input tensor are(10, 6, 250)\n",
    "        theGivenRange: only take vehicle pairs which distance are inside the given range into account.\n",
    "    Returns:\n",
    "        relationTensor: the relation tensors of each vehicle pairs in the given range\n",
    "        discountParameterTensor: the discount tensor of each relation, computed by the distance between vehicle pairs\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    #circulation for batch list\n",
    "    logging.debug(fromAllToStr('inputFrameTensor.shape:',inputFrameTensor.shape))\n",
    "    timeStepSize=inputFrameTensor.shape[0]\n",
    "    for timeStepCount in range(0,timeStepSize):\n",
    "        for vehicleId in range(0, inputFrameTensor.shape[2]):\n",
    "            relationTensor=relationCalculateWithRange(inputFrameTensor[timeStepCount].permute(1,0),\n",
    "                                                      theGivenRange,vehicleId,maxRelationsNumber=maxRelationsNumber)\n",
    "            if vehicleId==0:\n",
    "                relationTensorInOneTimeStep=relationTensor\n",
    "            elif vehicleId>0:\n",
    "                relationTensorInOneTimeStep=torch.cat((relationTensorInOneTimeStep,relationTensor),dim=0)\n",
    "        logging.debug(fromAllToStr('relationTensorInOneTimeStep.shape:\\n',relationTensorInOneTimeStep.shape))\n",
    "        discountParameterTensorInOneTimeStep=discountParameterByExponentialWithDistance(relationTensorInOneTimeStep)\n",
    "        if timeStepCount==0:\n",
    "            relationTensorOfAllTimeSteps=relationTensorInOneTimeStep.unsqueeze(0)\n",
    "            discountParameterTensorofAllTimeSteps=discountParameterTensorInOneTimeStep.unsqueeze(0)\n",
    "        else:\n",
    "            relationTensorOfAllTimeSteps=\\\n",
    "            torch.cat((relationTensorOfAllTimeSteps,relationTensorInOneTimeStep.unsqueeze(0)),dim=0)\n",
    "            discountParameterTensorofAllTimeSteps=\\\n",
    "            torch.cat((discountParameterTensorofAllTimeSteps,discountParameterTensorInOneTimeStep.unsqueeze(0)),dim=0)\n",
    "    return relationTensorOfAllTimeSteps,discountParameterTensorofAllTimeSteps\n",
    "    \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-16 21:32:05,024 -5 - <module> - DEBUG - tensor([0.3280, 0.6456, 0.0249, 0.3275, 0.5580, 0.3207])\n",
      "2020-08-16 21:32:05,077 -6 - <module> - DEBUG - tensor([0.6341, 0.1774, 0.4473, 0.0833, 0.3976, 0.8944])\n",
      "2020-08-16 21:32:05,084 -7 - <module> - DEBUG - tensor([0.8582, 0.8033, 0.0583, 0.9560, 0.5706, 0.9628])\n",
      "2020-08-16 21:32:05,088 -8 - <module> - DEBUG - tensor([[0.3280, 0.6456, 0.0249, 0.3275, 0.5580, 0.3207],\n",
      "        [0.6341, 0.1774, 0.4473, 0.0833, 0.3976, 0.8944],\n",
      "        [0.8582, 0.8033, 0.0583, 0.9560, 0.5706, 0.9628]])\n",
      "2020-08-16 21:32:05,095 -10 - <module> - DEBUG - tensor([[0.3280, 0.6456, 0.0249, 0.3275, 0.5580, 0.3207],\n",
      "        [0.3280, 0.6456, 0.0249, 0.3275, 0.5580, 0.3207],\n",
      "        [0.6341, 0.1774, 0.4473, 0.0833, 0.3976, 0.8944],\n",
      "        [0.8582, 0.8033, 0.0583, 0.9560, 0.5706, 0.9628]])\n"
     ]
    }
   ],
   "source": [
    "tensor1=torch.rand((6))\n",
    "tensor2=torch.rand((6))\n",
    "tensor3=torch.rand((6))\n",
    "tensor4=torch.stack((tensor1,tensor2,tensor3),dim=0)\n",
    "logging.debug(tensor1)\n",
    "logging.debug(tensor2)\n",
    "logging.debug(tensor3)\n",
    "logging.debug(tensor4)\n",
    "tensor5=torch.cat((tensor1.unsqueeze(0),tensor4),dim=0)\n",
    "logging.debug(tensor5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differenceBetweenTwoFrame(frameSeries):\n",
    "    \"\"\"\n",
    "    Given a series of frame, return the difference series of those frame. Since this function \n",
    "    compute diffrences by the gap between two adjacent frames, the quantity of frame in difference \n",
    "    series is one less than the input frame series\n",
    "    Args:\n",
    "        frameSeries: input frames, which dimension is (vehicleQuantity, vehicleProperties)\n",
    "    Returns:\n",
    "        the difference series, which first dimension (quantity dimension) is one less than input frame series.\n",
    "    \"\"\"\n",
    "    frameSeriesWithoutTheFirstFrame=frameSeries[1:]\n",
    "    frameSeriesWithoutTheLastFrame=frameSeries[0:-1]\n",
    "    logging.debug(str(frameSeriesWithoutTheFirstFrame))\n",
    "    logging.debug(str(frameSeriesWithoutTheLastFrame))\n",
    "    logging.debug(str(frameSeries))\n",
    "    differenceSeries=frameSeriesWithoutTheFirstFrame-frameSeriesWithoutTheLastFrame\n",
    "    return differenceSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differenceBetweenTwoFrameForTimeSteps(frameSeries):\n",
    "    \"\"\"\n",
    "    Given several time stpes of series of frame, return the difference series of all those frames. Since this function \n",
    "    compute diffrences by the gap between two adjacent frames, the quantity of frame in difference \n",
    "    series is one less than the input frame series\n",
    "    Args:\n",
    "        frameSeries: input frames, which dimension is (timeSteps,vehicleQuantity, vehicleProperties)\n",
    "    Returns:\n",
    "        differenceSeries:the difference series, which second dimension (quantity dimension) is one less than input frame series,\n",
    "        and the dimension of differenceSeries is (timeSteps, vehicleQuantity, vehiclePropertiesDifference)\n",
    "    \"\"\"\n",
    "    \n",
    "    frameSeriesWithoutTheFirstFrame=frameSeries[:,1:]\n",
    "    frameSeriesWithoutTheLastFrame=frameSeries[:,0:-1]\n",
    "    logging.debug(str(frameSeriesWithoutTheFirstFrame))\n",
    "    logging.debug(str(frameSeriesWithoutTheLastFrame))\n",
    "    logging.debug(str(frameSeries))\n",
    "    differenceSeries=frameSeriesWithoutTheFirstFrame-frameSeriesWithoutTheLastFrame\n",
    "    return differenceSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-16 21:32:05,139 -2 - <module> - DEBUG - tensor([[0.0675, 0.7193, 0.3434, 0.8403, 0.7547],\n",
      "        [0.9539, 0.1993, 0.2080, 0.4035, 0.7249],\n",
      "        [0.1155, 0.0179, 0.8843, 0.9894, 0.0436],\n",
      "        [0.3494, 0.4331, 0.9145, 0.2858, 0.7302],\n",
      "        [0.5358, 0.6017, 0.1698, 0.7595, 0.9041]])\n",
      "2020-08-16 21:32:05,143 -13 - differenceBetweenTwoFrame - DEBUG - tensor([[0.9539, 0.1993, 0.2080, 0.4035, 0.7249],\n",
      "        [0.1155, 0.0179, 0.8843, 0.9894, 0.0436],\n",
      "        [0.3494, 0.4331, 0.9145, 0.2858, 0.7302],\n",
      "        [0.5358, 0.6017, 0.1698, 0.7595, 0.9041]])\n",
      "2020-08-16 21:32:05,144 -14 - differenceBetweenTwoFrame - DEBUG - tensor([[0.0675, 0.7193, 0.3434, 0.8403, 0.7547],\n",
      "        [0.9539, 0.1993, 0.2080, 0.4035, 0.7249],\n",
      "        [0.1155, 0.0179, 0.8843, 0.9894, 0.0436],\n",
      "        [0.3494, 0.4331, 0.9145, 0.2858, 0.7302]])\n",
      "2020-08-16 21:32:05,145 -15 - differenceBetweenTwoFrame - DEBUG - tensor([[0.0675, 0.7193, 0.3434, 0.8403, 0.7547],\n",
      "        [0.9539, 0.1993, 0.2080, 0.4035, 0.7249],\n",
      "        [0.1155, 0.0179, 0.8843, 0.9894, 0.0436],\n",
      "        [0.3494, 0.4331, 0.9145, 0.2858, 0.7302],\n",
      "        [0.5358, 0.6017, 0.1698, 0.7595, 0.9041]])\n",
      "2020-08-16 21:32:05,145 -4 - <module> - DEBUG - shapesize2\n"
     ]
    }
   ],
   "source": [
    "inputTensor=torch.rand((5,5))\n",
    "logging.debug(inputTensor)\n",
    "differenceBetweenTwoFrame(inputTensor)\n",
    "logging.debug(fromAllToStr('shapesize',inputTensor.shape.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testExpand=torch.tensor(((1,2,3),(3,4,5))).expand(2,6)\n",
    "# print(testExpand)\n",
    "# theList=[1,2,3,5,7]\n",
    "# theList.remove(7)\n",
    "# d = {'lilee':25, 'wangyan':21, 'liqun':32, 'age':19}\n",
    "# print(d)\n",
    "# d=sorted(d.items(), key=lambda item:item[1])\n",
    "# print(d.__len__())\n",
    "# print(theList)\n",
    "# testTensor=torch.rand(10,10)\n",
    "# print(testTensor)\n",
    "# print(testTensor[[1,4,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save visulized images\n",
    "# for key in list(valueDict.keys())[1:10000]:\n",
    "#     image=visualizeData(valueDict[key])\n",
    "#     cv2.imwrite('visualizeFolder/image'+str(key)+'.png',image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensorsDataset(Dataset):\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=100,lableTensorEachBatch=2):\n",
    "        if(numberOfTensorsEachBatch<5):\n",
    "            raise Exception(\"THE NUMBER OF TENSORS IN EACH BATCH IS TOO SMALL\")\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the ture index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                 speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                 accTensor.mul(angleCosTensor)),0)\n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "            if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "            elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "            else:\n",
    "                allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "        return allCombineTensorTrain,allCombineTensorValid\n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class tensorsDatasetV2(Dataset):\n",
    "    \"\"\"\n",
    "    dataset for relation model\n",
    "    \"\"\"\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=1,lableTensorEachBatch=1):\n",
    "        if(numberOfTensorsEachBatch!=1 or lableTensorEachBatch!=1):\n",
    "            raise Exception(\"BOTH TRAIN AND VALID TENSOR NUMBERS SHOULD BE ONE!\")\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        if doNormalization:\n",
    "            self.positionXMax=0\n",
    "            self.positionXMin=999999\n",
    "            self.positionYMax=0\n",
    "            self.positionYMin=99999\n",
    "            self.speedMax=-100\n",
    "            self.speedMin=999999\n",
    "            self.accMax=-100\n",
    "            self.accMin=9999\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            if doNormalization:\n",
    "                #get the max and min value for normalization\n",
    "                maxAndMinDict=getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Vel','v_Acc'])\n",
    "                #position X\n",
    "                if self.positionXMax<maxAndMinDict['Local_X']['max']:\n",
    "                    self.positionXMax=maxAndMinDict['Local_X']['max']\n",
    "                if self.positionXMin>maxAndMinDict['Local_X']['min']:\n",
    "                    self.positionXMin=maxAndMinDict['Local_X']['min']\n",
    "                #position Y\n",
    "                if self.positionYMax<maxAndMinDict['Local_Y']['max']:\n",
    "                    self.positionYMax=maxAndMinDict['Local_Y']['max']\n",
    "                if self.positionYMin>maxAndMinDict['Local_Y']['min']:\n",
    "                    self.positionYMin=maxAndMinDict['Local_Y']['min']\n",
    "                #speed\n",
    "                if self.speedMax<maxAndMinDict['v_Vel']['max']:\n",
    "                    self.speedMax=maxAndMinDict['v_Vel']['max']\n",
    "                if self.speedMin>maxAndMinDict['v_Vel']['min']:\n",
    "                    self.speedMin=maxAndMinDict['v_Vel']['min']\n",
    "                #acc\n",
    "                if self.accMax<maxAndMinDict['v_Acc']['max']:\n",
    "                    self.accMax=maxAndMinDict['v_Acc']['max']\n",
    "                if self.accMin>maxAndMinDict['v_Acc']['min']:\n",
    "                    self.accMin=maxAndMinDict['v_Acc']['min']\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def getNormalizationDict(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            a dict:{'positionXMax':self.positionXMax,'positonYMax':self.self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "        '''\n",
    "        if not doNormalization:\n",
    "            raise Exception('NORMALIZATION IS NOT APPLIED')\n",
    "        return {'positionXMax':self.positionXMax,'positionYMax':self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':self.speedMin,\\\n",
    "               'accMax':self.accMax,'accMin':self.accMin}\n",
    "    \n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the ture index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        #first frame normalization\n",
    "        if doNormalization:\n",
    "#             print('before nomalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                     torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "            speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "            accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "#             print('after normalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        else:\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        #generate relation tensor for all vehicle pairs\n",
    "        print('in getitem, combinedTensor shape: ',combinedTensor.shape)\n",
    "        relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "        relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "        print('in getitem, relation tensorleft shape:',relationTensorLeft.shape)\n",
    "        print('in getitem, relationtensorright shape',relationTensorRight.shape)\n",
    "#         print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "        for i in range(1,combinedTensor.shape[1]):\n",
    "            relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                          combinedTensor[:,i].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "            relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                           torch.transpose(torch.cat((combinedTensor[:,:i],combinedTensor[:,i+1:]),1),0,1)),0)\n",
    "#         print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "        combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1)  \n",
    "        firstCombinedRelationTensor=combinedRelationTensor\n",
    "        \n",
    "        \n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            if doNormalization:\n",
    "                positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                         torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "                speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "                accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                         speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            else:\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            #generate relation tensor for all vehicle pairs\n",
    "            relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "            relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "#             print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "            for j in range(1,combinedTensor.shape[1]):\n",
    "                relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                              combinedTensor[:,j].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "                relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                               torch.transpose(torch.cat((combinedTensor[:,:j],combinedTensor[:,j+1:]),1),0,1)),0)\n",
    "#             print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "            combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1)  \n",
    "            secondRelationTensor=combinedRelationTensor\n",
    "            #since we only need two tensors, which is input and output tensor respectively, we could return\n",
    "            #the two tensors in the first loop\n",
    "            #(ok I admit that the true reason is that I am lazy)\n",
    "            return firstCombinedRelationTensor,secondRelationTensor\n",
    "#             if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "#                 allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "#             elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "#                 allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "#             else:\n",
    "#                 allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "        return allCombineTensorTrain,allCombineTensorValid\n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n",
    "#run on 2080 in g814\n",
    "if runOnG814:\n",
    "    trajectoryFileList=['/home/wangyuchen/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromObjectsToRelationPairsBatchAndTimestepVersion(batchAndTimestepCombinedTensor):\n",
    "    '''\n",
    "    This function based on the other function termed as 'fromObjectsToRelationPairs'. Instead of process a \n",
    "    single frame, this function takes batch and timestep(the other dimension) into consideration.\n",
    "    note: the dimension of combinedTensor is supposed to be (batchs, timesteps, properties, vehicles)\n",
    "    Args:\n",
    "        The input tensor should already be transposed if it is generated from the network's output\n",
    "    Returns:\n",
    "        Relation pairs\n",
    "    '''\n",
    "    #generate relation tensor for all vehicle pairs\n",
    "    batchSize=batchAndTimestepCombinedTensor.shape[0]\n",
    "    timesteps=batchAndTimestepCombinedTensor.shape[1]\n",
    "    for batch in range(batchSize):\n",
    "        for timestep in range(timesteps):\n",
    "            combinedTensor=batchAndTimestepCombinedTensor[batch,timestep,:,:]\n",
    "            relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "            relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "        #         print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "            for i in range(1,combinedTensor.shape[1]):\n",
    "                relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                              combinedTensor[:,i].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "                relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                               torch.transpose(torch.cat((combinedTensor[:,:i],combinedTensor[:,i+1:]),1),0,1)),0)\n",
    "        #         print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "            combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1) \n",
    "            if timestep==0:\n",
    "                combineRelationTensorsTimeStep=combinedRelationTensor.unsqueeze(0)\n",
    "            else:\n",
    "                combineRelationTensorsTimeStep=torch.cat((combineRelationTensorsTimeStep,\\\n",
    "                                                          combinedRelationTensor.unsqueeze(0)),0)\n",
    "        if batch==0:\n",
    "            combinedRelationTensorsTimeStepAndBatch=combineRelationTensorsTimeStep.unsqueeze(0)\n",
    "        else:\n",
    "            combinedRelationTensorsTimeStepAndBatch=torch.cat((combinedRelationTensorsTimeStepAndBatch,\\\n",
    "                                                              combineRelationTensorsTimeStep.unsqueeze(0)),0)\n",
    "    return combinedRelationTensorsTimeStepAndBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the combined tensor and relation tensor i tensorsDataV2\n",
    "import math\n",
    "class tensorsDatasetV2Test(Dataset):\n",
    "    \"\"\"\n",
    "    dataset for relation model\n",
    "    \"\"\"\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=1,lableTensorEachBatch=1):\n",
    "        if(numberOfTensorsEachBatch!=1 or lableTensorEachBatch!=1):\n",
    "            raise Exception(\"BOTH TRAIN AND VALID TENSOR NUMBERS SHOULD BE ONE!\")\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        if doNormalization:\n",
    "            self.positionXMax=0\n",
    "            self.positionXMin=999999\n",
    "            self.positionYMax=0\n",
    "            self.positionYMin=99999\n",
    "            self.speedMax=-100\n",
    "            self.speedMin=999999\n",
    "            self.accMax=-100\n",
    "            self.accMin=9999\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            if doNormalization:\n",
    "                #get the max and min value for normalization\n",
    "                maxAndMinDict=getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Vel','v_Acc'])\n",
    "                #position X\n",
    "                if self.positionXMax<maxAndMinDict['Local_X']['max']:\n",
    "                    self.positionXMax=maxAndMinDict['Local_X']['max']\n",
    "                if self.positionXMin>maxAndMinDict['Local_X']['min']:\n",
    "                    self.positionXMin=maxAndMinDict['Local_X']['min']\n",
    "                #position Y\n",
    "                if self.positionYMax<maxAndMinDict['Local_Y']['max']:\n",
    "                    self.positionYMax=maxAndMinDict['Local_Y']['max']\n",
    "                if self.positionYMin>maxAndMinDict['Local_Y']['min']:\n",
    "                    self.positionYMin=maxAndMinDict['Local_Y']['min']\n",
    "                #speed\n",
    "                if self.speedMax<maxAndMinDict['v_Vel']['max']:\n",
    "                    self.speedMax=maxAndMinDict['v_Vel']['max']\n",
    "                if self.speedMin>maxAndMinDict['v_Vel']['min']:\n",
    "                    self.speedMin=maxAndMinDict['v_Vel']['min']\n",
    "                #acc\n",
    "                if self.accMax<maxAndMinDict['v_Acc']['max']:\n",
    "                    self.accMax=maxAndMinDict['v_Acc']['max']\n",
    "                if self.accMin>maxAndMinDict['v_Acc']['min']:\n",
    "                    self.accMin=maxAndMinDict['v_Acc']['min']\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def getNormalizationDict(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            a dict:{'positionXMax':self.positionXMax,'positonYMax':self.self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "        '''\n",
    "        if not doNormalization:\n",
    "            raise Exception('NORMALIZATION IS NOT APPLIED')\n",
    "        return {'positionXMax':self.positionXMax,'positionYMax':self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':self.speedMin,\\\n",
    "               'accMax':self.accMax,'accMin':self.accMin}\n",
    "    \n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the ture index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        fileName='./'+'tensorFromGetitem'+'/'+str(10000000+idx)+'.png'\n",
    "        image=visualizeTensorData(positionTensor[0,:],positionTensor[1,:])\n",
    "        cv2.imwrite(fileName,image)\n",
    "        #first frame normalization\n",
    "        if doNormalization:\n",
    "#             print('before nomalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                     torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "            speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "            accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "#             print('after normalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        else:\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        #generate relation tensor for all vehicle pairs\n",
    "        print('in getitem, combinedTensor shape: ',combinedTensor.shape)\n",
    "        fileName='./'+'tensorFromGetitemAfterNormalization'+'/'+str(10000000+idx)+'.png'\n",
    "        image=visualizeTensorData(positionTensor[0,:],positionTensor[1,:],normalizationDict=self.getNormalizationDict())\n",
    "        cv2.imwrite(fileName,image)\n",
    "        relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "        relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "        print('in getitem, relation tensorleft shape:',relationTensorLeft.shape)\n",
    "        print('in getitem, relationtensorright shape',relationTensorRight.shape)\n",
    "#         print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "        for i in range(1,combinedTensor.shape[1]):\n",
    "            relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                          combinedTensor[:,i].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "            relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                           torch.transpose(torch.cat((combinedTensor[:,:i],combinedTensor[:,i+1:]),1),0,1)),0)\n",
    "#         print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "        combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1)  \n",
    "        firstCombinedRelationTensor=combinedRelationTensor\n",
    "        firstCombinedTensor=combinedTensor\n",
    "        \n",
    "        \n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            if doNormalization:\n",
    "                positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                         torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "                speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "                accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                         speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            else:\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            #generate relation tensor for all vehicle pairs\n",
    "            relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "            relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "#             print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "            for j in range(1,combinedTensor.shape[1]):\n",
    "                relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                              combinedTensor[:,j].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "                relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                               torch.transpose(torch.cat((combinedTensor[:,:j],combinedTensor[:,j+1:]),1),0,1)),0)\n",
    "#             print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "            combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1)  \n",
    "            secondRelationTensor=combinedRelationTensor\n",
    "            secondCombinedTensor=combinedTensor\n",
    "            #since we only need two tensors, which is input and output tensor respectively, we could return\n",
    "            #the two tensors in the first loop\n",
    "            #(ok I admit that the true reason is that I am lazy)\n",
    "            return firstCombinedTensor,secondCombinedTensor\n",
    "            return firstCombinedRelationTensor,secondRelationTensor\n",
    "#             if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "#                 allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "#             elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "#                 allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "#             else:\n",
    "#                 allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "        return allCombineTensorTrain,allCombineTensorValid\n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n",
    "#run on 2080 in g814\n",
    "if runOnG814:\n",
    "    trajectoryFileList=['/home/wangyuchen/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasetV2Test=tensorsDatasetV2Test(trajectoryFileList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(datasetV2Test.getNormalizationDict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "firstCombined,secondCombined=datasetV2Test.__getitem__(40)\n",
    "print(firstCombined.shape,secondCombined.shape)\n",
    "image=visualizeTensorData(firstCombined[0,:],firstCombined[1,:],normalizationDict=datasetV2Test.getNormalizationDict())\n",
    "dirName='combinedTensorFolder'+str(int(time.time()))\n",
    "os.mkdir(dirName)\n",
    "for i in range(0,2000):\n",
    "    fileName='./'+dirName+'/'+str(10000000+i)+'.png'\n",
    "    firstCombined,secondCombined=datasetV2Test.__getitem__(i)\n",
    "    image=visualizeTensorData(firstCombined[0,:],firstCombined[1,:],normalizationDict=datasetV2Test.getNormalizationDict())\n",
    "    cv2.imwrite(fileName,image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class tensorsDatasetV3(Dataset):\n",
    "    \"\"\"\n",
    "    dataset for relation lstm model\n",
    "    \"\"\"\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=10,lableTensorEachBatch=10):\n",
    "\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        if doNormalization:\n",
    "            self.positionXMax=0\n",
    "            self.positionXMin=999999\n",
    "            self.positionYMax=0\n",
    "            self.positionYMin=99999\n",
    "            self.speedMax=-100\n",
    "            self.speedMin=999999\n",
    "            self.accMax=-100\n",
    "            self.accMin=9999\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            if doNormalization:\n",
    "                #get the max and min value for normalization\n",
    "                maxAndMinDict=getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Vel','v_Acc'])\n",
    "                #position X\n",
    "                if self.positionXMax<maxAndMinDict['Local_X']['max']:\n",
    "                    self.positionXMax=maxAndMinDict['Local_X']['max']\n",
    "                if self.positionXMin>maxAndMinDict['Local_X']['min']:\n",
    "                    self.positionXMin=maxAndMinDict['Local_X']['min']\n",
    "                #position Y\n",
    "                if self.positionYMax<maxAndMinDict['Local_Y']['max']:\n",
    "                    self.positionYMax=maxAndMinDict['Local_Y']['max']\n",
    "                if self.positionYMin>maxAndMinDict['Local_Y']['min']:\n",
    "                    self.positionYMin=maxAndMinDict['Local_Y']['min']\n",
    "                #speed\n",
    "                if self.speedMax<maxAndMinDict['v_Vel']['max']:\n",
    "                    self.speedMax=maxAndMinDict['v_Vel']['max']\n",
    "                if self.speedMin>maxAndMinDict['v_Vel']['min']:\n",
    "                    self.speedMin=maxAndMinDict['v_Vel']['min']\n",
    "                #acc\n",
    "                if self.accMax<maxAndMinDict['v_Acc']['max']:\n",
    "                    self.accMax=maxAndMinDict['v_Acc']['max']\n",
    "                if self.accMin>maxAndMinDict['v_Acc']['min']:\n",
    "                    self.accMin=maxAndMinDict['v_Acc']['min']\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def getNormalizationDict(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            a dict:{'positionXMax':self.positionXMax,'positonYMax':self.self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "        '''\n",
    "        if not doNormalization:\n",
    "            raise Exception('NORMALIZATION IS NOT APPLIED')\n",
    "        return {'positionXMax':self.positionXMax,'positionYMax':self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':self.speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "    \n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the true index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        #first frame normalization\n",
    "        if doNormalization:\n",
    "#             print('before nomalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                     torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "            speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "            accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "#             print('after normalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        else:\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            if doNormalization:\n",
    "                positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                         torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "                speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "                accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                         speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            else:\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "            elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "            else:\n",
    "                allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "        return allCombineTensorTrain,allCombineTensorValid\n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n",
    "#run on 2080 in g814\n",
    "if runOnG814:\n",
    "    trajectoryFileList=['/home/wangyuchen/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class tensorsDatasetV4(Dataset):\n",
    "    \"\"\"\n",
    "    dataset for difference series type\n",
    "    Returns:\n",
    "    INPUTS:\n",
    "    relationTensors,discountParameterTensors,\n",
    "    OUTPUTS:\n",
    "    allCombineTensorTrain, allCombineTensorValid,\\\n",
    "        combinedRelationTensors, combinedDiscountParameterTensors,differenceLabels\n",
    "    \"\"\"\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=10,lableTensorEachBatch=10,\\\n",
    "                maxRelationNumber=20,givenRange=0.08):\n",
    "\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        self.maxRelationNumber=maxRelationNumber\n",
    "        self.givenRange=givenRange\n",
    "        if doNormalization:\n",
    "            self.positionXMax=0\n",
    "            self.positionXMin=999999\n",
    "            self.positionYMax=0\n",
    "            self.positionYMin=99999\n",
    "            self.speedMax=-100\n",
    "            self.speedMin=999999\n",
    "            self.accMax=-100\n",
    "            self.accMin=9999\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            if doNormalization:\n",
    "                #get the max and min value for normalization\n",
    "                maxAndMinDict=getMaxAndMinValueFromValueDict(valueDict,['Local_X','Local_Y','v_Vel','v_Acc'])\n",
    "                #position X\n",
    "                if self.positionXMax<maxAndMinDict['Local_X']['max']:\n",
    "                    self.positionXMax=maxAndMinDict['Local_X']['max']\n",
    "                if self.positionXMin>maxAndMinDict['Local_X']['min']:\n",
    "                    self.positionXMin=maxAndMinDict['Local_X']['min']\n",
    "                #position Y\n",
    "                if self.positionYMax<maxAndMinDict['Local_Y']['max']:\n",
    "                    self.positionYMax=maxAndMinDict['Local_Y']['max']\n",
    "                if self.positionYMin>maxAndMinDict['Local_Y']['min']:\n",
    "                    self.positionYMin=maxAndMinDict['Local_Y']['min']\n",
    "                #speed\n",
    "                if self.speedMax<maxAndMinDict['v_Vel']['max']:\n",
    "                    self.speedMax=maxAndMinDict['v_Vel']['max']\n",
    "                if self.speedMin>maxAndMinDict['v_Vel']['min']:\n",
    "                    self.speedMin=maxAndMinDict['v_Vel']['min']\n",
    "                #acc\n",
    "                if self.accMax<maxAndMinDict['v_Acc']['max']:\n",
    "                    self.accMax=maxAndMinDict['v_Acc']['max']\n",
    "                if self.accMin>maxAndMinDict['v_Acc']['min']:\n",
    "                    self.accMin=maxAndMinDict['v_Acc']['min']\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def getNormalizationDict(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            a dict:{'positionXMax':self.positionXMax,'positonYMax':self.self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "        '''\n",
    "        if not doNormalization:\n",
    "            raise Exception('NORMALIZATION IS NOT APPLIED')\n",
    "        return {'positionXMax':self.positionXMax,'positionYMax':self.positionYMax,\\\n",
    "               'positionXMin':self.positionXMin,'positionYMin':self.positionYMin,\\\n",
    "               'speedMax':self.speedMax,'speedMin':self.speedMin,\\\n",
    "               'accMax':self.accMax,'accMax':self.accMin}\n",
    "    \n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the true index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        #first frame normalization\n",
    "        if doNormalization:\n",
    "#             print('before nomalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                     torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "            speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "            accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "#             print('after normalization')\n",
    "#             print(positionTensor.shape,speedTensor.shape,accTensor.shape)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        else:\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            #generate tensor from general frame\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            if doNormalization:\n",
    "                positionTensor=torch.cat((torch.div(torch.sub(positionTensor[0,:],self.positionXMin),self.positionXMax-self.positionXMin).unsqueeze(0),\\\n",
    "                                         torch.div(torch.sub(positionTensor[1,:],self.positionYMin,),self.positionYMax-self.positionYMin).unsqueeze(0)),0)\n",
    "                speedTensor=torch.div(torch.sub(speedTensor,self.speedMin),self.speedMax-self.speedMin)\n",
    "                accTensor=torch.div(torch.sub(accTensor,self.accMin),self.accMax-self.accMin)\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                         speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            else:\n",
    "                combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                         accTensor.mul(angleCosTensor)),0)\n",
    "            if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "            elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "            else:\n",
    "                allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "#         return allCombineTensorTrain,allCombineTensorValid\n",
    "        #only consist of position tensor, ignore speed tensor and accelerate tensor\n",
    "        allCombineTensorTrain=allCombineTensorTrain[:,0:2,:]\n",
    "        allCombineTensorValid=allCombineTensorValid[:,0:2,:]\n",
    "        logging.debug(fromAllToStr('allCombineTensorTrain.shape:',allCombineTensorTrain.shape))\n",
    "        logging.debug(fromAllToStr('allCombineTensorValid.shape',allCombineTensorValid))\n",
    "        combinedRelationTensors,combinedDiscountParameterTensors=\\\n",
    "        computeRelationAndAllTheOtherTensorsWithDistance(\\\n",
    "        allCombineTensorTrain,theGivenRange=self.givenRange,\\\n",
    "        maxRelationsNumber=self.maxRelationNumber)\n",
    "        #permute the dimension order of the valid tensor\n",
    "        differenceLabels=differenceBetweenTwoFrameForTimeSteps(allCombineTensorValid.permute(0,2,1))\n",
    "        logging.debug(fromAllToStr(\"differenceLabels.shape:\",differenceLabels.shape))\n",
    "        return allCombineTensorTrain, allCombineTensorValid,\\\n",
    "        combinedRelationTensors, combinedDiscountParameterTensors,differenceLabels\n",
    "        \n",
    "        \n",
    "            \n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n",
    "#run on 2080 in g814\n",
    "if runOnG814:\n",
    "    trajectoryFileList=['/home/wangyuchen/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasetV2=tensorsDatasetV2(trajectoryFileList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "dataIter=iter(datasetV2)\n",
    "first,second=dataIter.__next__()\n",
    "print(first.shape, second.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxMatrixIndex=250\n",
    "dataloaderV2=DataLoader(datasetV2,batch_size=4,shuffle=True)\n",
    "for i,item in enumerate(dataloaderV2):\n",
    "    if(i>0):\n",
    "        break\n",
    "    print(i)\n",
    "    first,second=item\n",
    "    print(first.shape,second.shape)\n",
    "    print(first[0,:5,:6])\n",
    "    print(first[0,(2,245,246,247,248,249,250,251),6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def generateAdjacencyMatrix(batchedPositionTensor,lambdaX,lambdaY,omegaX,omegaY,m):\n",
    "    \"\"\"\n",
    "    Using batched position tensor generate batched adjacency matrix\n",
    "    Args:\n",
    "        batchedPositionTensor: a batch of position tensor, which size in (batch, timeSequence,2,vehicles), the \n",
    "        value 2 in dim=2 is the position of x and y. \n",
    "        lambda1,lambda2,omega1,omega2,m are parameters of the function. m<1\n",
    "        see detail in my notebook\n",
    "    Returns:\n",
    "        a batch of adjacency matrix\n",
    "    Example:\n",
    "        if given a batch of combined tensor, named theTensor, which size as below:\n",
    "            (4,100,6,250)\n",
    "        which means 4 batches, 100 time step, 6 dimension which respectively of positonx, positony, velocityx, \n",
    "        velocityy, accx,accy.\n",
    "        then we apply the function in such way:\n",
    "        generateAdjacencyMatrix(theTensor(:,:,0:1,:))\n",
    "    \"\"\"\n",
    "    print(batchedPositionTensor.size())\n",
    "    sizeOfEachMatrix=batchedPositionTensor[0,0,0,:].size()[0]\n",
    "    print(sizeOfEachMatrix)\n",
    "    for batchI in range(batchedPositionTensor.size()[0]): #revolve each batch\n",
    "#         print('batchI',batchI)\n",
    "        timeStepsMatrixList=[]\n",
    "        for timeStepI in range(batchedPositionTensor.size()[1]):#revolve each time step\n",
    "#             print('timeStepI:',timeStepI)\n",
    "#             adjacencyMatrix=np.zeros((sizeOfEachMatrix,sizeOfEachMatrix))\n",
    "            adjacencyList=[]\n",
    "            tempPositionList=batchedPositionTensor[batchI,timeStepI,:,:].numpy().tolist()\n",
    "#             start=time.time()\n",
    "            for i in range(sizeOfEachMatrix):\n",
    "                tempLineList=[]\n",
    "                for j in range(sizeOfEachMatrix):\n",
    "#                     adjacencyMatrix[i,j]=1\n",
    "                    if (tempPositionList[1][i]*tempPositionList[1][j]==0):\n",
    "                        toZero=0\n",
    "                    else:\n",
    "                        toZero=1\n",
    "                        \n",
    "                    #calculate original element with linear function\n",
    "#                     tempLineList.append((1-abs(tempPositionList[1][i]-tempPositionList[1][j]))*\\\n",
    "#                         (1-abs(tempPositionList[0][i]-tempPositionList[0][j]))*toZero)\n",
    "                    \n",
    "                    #calculate original element with exponential function\n",
    "                    element=(omegaY/(math.exp(lambdaY*(abs(tempPositionList[1][j]-tempPositionList[1][i])))))*\\\n",
    "                    (omegaX/(math.exp(lambdaX*(abs(tempPositionList[0][j]-tempPositionList[0][i])))))*toZero\n",
    "                    tempLineList.append(element)\n",
    "#                     adjacencyMatrix[i,j]=(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])*\\\n",
    "#                         (batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])\n",
    "#                     (omegaY/math.exp(lambdaX*abs(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])))*\\\n",
    "#                     (omegaX/math.exp(lambdaY*abs(batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])))\n",
    "                    \n",
    "                    #calculate original element with expenential\n",
    "#                     adjacencyMatrix[i,j]=\n",
    "#                     (omegaY/math.exp(lambdaX*abs(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])))*\\\n",
    "#                     (omegaX/math.exp(lambdaY*abs(batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])))\n",
    "                    if(tempPositionList[1][j]-tempPositionList[1][i]<0):\n",
    "                        #if i follows j, then multiple m, m<1\n",
    "                        tempLineList[j]=tempLineList[j]*m\n",
    "                adjacencyList.append(tempLineList)\n",
    "            \n",
    "#             end=time.time()\n",
    "#             print(end-start)\n",
    "            adjacencyMatrix=torch.tensor(adjacencyList).unsqueeze(0)\n",
    "            if timeStepI==0:\n",
    "                matrixSequenceInTimeStepDim=adjacencyMatrix\n",
    "            else:\n",
    "                matrixSequenceInTimeStepDim=\\\n",
    "                torch.cat((matrixSequenceInTimeStepDim,adjacencyMatrix),0)\n",
    "        matrixSequenceInTimeStepDim=matrixSequenceInTimeStepDim.unsqueeze(0)\n",
    "        if batchI==0:\n",
    "            matrixSequenceInBatchDim=matrixSequenceInTimeStepDim\n",
    "        else:\n",
    "            matrixSequenceInBatchDim=torch.cat((matrixSequenceInBatchDim,matrixSequenceInTimeStepDim),0)            \n",
    "    return matrixSequenceInBatchDim\n",
    "\n",
    "def tensorNormalization(inputTensor,minValue,maxValue):\n",
    "    inputTensor.div_(maxValue)\n",
    "    \n",
    "def batchNormalizationForCombinedTensor(inputBatchedTensor,minX,maxX,minY,maxY,minV,maxV,minA,maxA):\n",
    "    tensorNormalization(inputBatchedTensor[:,:,0,:],minX,maxX)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,1,:],minY,maxY)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,2:4,:],minV,maxV)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,4:6,:],minA,maxA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    __init__(self, input_size, cell_size, hidden_size, output_last = True)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, cell_size, hidden_size, output_last = True):\n",
    "        \"\"\"\n",
    "        cell_size is the size of cell_state.\n",
    "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, input, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((input, Hidden_State), 1)\n",
    "        f = F.sigmoid(self.fl(combined))\n",
    "        i = F.sigmoid(self.il(combined))\n",
    "        o = F.sigmoid(self.ol(combined))\n",
    "        C = F.tanh(self.Cl(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        time_step = inputs.size(1)\n",
    "        Hidden_State, Cell_State = self.initHidden(batch_size)\n",
    "        \n",
    "        if self.output_last:\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:]), Hidden_State, Cell_State)  \n",
    "            return Hidden_State\n",
    "        else:\n",
    "            outputs = None\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:]), Hidden_State, Cell_State)  \n",
    "                if outputs is None:\n",
    "                    outputs = Hidden_State.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "            return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        if use_gpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            return Hidden_State, Cell_State\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            return Hidden_State, Cell_State\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# maxMatrixIndex=250\n",
    "# trajectorDataSet=tensorsDataset(trajectoryFileList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataLoader=DataLoader(trajectorDataSet,batch_size=1,shuffle=True,num_workers=4)\n",
    "# for i,data in enumerate(dataLoader):\n",
    "#     print('11111')\n",
    "#     if(i>10):\n",
    "#         break\n",
    "#     print(data[0].shape)\n",
    "#     print(data[0][0,:,1,1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code fragment below is used to visualize tensor data\n",
    "# dataLoader=DataLoader(trajectorDataSet,batch_size=1,shuffle=True,num_workers=4)\n",
    "# for dataI,data in enumerate(dataLoader):\n",
    "#     if(dataI>10):\n",
    "#         break\n",
    "#     for i in range(int(data[0][0,:,0,0].shape[0])):\n",
    "#         tensorImage=visualizeTensorData(data[0][0,i,0,:],data[0][0,i,1,:],2500,100,10) \n",
    "#         fileName=str(100000+dataI)+'_'+str(100000+i)+'.png'\n",
    "#         cv2.imwrite('./tensorVisualizeFolder/'+fileName,tensorImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process objects to generate relation tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30,size1=10,size2=10,size3=10,size4=10):\n",
    "        super(relationNetwork,self).__init__()\n",
    "        self.size1=size1\n",
    "        self.size2=size2\n",
    "        self.size3=size3\n",
    "        self.size4=size4\n",
    "        self.layer1=nn.Linear(inputSize,size1)\n",
    "        self.layer2=nn.Linear(size1,size2)\n",
    "        self.layer3=nn.Linear(size2,size3)\n",
    "        self.layer4=nn.Linear(size3,size4)\n",
    "        self.layer5=nn.Linear(size4,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.ReLU(self.layer5(x4))\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class effectNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process relationTensor to generate effect tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30,size1=10,size2=10,size3=10,size4=10):\n",
    "        super(effectNetwork,self).__init__()\n",
    "        self.size1=size1\n",
    "        self.size2=size2\n",
    "        self.size3=size3\n",
    "        self.size4=size4\n",
    "        self.layer1=nn.Linear(inputSize,size1)\n",
    "        self.layer2=nn.Linear(size1,size2)\n",
    "        self.layer3=nn.Linear(size2,size3)\n",
    "        self.layer4=nn.Linear(size3,size4)\n",
    "        self.layer5=nn.Linear(size4,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.ReLU(self.layer5(x4))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class effectCombinationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process effect tensor to generate combined effect tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30,size1=10,size2=10,size3=10,size4=10):\n",
    "        super(effectCombinationNetwork,self).__init__()\n",
    "        self.size1=size1\n",
    "        self.size2=size2\n",
    "        self.size3=size3\n",
    "        self.size4=size4\n",
    "        self.layer1=nn.Linear(inputSize,size1)\n",
    "        self.layer2=nn.Linear(size1,size2)\n",
    "        self.layer3=nn.Linear(size2,size3)\n",
    "        self.layer4=nn.Linear(size3,size4)\n",
    "        self.layer5=nn.Linear(size4,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.ReLU(self.layer5(x4))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectModifyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    modify object with effect tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30,size1=10,size2=10,size3=10,size4=10):\n",
    "        super(objectModifyNetwork,self).__init__()\n",
    "        self.size1=size1\n",
    "        self.size2=size2\n",
    "        self.size3=size3\n",
    "        self.size4=size4\n",
    "        self.layer1=nn.Linear(inputSize,size1)\n",
    "        self.layer2=nn.Linear(size1,size2)\n",
    "        self.layer3=nn.Linear(size2,size3)\n",
    "        self.layer4=nn.Linear(size3,size4)\n",
    "        self.layer5=nn.Linear(size4,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.layer5(x4)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hiddenStateToEffectNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process hidden state tensor to generate combined effect tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30,size1=10,size2=10,size3=10,size4=10):\n",
    "        super(hiddenStateToEffectNetwork,self).__init__()\n",
    "        self.size1=size1\n",
    "        self.size2=size2\n",
    "        self.size3=size3\n",
    "        self.size4=size4\n",
    "        self.layer1=nn.Linear(inputSize,size1)\n",
    "        self.layer2=nn.Linear(size1,size2)\n",
    "        self.layer3=nn.Linear(size2,size3)\n",
    "        self.layer4=nn.Linear(size3,size4)\n",
    "        self.layer5=nn.Linear(size4,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.ReLU(self.layer5(x4))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hiddenStateToDifferenceNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    process hidden state tensor to generate difference tensors\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize=4,outputSize=30,size1=10,size2=10,size3=10,size4=10):\n",
    "        super(hiddenStateToDifferenceNetwork,self).__init__()\n",
    "        self.size1=size1\n",
    "        self.size2=size2\n",
    "        self.size3=size3\n",
    "        self.size4=size4\n",
    "        self.layer1=nn.Linear(inputSize,size1)\n",
    "        self.layer2=nn.Linear(size1,size2)\n",
    "        self.layer3=nn.Linear(size2,size3)\n",
    "        self.layer4=nn.Linear(size3,size4)\n",
    "        self.layer5=nn.Linear(size4,outputSize)\n",
    "        self.ReLU=nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1=self.ReLU(self.layer1(inputs))\n",
    "        x2=self.ReLU(self.layer2(x1))\n",
    "        x3=self.ReLU(self.layer3(x2))\n",
    "        x4=self.ReLU(self.layer4(x3))\n",
    "        outputs=self.ReLU(self.layer5(x4))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#this block build for relation network testing\n",
    "#delete later if needed\n",
    "relationTensorSize=40\n",
    "positionTuple=(0,1,6,7)\n",
    "velocityTuple=(2,3,8,9)\n",
    "acclerateTuple=(4,5,10,11)\n",
    "positionRelationNet=relationNetwork(outputSize=relationTensorSize)\n",
    "velocityRelationNet=relationNetwork(outputSize=relationTensorSize)\n",
    "accelerateRelationNet=relationNetwork(outputSize=relationTensorSize)\n",
    "\n",
    "maxMatrixIndex=250\n",
    "\n",
    "#load test data in network testing block\n",
    "dataloaderV2=DataLoader(datasetV2,batch_size=1,shuffle=True)\n",
    "for i,item in enumerate(dataloaderV2):\n",
    "    if(i>0):\n",
    "        break\n",
    "    print(i)\n",
    "    first,second=item\n",
    "#     print(first.shape,second.shape)\n",
    "#     print(first[0,:5,:6])\n",
    "#     print(first[0,(2,245,246,247,248,249,250,251),6:])\n",
    "    #from frame to position, velocity, accelerate\n",
    "    positionRelationTensors=positionRelationNet(first[:,:,positionTuple])\n",
    "    velocityRelationTensors=velocityRelationNet(first[:,:,velocityTuple])\n",
    "    accelerateRelationTensors=accelerateRelationNet(first[:,:,acclerateTuple])\n",
    "    print(positionRelationTensors.shape)\n",
    "    objectsAndRelationTensors=torch.cat((first[:,:,positionTuple],positionRelationTensors),2)\n",
    "    print(objectsAndRelationTensors.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "objectAndTensorSize=relationTensorSize+4 \n",
    "effectOutputTensorSize=20\n",
    "#the number 4 is is the size of positon(or velocity or accelerate) pairs,\n",
    "#such as (positonxObject1,positonyObject1,positionxObject2,positionyObject2)\n",
    "positionEffectNet=effectNetwork(inputSize=objectAndTensorSize,outputSize=effectOutputTensorSize)\n",
    "velocityEffectNet=effectNetwork(inputSize=objectAndTensorSize,outputSize=effectOutputTensorSize)\n",
    "accelerateEffectNet=effectNetwork(inputSize=objectAndTensorSize,outputSize=effectOutputTensorSize)\n",
    "first,second=next(iter(dataloaderV2))\n",
    "print(first.shape)\n",
    "\n",
    "#relation computation\n",
    "positionRelationTensors=positionRelationNet(first[:,:,positionTuple])\n",
    "velocityRelationTensors=velocityRelationNet(first[:,:,velocityTuple])\n",
    "accelerateRelationTensors=accelerateRelationNet(first[:,:,acclerateTuple])\n",
    "                                                      \n",
    "objectsAndPositionRelationTensors=torch.cat((first[:,:,positionTuple],positionRelationTensors),2)\n",
    "objectsAndVelocityRelationTensors=torch.cat((first[:,:,positionTuple],velocityRelationTensors),2)\n",
    "objectsAndAccelerateRelationTensors=torch.cat((first[:,:,positionTuple],accelerateRelationTensors),2)\n",
    "\n",
    "#effect computation\n",
    "positionEffectTensors=positionEffectNet(objectsAndPositionRelationTensors)\n",
    "velocityEffectTensors=velocityEffectNet(objectsAndVelocityRelationTensors)\n",
    "accelerateEffectTensors=accelerateEffectNet(objectsAndAccelerateRelationTensors)\n",
    "\n",
    "#effect combination type 1\n",
    "#tensor summation\n",
    "batchSize=positionRelationTensors.shape[0]\n",
    "positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "for i in range(maxMatrixIndex):\n",
    "    positionEffectSummation[:,i,:]=torch.sum(positionEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "    velocityEffectSummation[:,i,:]=torch.sum(velocityEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "    accelerateEffectSummation[:,i,:]=torch.sum(accelerateEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "    \n",
    "#effect combination net initial\n",
    "#combined tensors length\n",
    "combinedTensorSize=20\n",
    "effectCombinationNet=effectCombinationNetwork(inputSize=effectOutputTensorSize*3,outputSize=combinedTensorSize)\n",
    "\n",
    "#combine tensors and process the combined one\n",
    "combinedEffectTensors=torch.cat((positionEffectSummation,velocityEffectSummation,accelerateEffectSummation),2)\n",
    "print(combinedEffectTensors.shape)\n",
    "processedCombinedEffectTensors=effectCombinationNet(combinedEffectTensors)\n",
    "print(processedCombinedEffectTensors.shape)\n",
    "\n",
    "#generate a tuple in which each element is the index of a vehicle\n",
    "#the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "#this part has been put into init function of Module class\n",
    "listForEachVehicle=[]\n",
    "for i in range(maxMatrixIndex):\n",
    "    listForEachVehicle.append(i*(maxMatrixIndex-1))\n",
    "tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "\n",
    "#the property of each vehicle\n",
    "vehicleProperty=first[:,tupleForEachVehicle,0:6]\n",
    "\n",
    "objectAndFinalEffect=torch.cat((vehicleProperty,processedCombinedEffectTensors),2)\n",
    "print(objectAndFinalEffect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#initialize the final network to generate new objects properties\n",
    "objectModifyNet=objectModifyNetwork(inputSize=combinedTensorSize+6,outputSize=6)\n",
    "finalObjectState=objectModifyNet(objectAndFinalEffect)\n",
    "print(finalObjectState.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from relation to new object network\n",
    "class fromRelationToObjectNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(fromRelationToObjectNetwork,self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        \n",
    "        #position relation network initialize\n",
    "        self.relationTensorSize=40\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "        self.velocityTuple=(2,3,8,9)\n",
    "        self.acclerateTuple=(4,5,10,11)\n",
    "        self.positionRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        self.velocityRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        self.accelerateRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        \n",
    "        #effect network initialize\n",
    "        self.objectAndTensorSize=self.relationTensorSize+4 \n",
    "        self.effectOutputTensorSize=20\n",
    "        #the number 4 is is the size of positon(or velocity or accelerate) pairs,\n",
    "        #such as (positonxObject1,positonyObject1,positionxObject2,positionyObject2)\n",
    "        self.positionEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        self.velocityEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        self.accelerateEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        \n",
    "        #effect combination net initialize\n",
    "        #combined tensors length\n",
    "        self.combinedTensorSize=20\n",
    "        self.effectCombinationNet=\\\n",
    "        effectCombinationNetwork(inputSize=self.effectOutputTensorSize*3,outputSize=self.combinedTensorSize)\n",
    "        \n",
    "        #initialize the final network to generate new objects properties\n",
    "        self.objectModifyNet=objectModifyNetwork(inputSize=self.combinedTensorSize+6,outputSize=6)\n",
    "        \n",
    "    def forward(self,inputObjectsPairs):\n",
    "        #relation computation\n",
    "        positionRelationTensors=self.positionRelationNet(inputObjectsPairs[:,:,self.positionTuple])\n",
    "        velocityRelationTensors=self.velocityRelationNet(inputObjectsPairs[:,:,self.velocityTuple])\n",
    "        accelerateRelationTensors=self.accelerateRelationNet(inputObjectsPairs[:,:,self.acclerateTuple])\n",
    "\n",
    "        objectsAndPositionRelationTensors=torch.cat((inputObjectsPairs[:,:,self.positionTuple],positionRelationTensors),2)\n",
    "        objectsAndVelocityRelationTensors=torch.cat((inputObjectsPairs[:,:,self.velocityTuple],velocityRelationTensors),2)\n",
    "        objectsAndAccelerateRelationTensors=torch.cat((inputObjectsPairs[:,:,self.acclerateTuple],accelerateRelationTensors),2)\n",
    "\n",
    "        #effect computation\n",
    "        positionEffectTensors=self.positionEffectNet(objectsAndPositionRelationTensors)\n",
    "        velocityEffectTensors=self.velocityEffectNet(objectsAndVelocityRelationTensors)\n",
    "        accelerateEffectTensors=self.accelerateEffectNet(objectsAndAccelerateRelationTensors)\n",
    "\n",
    "        \n",
    "        #effect combination type 1\n",
    "        #tensor summation\n",
    "        batchSize=positionRelationTensors.shape[0]\n",
    "        if useGpu==True:\n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "            velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "            accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "        else: \n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "            velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "            accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "        for i in range(maxMatrixIndex):\n",
    "            positionEffectSummation[:,i,:]=torch.sum(positionEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "            velocityEffectSummation[:,i,:]=torch.sum(velocityEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "            accelerateEffectSummation[:,i,:]=torch.sum(accelerateEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "        combinedEffectTensors=torch.cat((positionEffectSummation,velocityEffectSummation,accelerateEffectSummation),2)\n",
    "        processedCombinedEffectTensors=self.effectCombinationNet(combinedEffectTensors)\n",
    "        \n",
    "        #the property of each vehicle\n",
    "        vehicleProperty=inputObjectsPairs[:,self.tupleForEachVehicle,0:6]\n",
    "        objectAndFinalEffect=torch.cat((vehicleProperty,processedCombinedEffectTensors),2)\n",
    "        \n",
    "        #compute final state\n",
    "        finalObjectState=self.objectModifyNet(objectAndFinalEffect)\n",
    "        return finalObjectState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This module is supposed to be placed in LSTM model\n",
    "class fromRelationToEffectNetwork(nn.Module):\n",
    "    def __init__(self,effectOutputTensorSize=20):\n",
    "        super(fromRelationToEffectNetwork,self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        \n",
    "        #position relation network initialize\n",
    "        self.relationTensorSize=40\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "        self.velocityTuple=(2,3,8,9)\n",
    "        self.acclerateTuple=(4,5,10,11)\n",
    "        self.positionRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        self.velocityRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        self.accelerateRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        \n",
    "        #effect network initialize\n",
    "        self.objectAndTensorSize=self.relationTensorSize+4 \n",
    "        self.effectOutputTensorSize=effectOutputTensorSize\n",
    "        #the number 4 is the size of positon(or velocity or accelerate) pairs,\n",
    "        #such as (positonxObject1,positonyObject1,positionxObject2,positionyObject2)\n",
    "        self.positionEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        self.velocityEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        self.accelerateEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        \n",
    "        #effect combination net initialize\n",
    "        #combined tensors length\n",
    "        self.combinedTensorSize=20\n",
    "        self.effectCombinationNet=\\\n",
    "        effectCombinationNetwork(inputSize=self.effectOutputTensorSize*3,outputSize=self.combinedTensorSize)\n",
    "        \n",
    "        #remove objectModifyNet in objectToEffectModel\n",
    "        #initialize the final network to generate new objects properties\n",
    "#         self.objectModifyNet=objectModifyNetwork(inputSize=self.combinedTensorSize+6,outputSize=6)\n",
    "        \n",
    "    def forward(self,inputObjectsPairs):\n",
    "        #relation computation\n",
    "        effectOutputTensorSize=self.effectOutputTensorSize\n",
    "        positionRelationTensors=self.positionRelationNet(inputObjectsPairs[:,:,self.positionTuple])\n",
    "        velocityRelationTensors=self.velocityRelationNet(inputObjectsPairs[:,:,self.velocityTuple])\n",
    "        accelerateRelationTensors=self.accelerateRelationNet(inputObjectsPairs[:,:,self.acclerateTuple])\n",
    "\n",
    "        objectsAndPositionRelationTensors=torch.cat((inputObjectsPairs[:,:,self.positionTuple],positionRelationTensors),2)\n",
    "        objectsAndVelocityRelationTensors=torch.cat((inputObjectsPairs[:,:,self.velocityTuple],velocityRelationTensors),2)\n",
    "        objectsAndAccelerateRelationTensors=torch.cat((inputObjectsPairs[:,:,self.acclerateTuple],accelerateRelationTensors),2)\n",
    "\n",
    "        #effect computation\n",
    "        positionEffectTensors=self.positionEffectNet(objectsAndPositionRelationTensors)\n",
    "        velocityEffectTensors=self.velocityEffectNet(objectsAndVelocityRelationTensors)\n",
    "        accelerateEffectTensors=self.accelerateEffectNet(objectsAndAccelerateRelationTensors)\n",
    "\n",
    "        \n",
    "        #effect combination type 1\n",
    "        #tensor summation\n",
    "        batchSize=positionRelationTensors.shape[0]\n",
    "        if useGpu==True:\n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "            velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "            accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "        else: \n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "            velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "            accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "        for i in range(maxMatrixIndex):\n",
    "            positionEffectSummation[:,i,:]=torch.sum(positionEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "            velocityEffectSummation[:,i,:]=torch.sum(velocityEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "            accelerateEffectSummation[:,i,:]=torch.sum(accelerateEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "        combinedEffectTensors=torch.cat((positionEffectSummation,velocityEffectSummation,accelerateEffectSummation),2)\n",
    "        processedCombinedEffectTensors=self.effectCombinationNet(combinedEffectTensors)\n",
    "        \n",
    "        #remove object extraction component and computation component\n",
    "#         #the property of each vehicle\n",
    "#         vehicleProperty=inputObjectsPairs[:,self.tupleForEachVehicle,0:6]\n",
    "#         objectAndFinalEffect=torch.cat((vehicleProperty,processedCombinedEffectTensors),2)\n",
    "        \n",
    "#         #compute final state\n",
    "#         finalObjectState=self.objectModifyNet(objectAndFinalEffect)\n",
    "        return processedCombinedEffectTensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This module is supposed to be placed in LSTM model\n",
    "class fromRelationToEffectNetworkPositionOnly(nn.Module):\n",
    "    def __init__(self,effectOutputTensorSize=20):\n",
    "        super(fromRelationToEffectNetworkPositionOnly,self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        \n",
    "        #position relation network initialize\n",
    "        self.relationTensorSize=40\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "#         self.velocityTuple=(2,3,8,9)\n",
    "#         self.acclerateTuple=(4,5,10,11)\n",
    "        self.positionRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "#         self.velocityRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "#         self.accelerateRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "        \n",
    "        #effect network initialize\n",
    "        self.objectAndTensorSize=self.relationTensorSize+4 \n",
    "        self.effectOutputTensorSize=effectOutputTensorSize\n",
    "        #the number 4 is the size of positon(or velocity or accelerate) pairs,\n",
    "        #such as (positonxObject1,positonyObject1,positionxObject2,positionyObject2)\n",
    "        self.positionEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "#         self.velocityEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "#         self.accelerateEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "        \n",
    "        #effect combination net initialize\n",
    "        #combined tensors length\n",
    "#         self.combinedTensorSize=20\n",
    "#         self.effectCombinationNet=\\\n",
    "#         effectCombinationNetwork(inputSize=self.effectOutputTensorSize*3,outputSize=self.combinedTensorSize)\n",
    "        \n",
    "        #remove objectModifyNet in objectToEffectModel\n",
    "        #initialize the final network to generate new objects properties\n",
    "#         self.objectModifyNet=objectModifyNetwork(inputSize=self.combinedTensorSize+6,outputSize=6)\n",
    "        \n",
    "    def forward(self,inputObjectsPairs):\n",
    "        #relation computation\n",
    "        effectOutputTensorSize=self.effectOutputTensorSize\n",
    "        positionRelationTensors=self.positionRelationNet(inputObjectsPairs[:,:,self.positionTuple])\n",
    "#         velocityRelationTensors=self.velocityRelationNet(inputObjectsPairs[:,:,self.velocityTuple])\n",
    "#         accelerateRelationTensors=self.accelerateRelationNet(inputObjectsPairs[:,:,self.acclerateTuple])\n",
    "\n",
    "        objectsAndPositionRelationTensors=torch.cat((inputObjectsPairs[:,:,self.positionTuple],positionRelationTensors),2)\n",
    "#         objectsAndVelocityRelationTensors=torch.cat((inputObjectsPairs[:,:,self.velocityTuple],velocityRelationTensors),2)\n",
    "#         objectsAndAccelerateRelationTensors=torch.cat((inputObjectsPairs[:,:,self.acclerateTuple],accelerateRelationTensors),2)\n",
    "\n",
    "        #effect computation\n",
    "        positionEffectTensors=self.positionEffectNet(objectsAndPositionRelationTensors)\n",
    "#         velocityEffectTensors=self.velocityEffectNet(objectsAndVelocityRelationTensors)\n",
    "#         accelerateEffectTensors=self.accelerateEffectNet(objectsAndAccelerateRelationTensors)\n",
    "\n",
    "        \n",
    "        #effect combination type 1\n",
    "        #tensor summation\n",
    "        batchSize=positionRelationTensors.shape[0]\n",
    "        if useGpu==True:\n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "#             velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "#             accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "        else: \n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "#             velocityEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "#             accelerateEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "        for i in range(maxMatrixIndex):\n",
    "            positionEffectSummation[:,i,:]=torch.sum(positionEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "#             velocityEffectSummation[:,i,:]=torch.sum(velocityEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "#             accelerateEffectSummation[:,i,:]=torch.sum(accelerateEffectTensors[:,(i*249):((i+1)*249),:],1)\n",
    "#         combinedEffectTensors=torch.cat((positionEffectSummation,velocityEffectSummation,accelerateEffectSummation),2)\n",
    "#         processedCombinedEffectTensors=self.effectCombinationNet(combinedEffectTensors)\n",
    "        \n",
    "        #remove object extraction component and computation component\n",
    "#         #the property of each vehicle\n",
    "#         vehicleProperty=inputObjectsPairs[:,self.tupleForEachVehicle,0:6]\n",
    "#         objectAndFinalEffect=torch.cat((vehicleProperty,processedCombinedEffectTensors),2)\n",
    "        \n",
    "#         #compute final state\n",
    "#         finalObjectState=self.objectModifyNet(objectAndFinalEffect)\n",
    "        return positionEffectSummation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationLSTM(nn.Module):\n",
    "    def __init__(self, input_size=20, cell_size=20, hidden_size=20, output_last = True):\n",
    "        \"\"\"\n",
    "        cell_size is the size of cell_state.\n",
    "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
    "        the input of LSTM  structure was the output of 'fromRelationToEffectNet' module, so that \n",
    "        the effectOutputTensorSize has the same number as input_size. \n",
    "        \n",
    "        \"\"\"\n",
    "        super(RelationLSTM, self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        \n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "        self.velocityTuple=(2,3,8,9)\n",
    "        self.acclerateTuple=(4,5,10,11)\n",
    "        \n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.fromRelationToEffectNet=fromRelationToEffectNetwork(effectOutputTensorSize=input_size)\n",
    "        self.objectModifyNet=objectModifyNetwork(inputSize=hidden_size+6,outputSize=6)\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, inputEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputEffectTensor, Hidden_State), 2)\n",
    "        print('in step,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl(combined))\n",
    "        i = F.sigmoid(self.il(combined))\n",
    "        o = F.sigmoid(self.ol(combined))\n",
    "        C = F.tanh(self.Cl(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        time_step = inputs.size(1)\n",
    "        Hidden_State, Cell_State = self.initHidden(batch_size)\n",
    "        \n",
    "        if self.output_last:\n",
    "            for i in range(time_step):\n",
    "                effects=self.fromRelationToEffectNet(inputs[:,i,:,:])\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(effects), Hidden_State, Cell_State) \n",
    "                #the property of each vehicle\n",
    "            print(inputs.shape)\n",
    "            vehicleProperty=inputs[:,-1,self.tupleForEachVehicle,0:6].squeeze()\n",
    "            objectAndFinalEffect=torch.cat((vehicleProperty,Hidden_State),2)\n",
    "            outputState=self.objectModifyNet(objectAndFinalEffect)\n",
    "            return outputState\n",
    "        else:\n",
    "            outputs = None\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:,:]), Hidden_State, Cell_State)  \n",
    "                if outputs is None:\n",
    "                    outputs = Hidden_State.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "            return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "#         use_gpu = torch.cuda.is_available()\n",
    "        if useGpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Cell_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            return Hidden_State, Cell_State\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            Cell_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            return Hidden_State, Cell_State\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def fromObjectsToRelationPairs(combinedTensor):\n",
    "    '''\n",
    "    Args:\n",
    "        The input tensor should already be transposed if it is generated from the network's output\n",
    "    Returns:\n",
    "        Relation pairs\n",
    "    '''\n",
    "    #generate relation tensor for all vehicle pairs\n",
    "    relationTensorLeft=combinedTensor[:,0].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])\n",
    "    relationTensorRight=torch.transpose(torch.cat((combinedTensor[:,:0],combinedTensor[:,1:]),1),0,1)\n",
    "#         print(relationTensorRight.shape,relationTensorRight.shape)\n",
    "    for i in range(1,combinedTensor.shape[1]):\n",
    "        relationTensorLeft=torch.cat((relationTensorLeft,\\\n",
    "                                      combinedTensor[:,i].expand(combinedTensor.shape[1]-1,combinedTensor.shape[0])),0)\n",
    "        relationTensorRight=torch.cat((relationTensorRight,\\\n",
    "                                       torch.transpose(torch.cat((combinedTensor[:,:i],combinedTensor[:,i+1:]),1),0,1)),0)\n",
    "#         print(relationTensorLeft.shape,relationTensorRight.shape)\n",
    "    combinedRelationTensor=torch.cat((relationTensorLeft,relationTensorRight),1) \n",
    "    return combinedRelationTensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationLSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, input_size=20, cell_size=20, hidden_size=20, \\\n",
    "                 input_size_2=20, cell_size_2=20,hidden_size_2=20, outputTimeFrame=5,output_last = True):\n",
    "        \"\"\"\n",
    "        cell_size is the size of cell_state.\n",
    "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
    "        the input of LSTM  structure was the output of 'fromRelationToEffectNet' module, so that \n",
    "        the effectOutputTensorSize has the same number as input_size. \n",
    "        \n",
    "        \"\"\"\n",
    "        super(RelationLSTMSeq2Seq, self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        \n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "        self.velocityTuple=(2,3,8,9)\n",
    "        self.acclerateTuple=(4,5,10,11)\n",
    "        \n",
    "        #effect representive vector computation lstm\n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        #object modifying lstm\n",
    "        self.cell_size2 = cell_size_2\n",
    "        self.hidden_size2 = hidden_size_2\n",
    "        self.fl2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.il2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.ol2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.Cl2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        \n",
    "        self.outputTimeFrame=outputTimeFrame\n",
    "\n",
    "        self.fromRelationToEffectNet=fromRelationToEffectNetwork(effectOutputTensorSize=input_size)\n",
    "        self.objectModifyNet=objectModifyNetwork(inputSize=hidden_size+6,outputSize=6)\n",
    "        self.hiddenStateToEffectNetwork=hiddenStateToEffectNetwork(inputSize=hidden_size,outputSize=input_size)\n",
    "        #descreption for the statement above: the input_size applied to the parameter outputSize is \n",
    "        #the size of effect tensor in the init function. \n",
    "        \n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, inputEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputEffectTensor, Hidden_State), 2)\n",
    "#         print('in step,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl(combined))\n",
    "        i = F.sigmoid(self.il(combined))\n",
    "        o = F.sigmoid(self.ol(combined))\n",
    "        C = F.tanh(self.Cl(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def step2(self, inputPreEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputPreEffectTensor, Hidden_State), 2)\n",
    "#         print('in step2,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl2(combined))\n",
    "        i = F.sigmoid(self.il2(combined))\n",
    "        o = F.sigmoid(self.ol2(combined))\n",
    "        C = F.tanh(self.Cl2(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        time_step = inputs.size(1)\n",
    "        Hidden_State, Cell_State,Hidden_State_2,Cell_State_2 = self.initHidden(batch_size)\n",
    "        \n",
    "        if self.output_last:\n",
    "            #effect computation\n",
    "            for i in range(time_step):\n",
    "                effects=self.fromRelationToEffectNet(inputs[:,i,:,:])\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(effects), Hidden_State, Cell_State) \n",
    "                #the property of each vehicle\n",
    "                \n",
    "            Hidden_State_2=Hidden_State\n",
    "            Cell_State_2=Cell_State\n",
    "            \n",
    "            #the lstm process below take as inputs the previous object states and output the next predicted states\n",
    "            #applying function permute to deal with the dimension inconsistency\n",
    "            '''\n",
    "            inputs should be processed by function \"fromObjectsToRelationPairsBatchAndTimestepVersion\" \n",
    "            IF DATASETV3 VERSION IS USED TO GET DATA\n",
    "            '''\n",
    "            print('in seq2seq, inputs shape:',inputs.shape)\n",
    "            preVehicleProperty=inputs[:,-1,self.tupleForEachVehicle,0:6].squeeze()\n",
    "            preVehicleRelations=inputs[:,-1,:,:]\n",
    "            \n",
    "            #object properties computation\n",
    "            for i in range(outputTimeFrame):\n",
    "                objectAndFinalEffect=torch.cat((preVehicleProperty,Hidden_State_2),2)  \n",
    "                preVehicleProperty=self.objectModifyNet(objectAndFinalEffect)\n",
    "                effects=self.fromRelationToEffectNet(preVehicleRelations)\n",
    "                Hidden_State_2,Cell_State_2=self.step2(torch.squeeze(effects),Hidden_State_2,Cell_State_2)\n",
    "                if i==0:\n",
    "                    #add dimension timestep, which in dimension 1\n",
    "                    outputVehicleProperties=preVehicleProperty.unsqueeze(1) \n",
    "                else:\n",
    "                    outputVehicleProperties=torch.cat((outputVehicleProperties,preVehicleProperty.unsqueeze(1)),1)\n",
    "                #don't need to compute new effect vector after the prediction of the last time step\n",
    "                if i<outputTimeFrame-1:\n",
    "                    effectFromHiddenState=self.hiddenStateToEffectNetwork(Hidden_State_2)\n",
    "                    self.objectModifyNet()\n",
    "                    preVehicleProperty.cpu()\n",
    "                    #add timestep dimension for further process\n",
    "                    preVehiclePropertyAddTimestep=preVehicleProperty.unsqueeze(1)\n",
    "                    relationPairs=fromObjectsToRelationPairsBatchAndTimestepVersion(preVehiclePropertyAddTimestep).squeeze()\n",
    "                    if useGpu:\n",
    "                        relationPairs.cuda()\n",
    "                    effectInPropertiesComputation=self.fromRelationToEffectNet(relationPairs)\n",
    "                    Hidden_State_2,Cell_State_2=self.step2(effectInPropertiesComputation,Hidden_State_2,Cell_State_2)\n",
    "                if i==outputTimeFrame:\n",
    "                    break\n",
    "#             print(inputs.shape)\n",
    "#             vehicleProperty=inputs[:,-1,self.tupleForEachVehicle,0:6].squeeze()\n",
    "#             objectAndFinalEffect=torch.cat((vehicleProperty,Hidden_State),2)\n",
    "#             outputState=self.objectModifyNet(objectAndFinalEffect)\n",
    "            return outputVehicleProperties\n",
    "        else:\n",
    "            outputs = None\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:,:]), Hidden_State, Cell_State)  \n",
    "                if outputs is None:\n",
    "                    outputs = Hidden_State.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "            return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "#         use_gpu = torch.cuda.is_available()\n",
    "        if useGpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Cell_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Hidden_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2).cuda())\n",
    "            Cell_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2).cuda())\n",
    "            return Hidden_State, Cell_State,Hidden_State_2,Cell_State_2\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            Cell_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            Hidden_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2))\n",
    "            Cell_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2))\n",
    "            return Hidden_State, Cell_State,Hidden_State_2,Cell_State_2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationLSTMSeq2SeqPositionOnly(nn.Module):\n",
    "    def __init__(self, input_size=20, cell_size=20, hidden_size=20, \\\n",
    "                 input_size_2=20, cell_size_2=20,hidden_size_2=20, outputTimeFrame=5,output_last = True):\n",
    "        \"\"\"\n",
    "        cell_size is the size of cell_state.\n",
    "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
    "        the input of LSTM  structure is the output of 'fromRelationToEffectNet' module, so that \n",
    "        the effectOutputTensorSize has the same number as input_size. \n",
    "        \n",
    "        \"\"\"\n",
    "        super(RelationLSTMSeq2SeqPositionOnly, self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        \n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "        self.positionTuple=(0,1,6,7)\n",
    "#         self.velocityTuple=(2,3,8,9)\n",
    "#         self.acclerateTuple=(4,5,10,11)\n",
    "        \n",
    "        #effect representive vector computation lstm\n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        #object modifying lstm\n",
    "        self.cell_size2 = cell_size_2\n",
    "        self.hidden_size2 = hidden_size_2\n",
    "        self.fl2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.il2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.ol2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.Cl2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        \n",
    "        self.outputTimeFrame=outputTimeFrame\n",
    "\n",
    "        self.fromRelationToEffectNet=fromRelationToEffectNetworkPositionOnly(effectOutputTensorSize=input_size)\n",
    "        self.objectModifyNet=objectModifyNetwork(inputSize=input_size+6,outputSize=6)\n",
    "        self.hiddenStateToEffectNetwork=hiddenStateToEffectNetwork(inputSize=hidden_size,outputSize=input_size)\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, inputEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputEffectTensor, Hidden_State), 2)\n",
    "#         print('in step,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl(combined))\n",
    "        i = F.sigmoid(self.il(combined))\n",
    "        o = F.sigmoid(self.ol(combined))\n",
    "        C = F.tanh(self.Cl(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def step2(self, inputPreEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputPreEffectTensor, Hidden_State), 2)\n",
    "#         print('in step2,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl2(combined))\n",
    "        i = F.sigmoid(self.il2(combined))\n",
    "        o = F.sigmoid(self.ol2(combined))\n",
    "        C = F.tanh(self.Cl2(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        time_step = inputs.size(1)\n",
    "        Hidden_State, Cell_State,Hidden_State_2,Cell_State_2 = self.initHidden(batch_size)\n",
    "        \n",
    "        if self.output_last:\n",
    "            #effect computation\n",
    "            for i in range(time_step):\n",
    "                effects=self.fromRelationToEffectNet(inputs[:,i,:,:])\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(effects), Hidden_State, Cell_State) \n",
    "                #the property of each vehicle\n",
    "                \n",
    "            Hidden_State_2=Hidden_State\n",
    "            Cell_State_2=Cell_State\n",
    "            \n",
    "            #the lstm process below take as inputs the previous object states and output the next predicted states\n",
    "            #applying function permute to deal with the dimension inconsistency\n",
    "            '''\n",
    "            inputs should be processed by function \"fromObjectsToRelationPairsBatchAndTimestepVersion\" \n",
    "            IF DATASETV3 VERSION IS USED TO GET DATA\n",
    "            '''\n",
    "            print('in seq2seq, inputs shape:',inputs.shape)\n",
    "            preVehicleProperty=inputs[:,-1,self.tupleForEachVehicle,0:6].squeeze()\n",
    "            preRelations=inputs[:,-1,:,:]\n",
    "            #object properties computation\n",
    "            for i in range(outputTimeFrame):\n",
    "                #objectAndFinalEffect=torch.cat((preVehicleProperty,Hidden_State_2),2)  #??????????\n",
    "                #WRONG!!!!The final hidden state of the first lstm model is not a effect tensor!!\n",
    "                #The hidden state of the first lstm represents somehow the encode hidden state in translation network,\n",
    "                #and obviously, the hidden state of encode component in translation network \n",
    "                #does not contain the result of translation. We could get the result from the output of\n",
    "                #the decode part of the translation network\n",
    "                #the shape of input tensor to the function fromRelationToEffectNet is (batch, vehicles, properties)\n",
    "                preEffect=self.fromRelationToEffectNet(preRelations)#the effect produced by this function is already a combined effect\n",
    "                logging.debug(fromAllToStr('preEffect.shape: ',preEffect.shape))\n",
    "                Hidden_State_2,Cell_State_2=self.step2(preEffect,Hidden_State_2,Cell_State_2)\n",
    "                logging.debug(fromAllToStr('Hidden_State_2.shape: ',Hidden_State_2.shape))\n",
    "                effectFromHiddenState=self.hiddenStateToEffectNetwork(Hidden_State_2)\n",
    "                logging.debug(fromAllToStr('effectFromeHiddenState.shape: ',effectFromHiddenState.shape))\n",
    "                objectAndEffectTensor=torch.cat((effectFromHiddenState,preVehicleProperty),2)\n",
    "                logging.debug(fromAllToStr('objectAndEffectTensor.shape: ',objectAndEffectTensor.shape))\n",
    "                modifiedObject=self.objectModifyNet(objectAndEffectTensor)\n",
    "                logging.debug(fromAllToStr('modifiedObject.shape: ',modifiedObject.shape))\n",
    "                newRelation=fromObjectsToRelationPairsBatchAndTimestepVersion(modifiedObject.unsqueeze(1).permute(0,1,3,2))\n",
    "                preRelations=newRelation.squeeze()\n",
    "                if i==0:\n",
    "                    #add dimension timestep, which in dimension 1\n",
    "                    outputVehicleProperties=modifiedObject.unsqueeze(1) \n",
    "                else:\n",
    "                    outputVehicleProperties=torch.cat((outputVehicleProperties,modifiedObject.unsqueeze(1)),1)\n",
    "                #doesn't need to compute new effect vector after the prediction of the last time step\n",
    "                if i==outputTimeFrame:\n",
    "                    break\n",
    "#             print(inputs.shape)\n",
    "#             vehicleProperty=inputs[:,-1,self.tupleForEachVehicle,0:6].squeeze()\n",
    "#             objectAndFinalEffect=torch.cat((vehicleProperty,Hidden_State),2)\n",
    "#             outputState=self.objectModifyNet(objectAndFinalEffect)\n",
    "            return outputVehicleProperties\n",
    "        else:\n",
    "            outputs = None\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:,:]), Hidden_State, Cell_State)  \n",
    "                if outputs is None:\n",
    "                    outputs = Hidden_State.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "            return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "#         use_gpu = torch.cuda.is_available()\n",
    "        if useGpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Cell_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Hidden_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2).cuda())\n",
    "            Cell_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2).cuda())\n",
    "            return Hidden_State, Cell_State,Hidden_State_2,Cell_State_2\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            Cell_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            Hidden_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2))\n",
    "            Cell_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2))\n",
    "            return Hidden_State, Cell_State,Hidden_State_2,Cell_State_2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fromRelationToEffectNetworkWithDiscountParameter(nn.Module):\n",
    "    '''\n",
    "    A new network for producing effect from relation with a discount parameter tensor \n",
    "    the dimension of the input relation tensor is (batches, timesteps, properties)\n",
    "    '''\n",
    "    def __init__(self,effectOutputTensorSize=20, maxRelationNumber=20,maxMatrixIndex=250,relationTensorSize=40):\n",
    "        super(fromRelationToEffectNetworkWithDiscountParameter,self).__init__()\n",
    "        self.maxMatrixIndex=maxMatrixIndex\n",
    "\n",
    "        \n",
    "        #position relation network initialize\n",
    "        self.relationTensorSize=relationTensorSize\n",
    "        self.positionTuple=(0,1,2,3)\n",
    "        self.positionRelationNet=relationNetwork(outputSize=self.relationTensorSize)\n",
    "       \n",
    "        #effect network initialize\n",
    "        self.objectAndTensorSize=self.relationTensorSize+4 #NOTE HERE: does it neccessary to contain both the tow object positions in the effect tensor?\n",
    "        self.effectOutputTensorSize=effectOutputTensorSize\n",
    "        #the number 4 is the size of positon(or velocity or accelerate) pairs,\n",
    "        #such as (positonxObject1,positonyObject1,positionxObject2,positionyObject2)\n",
    "        self.positionEffectNet=effectNetwork(inputSize=self.objectAndTensorSize,outputSize=self.effectOutputTensorSize)\n",
    "\n",
    "  \n",
    "    def forward(self,inputObjectsPairs,discountParameterTensor):\n",
    "        #relation computation\n",
    "        effectOutputTensorSize=self.effectOutputTensorSize\n",
    "        positionRelationTensors=self.positionRelationNet(inputObjectsPairs[:,:,self.positionTuple])\n",
    "        objectsAndPositionRelationTensors=torch.cat((inputObjectsPairs[:,:,self.positionTuple],positionRelationTensors),2)\n",
    "        #effect computation\n",
    "        positionEffectTensors=self.positionEffectNet(objectsAndPositionRelationTensors)\n",
    "        #effect combination with discount parameter\n",
    "        #tensor summation\n",
    "        batchSize=positionRelationTensors.shape[0]\n",
    "        #apply discount parameters of distance to effectTensors\n",
    "        positionEffectTensors=torch.mul(positionEffectTensors,discountParameterTensor)\n",
    "        if useGpu==True:\n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize).cuda()\n",
    "        else: \n",
    "            positionEffectSummation=torch.zeros(batchSize,maxMatrixIndex,effectOutputTensorSize)\n",
    "        for i in range(maxMatrixIndex):\n",
    "            positionEffectSummation[:,i,:]=torch.sum(positionEffectTensors[:,(i*self.maxRelationNumber):((i+1)*self.maxRelationNumber),:],1)\n",
    "        \n",
    "        #remove object extraction component and computation component\n",
    "#         #the property of each vehicle\n",
    "#         vehicleProperty=inputObjectsPairs[:,self.tupleForEachVehicle,0:6]\n",
    "#         objectAndFinalEffect=torch.cat((vehicleProperty,processedCombinedEffectTensors),2)\n",
    "        \n",
    "#         #compute final state\n",
    "#         finalObjectState=self.objectModifyNet(objectAndFinalEffect)\n",
    "        return processedCombinedEffectTensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class differenceLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=20, cell_size=20, hidden_size=20, \\\n",
    "                 input_size_2=20, cell_size_2=20,hidden_size_2=20, outputTimeFrame=5,output_last = True,\\\n",
    "                maxRelationNUmber=20,differenceSize=2,inputRelationTensor=4):\n",
    "        \"\"\"\n",
    "        cell_size is the size of cell_state.\n",
    "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
    "        the input of LSTM  structure was the output of 'fromRelationToEffectNet' module, so that \n",
    "        the effectOutputTensorSize has the same number as input_size. \n",
    "        The dataset for this mode, DatasetV4, only return tensors with position values without velocity and accelerate values.\n",
    "        \n",
    "        \"\"\"\n",
    "        super(differenceLSTMModel, self).__init__()\n",
    "        self.maxMatrixIndex=250\n",
    "        \n",
    "        #generate a tuple in which each element is the index of a vehicle\n",
    "        #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "        listForEachVehicle=[]\n",
    "        for i in range(self.maxMatrixIndex):\n",
    "            listForEachVehicle.append(i*(self.maxMatrixIndex-1))\n",
    "        self.tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "#         self.positionTuple=(0,1,6,7)\n",
    "#         self.velocityTuple=(2,3,8,9)\n",
    "#         self.acclerateTuple=(4,5,10,11)\n",
    "        \n",
    "        #effect representive vector computation lstm\n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        #object modifying lstm\n",
    "        self.cell_size2 = cell_size_2\n",
    "        self.hidden_size2 = hidden_size_2\n",
    "        self.fl2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.il2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.ol2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        self.Cl2 = nn.Linear(input_size_2 + hidden_size_2, hidden_size_2)\n",
    "        \n",
    "        self.outputTimeFrame=outputTimeFrame\n",
    "\n",
    "        self.fromRelationToEffectNet=fromRelationToEffectNetworkPositionOnly(effectOutputTensorSize=input_size)\n",
    "        self.objectModifyNet=objectModifyNetwork(inputSize=hidden_size+6,outputSize=6)\n",
    "        self.effectComputationWithDiscountParameterNet=fromRelationToEffectNetworkWithDiscountParameter(\\\n",
    "            maxRelationNumber=maxRelationsNumberGlobal,effectOutputTensorSize=input_size)\n",
    "        #the two variable in output size represent the difference of x and y positions\n",
    "        self.hiddenStateToDifferenceTensorNet=\\\n",
    "        hiddenStateToDifferenceTensorNet(inputSize=hidden_size,outputSize=2)\n",
    "        \n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, inputEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputEffectTensor, Hidden_State), 2)\n",
    "#         print('in step,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl(combined))\n",
    "        i = F.sigmoid(self.il(combined))\n",
    "        o = F.sigmoid(self.ol(combined))\n",
    "        C = F.tanh(self.Cl(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def step2(self, inputPreEffectTensor, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((inputPreEffectTensor, Hidden_State), 2)\n",
    "#         print('in step2,combined.shape',combined.shape)\n",
    "        f = F.sigmoid(self.fl2(combined))\n",
    "        i = F.sigmoid(self.il2(combined))\n",
    "        o = F.sigmoid(self.ol2(combined))\n",
    "        C = F.tanh(self.Cl2(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def forward(self, inputs,discountParameterTensor,vehicleGraph):\n",
    "        '''\n",
    "        Args(only for notes):\n",
    "            inputs:relation tensors\n",
    "        '''\n",
    "        batch_size = inputs.size(0)\n",
    "        time_step = inputs.size(1)\n",
    "        Hidden_State, Cell_State,Hidden_State_2,Cell_State_2 = self.initHidden(batch_size)\n",
    "        \n",
    "        if self.output_last:\n",
    "            #effect computation\n",
    "            for i in range(time_step):\n",
    "                effects=self.effectComputationWithDiscountParameterNet\\\n",
    "                (inputs[:,i,:,:],discountParameterTensor[:,i,:,:])\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(effects), Hidden_State, Cell_State) \n",
    "                #the property of each vehicle\n",
    "                \n",
    "            Hidden_State_2=Hidden_State\n",
    "            Cell_State_2=Cell_State\n",
    "            \n",
    "            #the lstm process below take as inputs the previous object states and output the next predicted states\n",
    "            #applying function permute to deal with the dimension inconsistency\n",
    "            logging.debug(fromAllToStr('in seq2seq, inputs shape:',inputs.shape))\n",
    "            preVehicleProperty=vehicleGraph[:,-1,:,:].squeeze()\n",
    "            \n",
    "            \n",
    "            #object properties computation\n",
    "            for i in range(outputTimeFrame):\n",
    "                #produce relation first\n",
    "                for relationComputingBatch in range(preVehicleProperty.shape[0]):\n",
    "                    relationTensorInBatchComputing, discountParameterTensorInBatchComputing=\\\n",
    "                    computeRelationAndAllTheOtherTensorsWithDistance(preVehicleProperty[relationComputingBatch].unsqueeze(0))\n",
    "                    relationTensorInBatchComputing,discountParameterTensorInBatchComputing=\\\n",
    "                    relationTensorInBatchComputing.unsqueeze(0),discountParameterTensorInBatchComputing.unsqueeze(0)\n",
    "                    if relationComputingBatch==0:\n",
    "                        allRelationTensorOfPrevehicle,allDiscountParameterTensor=\\\n",
    "                        relationTensorInBatchComputing,discountParameterTensorInBatchComputing\n",
    "                    else:\n",
    "                        allRelationTensorOfPrevehicle,allRelationTensorOfPrevehicle=\\\n",
    "                        torch.cat((allRelationTensorOfPrevehicle,relationTensorInBatchComputing),0),\\\n",
    "                        torch.cat((allDiscountParameterTensor,discountParameterTensorInBatchComputing),0)\n",
    "                #then compute effect using relations and discount parameters\n",
    "                combinedEffect=self.effectComputationWithDiscountParameterNet\\\n",
    "                (allRelationTensorOfPrevehicle,allDiscountParameterTensor)\n",
    "#                 if i==0:\n",
    "#                     #add dimension timestep, which in dimension 1\n",
    "#                     outputVehicleProperties=preVehicleProperty.unsqueeze(1) \n",
    "#                 else:\n",
    "#                     outputVehicleProperties=torch.cat((outputVehicleProperties,preVehicleProperty.unsqueeze(1)),1)\n",
    "                #don't need to compute new effect vector after the prediction of the last time step\n",
    "                if i<outputTimeFrame-1:\n",
    "                    Hidden_State_2,Cell_State_2=self.step2(combinedEffect,Hidden_State_2,Cell_State_2)\n",
    "                    differenceTensor=self.hiddenStateToDifferenceTensorNet(Hidden_State_2)\n",
    "                    logging.debug(fromAllToStr('differenceTensor.shape:',differenceTensor.shape))\n",
    "                    logging.debug(fromAllToStr('preVehicleProperty.shape:',preVehicleProperty.shape))\n",
    "                    newGraph=preVehicleProperty+differenceTensor\n",
    "                    #tensors of difference and predicted graph should be stored as the output of the model\n",
    "                    if i==0:\n",
    "                        #the reason for we do not unsqueeze dimension 0 but 1 is that 0 is the dimension of batch, and 1 represent time step\n",
    "                        allDifferenceTensor=differenceTensor.unsqueeze(1)\n",
    "                        allGraphTensor=newGraph.unsqueeze(1)\n",
    "                    else:\n",
    "                        #see above why we cat dimension 1 but not 0\n",
    "                        allDifferenceTensor=torch.cat((allDifferenceTensor,differenceTensor),1)\n",
    "                        allGraphTensor=torch.cat((allGraphTensor,newGraph))\n",
    "                if i==outputTimeFrame:\n",
    "                    break\n",
    "#             print(inputs.shape)\n",
    "#             vehicleProperty=inputs[:,-1,self.tupleForEachVehicle,0:6].squeeze()\n",
    "#             objectAndFinalEffect=torch.cat((vehicleProperty,Hidden_State),2)\n",
    "#             outputState=self.objectModifyNet(objectAndFinalEffect)\n",
    "            return allDifferenceTensor,allGraphTensor\n",
    "        else:\n",
    "            outputs = None\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:,:]), Hidden_State, Cell_State)  \n",
    "                if outputs is None:\n",
    "                    outputs = Hidden_State.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "            return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "#         use_gpu = torch.cuda.is_available()\n",
    "        if useGpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Cell_State = Variable(torch.zeros(batch_size,maxMatrixIndex, self.hidden_size).cuda())\n",
    "            Hidden_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2).cuda())\n",
    "            Cell_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2).cuda())\n",
    "            return Hidden_State, Cell_State,Hidden_State_2,Cell_State_2\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            Cell_State = Variable(torch.zeros(batch_size, maxMatrixIndex,self.hidden_size))\n",
    "            Hidden_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2))\n",
    "            Cell_State_2=Variable(torch.zeros(batch_size,maxMatrixIndex,self.hidden_size2))\n",
    "            return Hidden_State, Cell_State,Hidden_State_2,Cell_State_2\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class elementWeightedLoss(nn.Module):\n",
    "    def __init__(self,lambdaDistance,lambdaVelocity,lambdaAccelerate):\n",
    "        super(elementWeightedLoss,self).__init__()\n",
    "        self.lambdaDistance,self.lambdaVelocity,self.lambdaAccelerate=\\\n",
    "        lambdaDistance,lambdaVelocity,lambdaAccelerate\n",
    "        \n",
    "    def forward(self,output,label):\n",
    "        '''\n",
    "        output and label dimension: [batch, timestep, vehiclenum, properties]\n",
    "        properties dimension:(distancex,distancey,velocityx,velocityy,acceleratex,acceleratey)\n",
    "        '''\n",
    "        distanceLoss=torch.mean(torch.pow(output[:,:,:,0:2]-label[:,:,:,0:2],2))\n",
    "        velocityLoss=torch.mean(torch.pow(output[:,:,:,2:4]-label[:,:,:,2:4],2))\n",
    "        accelerateLoss=torch.mean(torch.pow(output[:,:,:,4:6]-label[:,:,:,4:6],2))\n",
    "        return torch.mul(distanceLoss,self.lambdaDistance)+torch.mul(velocityLoss,self.lambdaVelocity)+\\\n",
    "                torch.mul(accelerateLoss,self.lambdaAccelerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# seperation of model and testing part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing differenceLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runDifferenceLSTMModel:\n",
    "    differenceLSTMNet=differenceLSTMModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST RELATION WITHIN A GIVEN RANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasetV4Instance=tensorsDatasetV4(trajectoryFileList,lableTensorEachBatch=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasetV4Loader=DataLoader(datasetV4Instance,batch_size=5,num_workers=4,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "itemList=datasetV4Instance.__getitem__(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for item in itemList:\n",
    "    logging.debug(fromAllToStr(item.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combineTensorTrain,combineTensorValid,combinedRelationTensors,combinedDiscountParameterTensors,\\\n",
    "differenceLabels=itemList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logging.debug(fromAllToStr('combnedRelationTensors.shape',combinedRelationTensors.shape))\n",
    "for i in range(combinedRelationTensors.shape[0]):\n",
    "    logging.debug(fromAllToStr('combinedRelationTensor, batch ', i, ':',combinedRelationTensors[i,0:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logging.debug(fromAllToStr(\"relationInAllTimeStep.shape:\",relationInAllTimeStep.shape))\n",
    "logging.debug(fromAllToStr(\"discountInAllTimeStep,shape:\",discountInAllTimeStep.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorA=torch.randn(10,10,100)\n",
    "tensorB=torch.randn(10,10,1)\n",
    "logging.debug(fromAllToStr('tensorA',tensorA))\n",
    "logging.debug(fromAllToStr('tensorB',tensorB))\n",
    "tensorAmB=torch.mul(tensorA,tensorB)\n",
    "logging.debug(fromAllToStr('tensorAmB',tensorAmB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loaderIter=iter(datasetV3Loader)\n",
    "toTestDifferenceItem, toTestDifferenceLabel=loaderIter.__next__()\n",
    "logging.debug(fromAllToStr(\"toTestDifferenceItem[0]:\",toTestDifferenceItem[0][0][0]))\n",
    "differenceSeries=differenceBetweenTwoFrameForBatch(toTestDifferenceItem)\n",
    "logging.debug(fromAllToStr('differenceSeries.shape:',differenceSeries.shape,'toTestDifferenceItem.shape:',\\\n",
    "                          toTestDifferenceItem.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logging.info('test')\n",
    "logTensor=torch.zeros(3,3)\n",
    "logging.debug(str(logTensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testTensor=torch.rand((6))\n",
    "logging.debug(testTensor)\n",
    "logging.debug(testTensor.expand(3,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test seq2seq relation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test code\n",
    "if runSeq2SeqRelationModel:\n",
    "    outputTimeFrame=5\n",
    "    RelationLSTMSeq2SeqModel=RelationLSTMSeq2Seq(outputTimeFrame=outputTimeFrame)\n",
    "    datasetV3Instance=tensorsDatasetV3(trajectoryFileList,lableTensorEachBatch=outputTimeFrame)\n",
    "    datasetV3Loader=DataLoader(datasetV3Instance,batch_size=2)\n",
    "    V3iter=iter(datasetV3Loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test code\n",
    "if runSeq2SeqRelationModel:\n",
    "    RelationLSTMSeq2SeqModel=RelationLSTMSeq2Seq(outputTimeFrame=outputTimeFrame)\n",
    "    inputs, labels=V3iter.__next__()\n",
    "    inputs=fromObjectsToRelationPairsBatchAndTimestepVersion(inputs)\n",
    "    outputs=RelationLSTMSeq2SeqModel(inputs)\n",
    "    print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model and dataset\n",
    "if runSeq2SeqRelationModel:\n",
    "    epochs=500\n",
    "    itersInEachEpoch=100\n",
    "    outputTimeFrame=5\n",
    "    relationLSTMSeq2SeqModel=RelationLSTMSeq2SeqPositionOnly(outputTimeFrame=outputTimeFrame)\n",
    "    datasetV3Instance=tensorsDatasetV3(trajectoryFileList,lableTensorEachBatch=outputTimeFrame)\n",
    "    datasetV3Loader=DataLoader(datasetV3Instance,batch_size=5,num_workers=4)\n",
    "    V3iter=iter(datasetV3Loader)\n",
    "    if useGpu:\n",
    "        relationLSTMSeq2SeqModel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "    relationLSTMSeq2SeqModel=RelationLSTMSeq2SeqPositionOnly(outputTimeFrame=outputTimeFrame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runSeq2SeqRelationModel:\n",
    "    if not isTest:\n",
    "        learningRate=1e-3\n",
    "        lossFn=elementWeightedLoss(100,0.1,0.1)\n",
    "        lambdaWholeNet=lambda epoch: 0.5**(epoch//30)\n",
    "        optim=torch.optim.RMSprop([{'params':relationLSTMSeq2SeqModel.parameters(),'initial_lr':learningRate}],lr=learningRate)\n",
    "        lrSchedule=torch.optim.lr_scheduler.LambdaLR(optim,lambdaWholeNet,last_epoch=10)\n",
    "        relationLSTMSeq2SeqModel.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training seq2seqRelation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "2020-08-12 16:04:18,205 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:04:18,207 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:04:18,209 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:04:18,210 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:04:18,211 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in seq2seq, inputs shape: torch.Size([5, 10, 62250, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-12 16:04:18,573 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:04:18,575 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:04:18,577 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:04:18,579 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:04:18,580 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:04:18,924 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:04:18,926 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:04:18,927 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:04:18,928 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:04:18,930 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:04:19,269 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:04:19,271 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:04:19,274 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:04:19,275 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:04:19,276 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:04:19,603 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:04:19,605 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:04:19,607 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:04:19,608 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:04:19,610 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:04:19,798 -17 - <module> - DEBUG - compare the shape of outputs and labels:torch.Size([5, 5, 250, 6])torch.Size([5, 5, 250, 6])\n",
      "2020-08-12 16:04:59,457 -23 - <module> - INFO - loss in opoch 0,iteration0:tensor(0.8495, grad_fn=<AddBackward0>)\n",
      "2020-08-12 16:04:59,489 -1259 - _findfont_cached - DEBUG - findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0.\n",
      "2020-08-12 16:04:59,490 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Serif' (DejaVuSerif-BoldItalic.ttf) italic normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,490 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'STIXNonUnicode' (STIXNonUniBol.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,491 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'STIXNonUnicode' (STIXNonUniIta.ttf) italic normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,492 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'cmmi10' (cmmi10.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,492 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Serif' (DejaVuSerif-Italic.ttf) italic normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,493 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Sans Mono' (DejaVuSansMono-Oblique.ttf) oblique normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,493 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'cmsy10' (cmsy10.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,494 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Sans' (DejaVuSans.ttf) normal normal 400 normal>) = 0.05\n",
      "2020-08-12 16:04:59,494 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'cmex10' (cmex10.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,495 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'STIXSizeOneSym' (STIXSizOneSymBol.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,495 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Serif' (DejaVuSerif-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,496 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'cmtt10' (cmtt10.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,496 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Sans Display' (DejaVuSansDisplay.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,496 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Sans' (DejaVuSans-Bold.ttf) normal normal bold normal>) = 0.33499999999999996\n",
      "2020-08-12 16:04:59,497 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'STIXGeneral' (STIXGeneral.ttf) normal normal regular normal>) = 10.05\n",
      "2020-08-12 16:04:59,497 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Sans Mono' (DejaVuSansMono-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,497 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Sans' (DejaVuSans-BoldOblique.ttf) oblique normal bold normal>) = 1.335\n",
      "2020-08-12 16:04:59,498 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'STIXSizeThreeSym' (STIXSizThreeSymReg.ttf) normal normal regular normal>) = 10.05\n",
      "2020-08-12 16:04:59,498 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Sans' (DejaVuSans-Oblique.ttf) oblique normal 400 normal>) = 1.05\n",
      "2020-08-12 16:04:59,498 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'STIXSizeTwoSym' (STIXSizTwoSymBol.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,499 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Serif' (DejaVuSerif.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,499 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'cmr10' (cmr10.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,501 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'cmss10' (cmss10.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,501 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'STIXGeneral' (STIXGeneralItalic.ttf) italic normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,501 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'STIXSizeFourSym' (STIXSizFourSymReg.ttf) normal normal regular normal>) = 10.05\n",
      "2020-08-12 16:04:59,502 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'STIXSizeFiveSym' (STIXSizFiveSymReg.ttf) normal normal regular normal>) = 10.05\n",
      "2020-08-12 16:04:59,502 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'STIXSizeThreeSym' (STIXSizThreeSymBol.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,503 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'STIXNonUnicode' (STIXNonUniBolIta.ttf) italic normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,504 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'STIXGeneral' (STIXGeneralBolIta.ttf) italic normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,504 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'STIXSizeFourSym' (STIXSizFourSymBol.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,505 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Sans Mono' (DejaVuSansMono.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,505 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'cmb10' (cmb10.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,505 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'STIXNonUnicode' (STIXNonUni.ttf) normal normal regular normal>) = 10.05\n",
      "2020-08-12 16:04:59,506 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'STIXSizeTwoSym' (STIXSizTwoSymReg.ttf) normal normal regular normal>) = 10.05\n",
      "2020-08-12 16:04:59,506 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Sans Mono' (DejaVuSansMono-BoldOblique.ttf) oblique normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,507 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'STIXGeneral' (STIXGeneralBol.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,507 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Serif Display' (DejaVuSerifDisplay.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,508 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'STIXSizeOneSym' (STIXSizOneSymReg.ttf) normal normal regular normal>) = 10.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-12 16:04:59,508 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Samyak Tamil' (Samyak-Tamil.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,509 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Sahadeva' (sahadeva.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,509 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'ori1Uni' (utkal.ttf) normal normal medium normal>) = 10.145\n",
      "2020-08-12 16:04:59,510 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Noto Mono' (NotoMono-Regular.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,510 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Ubuntu Mono' (UbuntuMono-B.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,511 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Norasi' (Norasi.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,512 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Garuda' (Garuda-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,513 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Keraleeyam' (Keraleeyam.ttf) normal normal 700 normal>) = 10.335\n",
      "2020-08-12 16:04:59,513 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'KacstDigital' (KacstDigital.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,514 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Liberation Sans Narrow' (LiberationSansNarrow-BoldItalic.ttf) italic normal bold condensed>) = 11.535\n",
      "2020-08-12 16:04:59,514 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Meera' (Meera.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,515 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Lohit Odia' (Lohit-Odia.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,515 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Khmer OS' (KhmerOS.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,516 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Liberation Mono' (LiberationMono-Italic.ttf) italic normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,516 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Rekha' (Rekha.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,517 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'KacstOne' (KacstOne.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,517 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Samyak Devanagari' (Samyak-Devanagari.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,517 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Liberation Mono' (LiberationMono-BoldItalic.ttf) italic normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,518 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Sans' (DejaVuSansCondensed-Bold.ttf) normal normal bold condensed>) = 0.5349999999999999\n",
      "2020-08-12 16:04:59,518 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Likhan' (LikhanNormal.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,518 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'mry_KacstQurn' (mry_KacstQurn.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,519 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Mukti Narrow' (MuktiNarrowBold.ttf) normal normal 700 normal>) = 10.335\n",
      "2020-08-12 16:04:59,519 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Liberation Sans' (LiberationSans-Regular.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,519 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'FreeSerif' (FreeSerifItalic.ttf) italic normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,519 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Kalapi' (Kalapi.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,520 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Umpush' (Umpush-LightOblique.ttf) oblique normal light normal>) = 11.24\n",
      "2020-08-12 16:04:59,520 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Liberation Serif' (LiberationSerif-Italic.ttf) italic normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,520 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Ubuntu' (Ubuntu-B.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,521 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Waree' (Waree-Oblique.ttf) oblique normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,521 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Tlwg Typist' (TlwgTypist-Oblique.ttf) oblique normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,521 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Phetsarath OT' (Phetsarath_OT.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,522 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Liberation Mono' (LiberationMono-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,522 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Jamrul' (JamrulNormal.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,522 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Rachana' (Rachana-Regular.ttf) normal normal regular normal>) = 10.05\n",
      "2020-08-12 16:04:59,523 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Karumbi' (Karumbi.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,523 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Rachana' (Rachana-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,523 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Ubuntu' (Ubuntu-M.ttf) normal normal medium normal>) = 10.145\n",
      "2020-08-12 16:04:59,523 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Lohit Tamil Classical' (Lohit-Tamil-Classical.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,524 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'padmaa' (padmaa.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,524 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Serif' (DejaVuSerifCondensed-Bold.ttf) normal normal bold condensed>) = 10.535\n",
      "2020-08-12 16:04:59,524 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Lohit Assamese' (Lohit-Assamese.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,527 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Padauk' (Padauk-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,527 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Math TeX Gyre' (DejaVuMathTeXGyre.ttf) normal normal regular normal>) = 10.05\n",
      "2020-08-12 16:04:59,527 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Norasi' (Norasi-BoldOblique.ttf) oblique normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,528 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'KacstOne' (KacstOne-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,528 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Ubuntu' (Ubuntu-MI.ttf) italic normal medium normal>) = 11.145\n",
      "2020-08-12 16:04:59,528 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'AR PL UMing CN' (uming.ttc) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,529 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Liberation Serif' (LiberationSerif-Regular.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,529 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'AR PL UKai CN' (ukai.ttc) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,529 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Pagul' (Pagul.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,529 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Waree' (Waree-BoldOblique.ttf) oblique normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,530 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Lohit Devanagari' (Lohit-Devanagari.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,530 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Nakula' (nakula.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,530 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Tibetan Machine Uni' (TibetanMachineUni.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,531 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Sans' (DejaVuSansCondensed-BoldOblique.ttf) oblique normal bold condensed>) = 1.535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-12 16:04:59,531 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Purisa' (Purisa-BoldOblique.ttf) oblique normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,531 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'FreeMono' (FreeMono.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,532 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Ubuntu' (Ubuntu-BI.ttf) italic normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,532 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Norasi' (Norasi-Italic.ttf) italic normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,532 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Khmer OS System' (KhmerOSsys.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,533 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Liberation Mono' (LiberationMono-Regular.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,533 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Tlwg Mono' (TlwgMono-Oblique.ttf) oblique normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,533 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Uroob' (Uroob.ttf) normal normal 700 normal>) = 10.335\n",
      "2020-08-12 16:04:59,533 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Tlwg Mono' (TlwgMono-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,534 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Manjari' (Manjari-Thin.otf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,534 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'KacstPoster' (KacstPoster.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,534 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Kinnari' (Kinnari-Italic.ttf) italic normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,535 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Noto Serif CJK JP' (NotoSerifCJK-Bold.ttc) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,535 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'aakar' (aakar-medium.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,535 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Navilu' (Navilu.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,536 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Umpush' (Umpush-Oblique.ttf) oblique normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,536 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Norasi' (Norasi-BoldItalic.ttf) italic normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,536 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Sawasdee' (Sawasdee-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,536 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Liberation Sans Narrow' (LiberationSansNarrow-Regular.ttf) normal normal 400 condensed>) = 10.25\n",
      "2020-08-12 16:04:59,537 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Noto Serif CJK JP' (NotoSerifCJK-Regular.ttc) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,537 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Liberation Sans' (LiberationSans-BoldItalic.ttf) italic normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,537 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Samanata' (samanata.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,538 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'LKLUG' (lklug.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,538 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Waree' (Waree.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,538 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Norasi' (Norasi-Oblique.ttf) oblique normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,538 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Droid Sans Fallback' (DroidSansFallbackFull.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,539 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Umpush' (Umpush-BoldOblique.ttf) oblique normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,539 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Liberation Sans' (LiberationSans-Italic.ttf) italic normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,539 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Tlwg Typewriter' (TlwgTypewriter.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,540 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Abyssinica SIL' (AbyssinicaSIL-R.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,540 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'padmaa' (padmaa-Medium-0.5.ttf) normal normal normal normal>) = 10.0\n",
      "2020-08-12 16:04:59,540 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'KacstBook' (KacstBook.ttf) normal normal book normal>) = 10.05\n",
      "2020-08-12 16:04:59,541 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Dyuthi' (Dyuthi.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,541 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Kinnari' (Kinnari-BoldOblique.ttf) oblique normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,541 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Suruma' (Suruma.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,541 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Pothana2000' (Pothana2000.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,542 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Tlwg Typist' (TlwgTypist-BoldOblique.ttf) oblique normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,542 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'padmaa-Bold.1.1' (padmaa-Bold.1.1.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,542 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Kinnari' (Kinnari-Oblique.ttf) oblique normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,543 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Mukti Narrow' (MuktiNarrow.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,543 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Loma' (Loma-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,547 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Ubuntu Mono' (UbuntuMono-RI.ttf) italic normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,548 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Sarai' (Sarai.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,548 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Vemana2000' (vemana2000.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,548 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Lohit Gujarati' (Lohit-Gujarati.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,549 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Lohit Tamil' (Lohit-Tamil.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,549 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Liberation Sans' (LiberationSans-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,550 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Garuda' (Garuda-Oblique.ttf) oblique normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,550 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Sans' (DejaVuSansCondensed-Oblique.ttf) oblique normal 400 condensed>) = 1.25\n",
      "2020-08-12 16:04:59,550 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'FreeSans' (FreeSansBoldOblique.ttf) oblique normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,551 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'KacstDecorative' (KacstDecorative.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,551 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'FreeSerif' (FreeSerif.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,552 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Lohit Malayalam' (Lohit-Malayalam.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,552 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Manjari' (Manjari-Bold.otf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,553 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Tlwg Typist' (TlwgTypist.ttf) normal normal 400 normal>) = 10.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-12 16:04:59,553 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Tlwg Mono' (TlwgMono-BoldOblique.ttf) oblique normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,553 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Liberation Serif' (LiberationSerif-BoldItalic.ttf) italic normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,554 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Kinnari' (Kinnari-BoldItalic.ttf) italic normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,554 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Padauk Book' (PadaukBook-Regular.ttf) normal normal book normal>) = 10.05\n",
      "2020-08-12 16:04:59,555 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Tlwg Mono' (TlwgMono.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,555 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Tlwg Typo' (TlwgTypo-BoldOblique.ttf) oblique normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,555 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Sawasdee' (Sawasdee-BoldOblique.ttf) oblique normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,555 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'KacstFarsi' (KacstFarsi.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,556 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Loma' (Loma-Oblique.ttf) oblique normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,556 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Chilanka' (Chilanka-Regular.ttf) normal normal regular normal>) = 10.05\n",
      "2020-08-12 16:04:59,557 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Noto Sans CJK JP' (NotoSansCJK-Regular.ttc) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,558 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'FreeMono' (FreeMonoOblique.ttf) oblique normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,558 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Samyak Malayalam' (Samyak-Malayalam.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,559 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'KacstScreen' (KacstScreen.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,559 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Serif' (DejaVuSerifCondensed-BoldItalic.ttf) italic normal bold condensed>) = 11.535\n",
      "2020-08-12 16:04:59,559 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Chandas' (chandas1-2.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,560 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'WenQuanYi Micro Hei' (wqy-microhei.ttc) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,560 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Loma' (Loma.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,561 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Tlwg Typewriter' (TlwgTypewriter-Oblique.ttf) oblique normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,561 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Purisa' (Purisa-Oblique.ttf) oblique normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,562 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'KacstTitleL' (KacstTitleL.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,562 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Ubuntu' (Ubuntu-R.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,563 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'KacstOffice' (KacstOffice.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,563 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Mitra Mono' (mitra.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,564 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'KacstPen' (KacstPen.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,564 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'FreeSerif' (FreeSerifBold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,565 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Noto Sans CJK JP' (NotoSansCJK-Bold.ttc) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,565 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Liberation Serif' (LiberationSerif-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,565 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Ubuntu' (Ubuntu-RI.ttf) italic normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,566 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Kalimati' (kalimati.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,566 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Umpush' (Umpush-Light.ttf) normal normal light normal>) = 10.24\n",
      "2020-08-12 16:04:59,567 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'KacstArt' (KacstArt.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,567 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Umpush' (Umpush-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,568 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'FreeMono' (FreeMonoBoldOblique.ttf) oblique normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,568 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Tlwg Typist' (TlwgTypist-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,569 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'AnjaliOldLipi' (AnjaliOldLipi.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,569 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Tlwg Typewriter' (TlwgTypewriter-BoldOblique.ttf) oblique normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,570 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Lohit Kannada' (Lohit-Kannada.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,570 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Lohit Telugu' (Lohit-Telugu.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,571 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Sawasdee' (Sawasdee.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,571 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Lohit Gurmukhi' (Lohit-Gurmukhi.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,571 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Purisa' (Purisa-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,572 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Tlwg Typo' (TlwgTypo-Oblique.ttf) oblique normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,572 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Samyak Gujarati' (Samyak-Gujarati.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,573 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Sans' (DejaVuSans-ExtraLight.ttf) normal normal light normal>) = 0.24\n",
      "2020-08-12 16:04:59,573 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Ubuntu' (Ubuntu-L.ttf) normal normal light normal>) = 10.24\n",
      "2020-08-12 16:04:59,574 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Laksaman' (Laksaman-BoldItalic.ttf) italic normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,574 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Tlwg Typo' (TlwgTypo-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,575 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'RaghuMalayalam' (RaghuMalayalamSans-Regular.ttf) normal normal regular normal>) = 10.05\n",
      "2020-08-12 16:04:59,575 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Ubuntu' (Ubuntu-LI.ttf) italic normal light normal>) = 11.24\n",
      "2020-08-12 16:04:59,575 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Garuda' (Garuda.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,576 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Saab' (Saab.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,576 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Laksaman' (Laksaman-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,577 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'FreeSerif' (FreeSerifBoldItalic.ttf) italic normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,577 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Lohit Bengali' (Lohit-Bengali.ttf) normal normal 400 normal>) = 10.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-12 16:04:59,578 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Manjari' (Manjari-Regular.otf) normal normal regular normal>) = 10.05\n",
      "2020-08-12 16:04:59,578 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Sawasdee' (Sawasdee-Oblique.ttf) oblique normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,579 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Serif' (DejaVuSerifCondensed-Italic.ttf) italic normal 400 condensed>) = 11.25\n",
      "2020-08-12 16:04:59,579 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Purisa' (Purisa.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,580 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Garuda' (Garuda-BoldOblique.ttf) oblique normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,580 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'KacstQurn' (KacstQurn.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,581 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Loma' (Loma-BoldOblique.ttf) oblique normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,581 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Ani' (ani.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,581 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Norasi' (Norasi-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,582 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'KacstNaskh' (KacstNaskh.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,582 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Sans' (DejaVuSansCondensed.ttf) normal normal 400 condensed>) = 0.25\n",
      "2020-08-12 16:04:59,583 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Liberation Sans Narrow' (LiberationSansNarrow-Italic.ttf) italic normal 400 condensed>) = 11.25\n",
      "2020-08-12 16:04:59,583 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'FreeSans' (FreeSansBold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,584 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Padauk' (Padauk-Regular.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,584 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'FreeSans' (FreeSans.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,585 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Tlwg Typewriter' (TlwgTypewriter-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,585 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Ubuntu Mono' (UbuntuMono-BI.ttf) italic normal bold normal>) = 11.335\n",
      "2020-08-12 16:04:59,586 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Ubuntu Condensed' (Ubuntu-C.ttf) normal normal 400 condensed>) = 10.25\n",
      "2020-08-12 16:04:59,586 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Liberation Sans Narrow' (LiberationSansNarrow-Bold.ttf) normal normal bold condensed>) = 10.535\n",
      "2020-08-12 16:04:59,586 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Laksaman' (Laksaman.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,587 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'FreeSans' (FreeSansOblique.ttf) oblique normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,587 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'OpenSymbol' (opens___.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,588 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'KacstTitle' (KacstTitle.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,588 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'KacstLetter' (KacstLetter.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,589 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Tlwg Typo' (TlwgTypo.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,589 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Gubbi' (Gubbi.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,590 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'DejaVu Serif' (DejaVuSerifCondensed.ttf) normal normal 400 condensed>) = 10.25\n",
      "2020-08-12 16:04:59,590 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Kinnari' (Kinnari.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,590 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Kinnari' (Kinnari-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,591 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Laksaman' (Laksaman-Italic.ttf) italic normal 400 normal>) = 11.05\n",
      "2020-08-12 16:04:59,591 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Gargi' (Gargi.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,592 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Waree' (Waree-Bold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,592 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'FreeMono' (FreeMonoBold.ttf) normal normal bold normal>) = 10.335\n",
      "2020-08-12 16:04:59,593 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Umpush' (Umpush.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,593 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Ubuntu Mono' (UbuntuMono-R.ttf) normal normal 400 normal>) = 10.05\n",
      "2020-08-12 16:04:59,594 -1271 - _findfont_cached - DEBUG - findfont: score(<Font 'Padauk Book' (PadaukBook-Bold.ttf) normal normal book normal>) = 10.05\n",
      "2020-08-12 16:04:59,594 -1294 - _findfont_cached - DEBUG - findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000.\n",
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "2020-08-12 16:05:02,106 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:02,107 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:02,108 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:02,109 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:05:02,110 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in seq2seq, inputs shape: torch.Size([5, 10, 62250, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-12 16:05:02,336 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:02,337 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:02,338 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:02,339 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:05:02,340 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:05:02,578 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:02,580 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:02,581 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:02,582 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:05:02,583 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:05:02,898 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:02,900 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:02,901 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:02,903 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:05:02,904 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:05:03,243 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:03,245 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:03,246 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:03,247 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:05:03,249 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:05:03,443 -17 - <module> - DEBUG - compare the shape of outputs and labels:torch.Size([5, 5, 250, 6])torch.Size([5, 5, 250, 6])\n",
      "2020-08-12 16:05:47,453 -23 - <module> - INFO - loss in opoch 0,iteration1:tensor(0.4481, grad_fn=<AddBackward0>)\n",
      "2020-08-12 16:05:50,008 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:50,009 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:50,010 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:50,011 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:05:50,012 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in seq2seq, inputs shape: torch.Size([5, 10, 62250, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-12 16:05:50,252 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:50,254 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:50,255 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:50,256 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:05:50,257 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:05:50,518 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:50,520 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:50,521 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:50,522 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:05:50,524 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:05:50,841 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:50,843 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:50,845 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:50,846 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:05:50,847 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:05:51,189 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:51,191 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:51,193 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:05:51,194 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:05:51,197 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:05:51,390 -17 - <module> - DEBUG - compare the shape of outputs and labels:torch.Size([5, 5, 250, 6])torch.Size([5, 5, 250, 6])\n",
      "2020-08-12 16:06:32,038 -23 - <module> - INFO - loss in opoch 0,iteration2:tensor(0.2897, grad_fn=<AddBackward0>)\n",
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in seq2seq, inputs shape: torch.Size([5, 10, 62250, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-12 16:06:36,290 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:06:36,366 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:06:36,367 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:06:36,368 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:06:36,369 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:06:37,348 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:06:37,350 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:06:37,351 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:06:37,351 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:06:37,352 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:06:38,339 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:06:38,340 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:06:38,341 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:06:38,342 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:06:38,343 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:06:39,327 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:06:39,340 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:06:39,341 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:06:39,341 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:06:39,342 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:06:40,238 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:06:40,239 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:06:40,241 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:06:40,241 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:06:40,244 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:06:40,415 -17 - <module> - DEBUG - compare the shape of outputs and labels:torch.Size([5, 5, 250, 6])torch.Size([5, 5, 250, 6])\n",
      "2020-08-12 16:07:20,579 -23 - <module> - INFO - loss in opoch 0,iteration3:tensor(0.2156, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in seq2seq, inputs shape: torch.Size([5, 10, 62250, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-12 16:07:26,605 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:07:26,654 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:07:26,660 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:07:26,663 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:07:26,671 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:07:27,930 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:07:27,983 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:07:27,989 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:07:27,992 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:07:27,997 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:07:28,756 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:07:28,758 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:07:28,760 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:07:28,760 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:07:28,761 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:07:29,685 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:07:29,727 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:07:29,729 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:07:29,730 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:07:29,731 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:07:30,784 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:07:30,796 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:07:30,797 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:07:30,798 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:07:30,812 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:07:31,052 -17 - <module> - DEBUG - compare the shape of outputs and labels:torch.Size([5, 5, 250, 6])torch.Size([5, 5, 250, 6])\n",
      "2020-08-12 16:08:13,109 -23 - <module> - INFO - loss in opoch 0,iteration4:tensor(0.2244, grad_fn=<AddBackward0>)\n",
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in seq2seq, inputs shape: torch.Size([5, 10, 62250, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-12 16:08:19,302 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:08:19,323 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:08:19,329 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:08:19,332 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:08:19,337 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:08:20,383 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:08:20,428 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:08:20,430 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:08:20,430 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:08:20,432 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:08:21,440 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:08:21,445 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:08:21,447 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:08:21,447 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:08:21,449 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:08:22,434 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:08:22,439 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:08:22,440 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:08:22,441 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:08:22,443 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:08:23,627 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:08:23,688 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:08:23,689 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:08:23,690 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:08:23,693 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:08:23,935 -17 - <module> - DEBUG - compare the shape of outputs and labels:torch.Size([5, 5, 250, 6])torch.Size([5, 5, 250, 6])\n",
      "2020-08-12 16:09:16,229 -23 - <module> - INFO - loss in opoch 0,iteration5:tensor(0.2771, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in seq2seq, inputs shape: torch.Size([5, 10, 62250, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-12 16:09:24,546 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:09:24,587 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:09:24,599 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:09:24,601 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:09:24,605 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:09:25,953 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:09:26,018 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:09:26,019 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:09:26,020 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:09:26,102 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:09:27,017 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:09:27,019 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:09:27,021 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:09:27,022 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:09:27,024 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:09:28,324 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:09:28,326 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:09:28,335 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:09:28,341 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:09:28,345 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:09:29,396 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:09:29,399 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:09:29,401 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:09:29,402 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:09:29,404 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:09:29,723 -17 - <module> - DEBUG - compare the shape of outputs and labels:torch.Size([5, 5, 250, 6])torch.Size([5, 5, 250, 6])\n",
      "2020-08-12 16:10:12,508 -23 - <module> - INFO - loss in opoch 0,iteration6:tensor(0.2684, grad_fn=<AddBackward0>)\n",
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in seq2seq, inputs shape: torch.Size([5, 10, 62250, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-12 16:10:19,623 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:10:19,744 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:10:19,745 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:10:19,746 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:10:19,747 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:10:20,651 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:10:20,653 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:10:20,657 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:10:20,659 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:10:20,662 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:10:21,709 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:10:21,745 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:10:21,749 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:10:21,752 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:10:21,756 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:10:23,183 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:10:23,359 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:10:23,361 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:10:23,362 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:10:23,363 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:10:24,711 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:10:24,739 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:10:24,740 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:10:24,741 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:10:24,743 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:10:25,001 -17 - <module> - DEBUG - compare the shape of outputs and labels:torch.Size([5, 5, 250, 6])torch.Size([5, 5, 250, 6])\n",
      "2020-08-12 16:11:09,448 -23 - <module> - INFO - loss in opoch 0,iteration7:tensor(0.3153, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in seq2seq, inputs shape: torch.Size([5, 10, 62250, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-12 16:11:16,220 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:11:16,240 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:11:16,244 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:11:16,245 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:11:16,246 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:11:17,460 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:11:17,471 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:11:17,473 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:11:17,474 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:11:17,476 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:11:18,642 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:11:18,647 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:11:18,652 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:11:18,653 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:11:18,654 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:11:20,922 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:11:20,924 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:11:20,933 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:11:20,934 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:11:20,935 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:11:23,028 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:11:23,032 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:11:23,033 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:11:23,034 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:11:23,035 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n",
      "2020-08-12 16:11:23,307 -17 - <module> - DEBUG - compare the shape of outputs and labels:torch.Size([5, 5, 250, 6])torch.Size([5, 5, 250, 6])\n",
      "2020-08-12 16:12:13,228 -23 - <module> - INFO - loss in opoch 0,iteration8:tensor(0.3935, grad_fn=<AddBackward0>)\n",
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in seq2seq, inputs shape: torch.Size([5, 10, 62250, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-12 16:12:21,055 -106 - forward - DEBUG - preEffect.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:12:21,084 -108 - forward - DEBUG - Hidden_State_2.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:12:21,086 -110 - forward - DEBUG - effectFromeHiddenState.shape: torch.Size([5, 250, 20])\n",
      "2020-08-12 16:12:21,087 -112 - forward - DEBUG - objectAndEffectTensor.shape: torch.Size([5, 250, 26])\n",
      "2020-08-12 16:12:21,088 -114 - forward - DEBUG - modifiedObject.shape: torch.Size([5, 250, 6])\n"
     ]
    }
   ],
   "source": [
    "if runSeq2SeqRelationModel:\n",
    "    if not isTest:\n",
    "        lossCurve=[]\n",
    "        for epoch in range(epochs):\n",
    "            for iteration in range(itersInEachEpoch):\n",
    "                try:\n",
    "                    inputs, labels=V3iter.__next__()\n",
    "                except StopIteration:\n",
    "                    V3iter=iter(datasetV3Loader)\n",
    "                    inputs, labels=V3iter.__next__()\n",
    "                inputs=fromObjectsToRelationPairsBatchAndTimestepVersion(inputs)\n",
    "                labels=labels.permute(0,1,3,2)\n",
    "                if useGpu:\n",
    "                    inputs=Variable(inputs.cuda())\n",
    "                    labels=Variable(labels.cuda())\n",
    "                outputs=relationLSTMSeq2SeqModel(inputs)\n",
    "                logging.debug(fromAllToStr('compare the shape of outputs and labels:',outputs.shape, labels.shape))\n",
    "                loss=lossFn(outputs,labels)\n",
    "                lossCurve.append(loss)\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                logging.info(fromAllToStr('loss in opoch ',epoch,',iteration',iteration,':',loss))\n",
    "                if(iteration%2==0):\n",
    "                    plt.plot(lossCurve)\n",
    "                    plt.savefig(fromAllToStr('lossCurve iteration ',iteration,'.jpg'))\n",
    "            lrSchedule.step()\n",
    "            if epoch%10==0:\n",
    "                torch.save(relationLSTMSeq2SeqModel.state_dict(),\\\n",
    "                           'positionOnly\\relationLSTMSeq2SeqMode_in_epoch_weightedLossAndPositionOnly_'+str(epoch)+'.pt')\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing seq2seqRelation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetV3Loader=DataLoader(datasetV3Instance,batch_size=2,num_workers=4)\n",
    "V3iter=iter(datasetV3Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 6, 250]) torch.Size([5, 5, 6, 250])\n",
      "torch.Size([5, 10, 62250, 12])\n",
      "in seq2seq, inputs shape: torch.Size([5, 10, 62250, 12])\n",
      "torch.Size([5, 5, 250, 6])\n"
     ]
    }
   ],
   "source": [
    "if isTest:\n",
    "    relationLSTMSeq2SeqModel.eval()\n",
    "    with torch.no_grad():\n",
    "        relationLSTMSeq2SeqModel.load_state_dict(torch.load('relationLSTMSeq2SeqMode_in_epoch_weightedLoss_80.pt'))\n",
    "#         inputs,labels=V3iter.__next__()\n",
    "        itemIndex=8000\n",
    "        inputs,labels=datasetV3Instance.__getitem__(itemIndex)\n",
    "        inputs=inputs.unsqueeze(0)\n",
    "        labels=labels.unsqueeze(0)\n",
    "        for ii in range(4):\n",
    "            newInput, newLabel=datasetV3Instance.__getitem__(itemIndex+ii)\n",
    "            newInput=newInput.unsqueeze(0)\n",
    "            newLabel=newLabel.unsqueeze(0)\n",
    "            inputs=torch.cat((inputs,newInput),0)\n",
    "            labels=torch.cat((labels,newLabel),0)\n",
    "            \n",
    "        print(inputs.shape,labels.shape)\n",
    "\n",
    "        inputs=fromObjectsToRelationPairsBatchAndTimestepVersion(inputs)\n",
    "        print(inputs.shape)\n",
    "        labels=labels.permute(0,1,3,2)\n",
    "        outputs=relationLSTMSeq2SeqModel(inputs)\n",
    "        print(outputs.shape)\n",
    "        normalizationDict=datasetV3Instance.getNormalizationDict()\n",
    "        for i in range(labels.shape[1]):\n",
    "            resultImage=visualizeTensorData(outputs[0,i,:,0],outputs[0,i,:,1],normalizationDict=normalizationDict)\n",
    "            fileName='./predictWithRelationSeq2Seq/'+str(i)+'.png'\n",
    "            cv2.imwrite(fileName,resultImage)\n",
    "            resultImage=visualizeTensorData(labels[0,i,:,0],labels[0,i,:,1],normalizationDict=normalizationDict)\n",
    "            fileName='./predictWithRelationSeq2Seq/'+'l'+str(i)+'.png'\n",
    "            cv2.imwrite(fileName,resultImage)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test lstm relation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start test lstm relation model\n"
     ]
    }
   ],
   "source": [
    "print('start test lstm relation model')\n",
    "if runRelationLSTM:\n",
    "    datasetV3Instance=tensorsDatasetV3(trajectoryFileList)\n",
    "    datasetV3Loader=DataLoader(datasetV3Instance,batch_size=2)\n",
    "    V3iter=iter(datasetV3Loader)\n",
    "    relationLSTMInstance=RelationLSTM()\n",
    "    if useGpu:\n",
    "        relationLSTMInstance.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "def calculateXandN(x,n):\n",
    "    return (1+x)/(n+1+2*x)\n",
    "ns=[]\n",
    "xs=[]\n",
    "for i in range(0,100):\n",
    "    xs.append(i)\n",
    "for i in range(0,10):\n",
    "    ns.append(i)\n",
    "lines=[]\n",
    "for i in range(20):\n",
    "    lines.append([])\n",
    "for x in xs:\n",
    "    for n in ns:\n",
    "        lines[n].append(calculateXandN(x,n))\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(241)\n",
    "plt.plot(xs,lines[0],'b')\n",
    "plt.plot(xs,lines[1],'g')\n",
    "plt.plot(xs,lines[2],'k')\n",
    "plt.plot(xs,lines[3],'y')\n",
    "plt.plot(xs,lines[4],'m')\n",
    "plt.plot(xs,lines[5],'-')\n",
    "plt.subplot(242)\n",
    "plt.plot(ns,lines[0],'b')\n",
    "plt.subplot(243)\n",
    "plt.plot(ns,lines[1],'b')\n",
    "plt.subplot(244)\n",
    "plt.plot(ns,lines[2],'b')\n",
    "plt.subplot(245)\n",
    "plt.plot(ns,lines[3],'b')\n",
    "plt.subplot(246)\n",
    "plt.plot(ns,lines[4],'b')\n",
    "plt.subplot(247)\n",
    "plt.plot(ns,lines[5],'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training relation lstm\n",
    "if runRelationLSTM:\n",
    "    if not isTest:\n",
    "        learningRate=1e-3\n",
    "        MSELoss=nn.MSELoss()\n",
    "        lambdaWholeNet=lambda epoch: 0.5**(epoch//30)\n",
    "        optim=torch.optim.RMSprop([{'params':relationLSTMInstance.parameters(),'initial_lr':learningRate}],lr=learningRate)\n",
    "        lrSchedule=torch.optim.lr_scheduler.LambdaLR(optim,lambdaWholeNet,last_epoch=10)\n",
    "        relationLSTMInstance.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "label.shape torch.Size([2, 10, 6, 250])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/wangyuchen/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "torch.Size([2, 10, 62250, 12])\n",
      "output.shape torch.Size([2, 250, 6])\n",
      "epoch  0  i 1  loss tensor(0.0382, grad_fn=<MseLossBackward>)\n",
      "label.shape torch.Size([2, 10, 6, 250])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "torch.Size([2, 10, 62250, 12])\n",
      "output.shape torch.Size([2, 250, 6])\n",
      "epoch  0  i 2  loss tensor(0.0286, grad_fn=<MseLossBackward>)\n",
      "label.shape torch.Size([2, 10, 6, 250])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "in step,combined.shape torch.Size([2, 250, 40])\n",
      "torch.Size([2, 10, 62250, 12])\n",
      "output.shape torch.Size([2, 250, 6])\n",
      "epoch  0  i 3  loss tensor(0.0229, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-c7320c2e96b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m#             outputView=output.reshape((7,6,250)).cpu()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if runRelationLSTM:\n",
    "    #training relation lstm\n",
    "    if not runOnG814:\n",
    "        %matplotlib inline\n",
    "    from IPython.display import clear_output\n",
    "    normalizationDict=datasetV3Instance.getNormalizationDict()\n",
    "    optim.zero_grad()\n",
    "    if not isTest:  \n",
    "        losses=[]\n",
    "        iterInEpoch=50\n",
    "        for epoch in range(5):\n",
    "            print(epoch)\n",
    "            i=0\n",
    "            for inputs,label in V3iter:\n",
    "                i=i+1\n",
    "                if i>iterInEpoch*(epoch+1):\n",
    "                    break\n",
    "        #         print(i)\n",
    "                inputs=fromObjectsToRelationPairsBatchAndTimestepVersion(inputs)\n",
    "                print('label.shape',label.shape)\n",
    "                label=label[:,0,:,:].squeeze()\n",
    "                label=label.permute(0,2,1)\n",
    "#                 label=label[:,tupleForEachVehicle,0:6]\n",
    "                if useGpu:\n",
    "                    inputs=Variable(inputs.cuda())\n",
    "                    label=Variable(label.cuda())\n",
    "                output=relationLSTMInstance(inputs)\n",
    "\n",
    "        #         print(output[0,0:10,:],secondObjects[0,0:10,:])\n",
    "                print('output.shape',output.shape)\n",
    "                loss=MSELoss(output,label)\n",
    "                if i<5:\n",
    "                    print('epoch ',epoch, ' i', i,' loss',loss)\n",
    "        #         print(loss)\n",
    "                losses.append(loss.item())\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "    #             outputView=output.reshape((7,6,250)).cpu()\n",
    "    #             for j in range(10):\n",
    "    #                 inputs=inputs.cpu()\n",
    "    #                 resultImage=visualizeTensorData(inputs[0,j,0,:],inputs[0,j,1,:],normalizationDict=normalizationDict)\n",
    "    #                 fileName='./predictWithRelationLSTM/'+str((epoch+1)*10000000+i*100000+j)+'.png'\n",
    "    #                 cv2.imwrite(fileName,resultImage)\n",
    "    #             resultImage=visualizeTensorData(outputView[0,0,:],outputView[0,1,:],normalizationDict=normalizationDict)\n",
    "    #             fileName='./predictWithLSTMOnly/'+str((epoch+1)*10000000+i*100000+50)+'.png' #the predicted image is named with string which last two number is 50(because j < 50)\n",
    "    #             cv2.imwrite(fileName,resultImage)\n",
    "            lrSchedule.step()\n",
    "        plt.figure(figsize=(30,30))\n",
    "        plt.plot(losses)\n",
    "        plt.savefig('./losses.png')\n",
    "        torch.save(relationLSTMInstance.state_dict(),'./relationLSTM.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test relation-object model over a period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runObjectRelationNet:\n",
    "    normalizationDict=datasetV2.getNormalizationDict()\n",
    "\n",
    "    datas=[]\n",
    "    for i in range(0,6):\n",
    "        break\n",
    "        datas.append([])\n",
    "    for ii in range(0,2000):\n",
    "        break\n",
    "        theInput,second=datasetV2.__getitem__(ii)\n",
    "        theInput=theInput.unsqueeze(0)\n",
    "        second=second.unsqueeze(0)\n",
    "        #         print(i)\n",
    "        #         theInput,second=item\n",
    "        if useGpu:\n",
    "            theInput=Variable(theInput.cuda())\n",
    "            second=Variable(second.cuda())\n",
    "        inputObjects=theInput[:,tupleForEachVehicle,0:6]\n",
    "        secondObjects=second[:,tupleForEachVehicle,0:6]\n",
    "        for i in range(0,6):\n",
    "            datas[i].append(inputObjects[0,0,i])\n",
    "    #     print('inputObjects.shape',inputObjects.shape)\n",
    "    #     resultImage=visualizeTensorData(inputObjects[0,:,0].cpu(),inputObjects[0,:,1].cpu(),normalizationDict=normalizationDict)\n",
    "    #     fileName='./resultImage/'+str(ii)+'.png'\n",
    "    #     cv2.imwrite(fileName,resultImage)\n",
    "    timeStamp=int(time.time())\n",
    "    dirName='resultImage'+str(timeStamp)\n",
    "    os.mkdir(dirName)\n",
    "    if isTest:\n",
    "        with torch.no_grad():\n",
    "            wholeNet.eval()\n",
    "            theInput,second=datasetV2.__getitem__(5000)\n",
    "            theInput=theInput.unsqueeze(0)\n",
    "            second=second.unsqueeze(0)\n",
    "        #         print(i)\n",
    "    #         theInput,second=item\n",
    "            if useGpu:\n",
    "                theInput=Variable(theInput.cuda())\n",
    "                second=Variable(second.cuda())\n",
    "            inputObjects=theInput[:,tupleForEachVehicle,0:6]\n",
    "            secondObjects=second[:,tupleForEachVehicle,0:6]\n",
    "            print('inputObjects.shape',inputObjects.shape)\n",
    "            resultImage=visualizeTensorData(inputObjects[0,:,0].cpu(),inputObjects[0,:,1].cpu(),normalizationDict=normalizationDict)\n",
    "            fileName='./resultImage/'+'0000000000000000'+'.png'\n",
    "            cv2.imwrite(fileName,resultImage)\n",
    "            stepInput=fromObjectsToRelationPairs(inputObjects[0].permute(1,0)).unsqueeze(0)\n",
    "        #             output=wholeNet(theInput)\n",
    "            print(stepInput.shape)\n",
    "        #             print(output.shape)\n",
    "            #predict step by step\n",
    "            for step in range(500):\n",
    "                output=wholeNet(stepInput)\n",
    "                print('step: ',step)\n",
    "                for ii in range(output.shape[1]):\n",
    "                    print(output[0,ii])\n",
    "    #             break\n",
    "    #             for j in range(10):\n",
    "    #                 print(stepInput[:,tupleForEachVehicle,0:6][0,j,:])\n",
    "    #                 print(output[0,j,:])\n",
    "    #                 print()\n",
    "                stepInput=fromObjectsToRelationPairs(output[0].permute(1,0)).unsqueeze(0)\n",
    "    #             print('outputShape',output.shape)\n",
    "    #             print('outputshape[0]',output[0].shape)\n",
    "                resultImage=visualizeTensorData(output[0,:,0].cpu(),output[0,:,1].cpu(),normalizationDict=normalizationDict)\n",
    "\n",
    "                import os\n",
    "                fileName='./'+dirName+'/'+str(1000000+step)+'.png'\n",
    "                cv2.imwrite(fileName,resultImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training and testing process for 'fromRelationToObjectnetwork'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runObjectRelationNet:\n",
    "    #generate a tuple in which each element is the index of a vehicle\n",
    "    #the tuple is used to get the property of each vehicle from the left side of data from the dataset function\n",
    "    listForEachVehicle=[]\n",
    "    for i in range(maxMatrixIndex):\n",
    "        listForEachVehicle.append(i*(maxMatrixIndex-1))\n",
    "    tupleForEachVehicle=tuple(listForEachVehicle)\n",
    "\n",
    "    dataloaderV2=DataLoader(datasetV2,batch_size=1,shuffle=True)\n",
    "\n",
    "\n",
    "    wholeNet=fromRelationToObjectNetwork()\n",
    "    if isTest:\n",
    "        wholeNet.load_state_dict(torch.load(modelPath))\n",
    "\n",
    "\n",
    "    if useGpu:\n",
    "        wholeNet.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runObjectRelationNet:\n",
    "    if not isTest:\n",
    "        learningRate=1e-3\n",
    "        MSELoss=nn.MSELoss()\n",
    "        lambdaWholeNet=lambda epoch: 0.5**(epoch//30)\n",
    "        optim=torch.optim.RMSprop([{'params':wholeNet.parameters(),'initial_lr':learningRate}],lr=learningRate)\n",
    "        lrSchedule=torch.optim.lr_scheduler.LambdaLR(optim,lambdaWholeNet,last_epoch=10)\n",
    "        wholeNet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if runObjectRelationNet:\n",
    "    if not runOnG814:\n",
    "        %matplotlib inline\n",
    "    from IPython.display import clear_output\n",
    "    if not isTest:  \n",
    "        losses=[]\n",
    "        iterInEpoch=50\n",
    "        for epoch in range(300):\n",
    "            print(epoch)\n",
    "            for i,item in enumerate(dataloaderV2):\n",
    "                if i>iterInEpoch:\n",
    "                    break\n",
    "        #         print(i)\n",
    "                theInput,second=item\n",
    "                if useGpu:\n",
    "                    theInput=Variable(theInput.cuda())\n",
    "                    second=Variable(second.cuda())\n",
    "                secondObjects=second[:,tupleForEachVehicle,0:6]\n",
    "\n",
    "                output=wholeNet(theInput)\n",
    "        #         print(output[0,0:10,:],secondObjects[0,0:10,:])\n",
    "                loss=MSELoss(output,secondObjects)\n",
    "                if i<5:\n",
    "                    print('epoch ',epoch, ' i', i,' loss',loss)\n",
    "        #         print(loss)\n",
    "                losses.append(loss.item())\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            lrSchedule.step()\n",
    "        plt.figure(figsize=(30,30))\n",
    "        plt.plot(losses)\n",
    "        plt.savefig('./losses.png')\n",
    "        torch.save(wholeNet.state_dict(),'./wholeNet_300epoch_50perEpoch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training simple LSTM module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runLSTM:\n",
    "    #lstm version \n",
    "    if not isTest:\n",
    "        learningRate=1e-3\n",
    "        MSELoss=nn.MSELoss()\n",
    "        lambdaWholeNet=lambda epoch: 0.5**(epoch//30)\n",
    "        optim=torch.optim.RMSprop([{'params':lstmModel.parameters(),'initial_lr':learningRate}],lr=learningRate)\n",
    "        lrSchedule=torch.optim.lr_scheduler.LambdaLR(optim,lambdaWholeNet,last_epoch=10)\n",
    "        lstmModel.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if runLSTM:\n",
    "    #lstm version\n",
    "    if not runOnG814:\n",
    "        %matplotlib inline\n",
    "    from IPython.display import clear_output\n",
    "    normalizationDict=datasetV3.getNormalizationDict()\n",
    "    if not isTest:  \n",
    "        losses=[]\n",
    "        iterInEpoch=50\n",
    "        for epoch in range(5):\n",
    "            print(epoch)\n",
    "            i=0\n",
    "            for inputs,label in iterV3:\n",
    "                i=i+1\n",
    "                if i>iterInEpoch*(epoch+1):\n",
    "                    break\n",
    "        #         print(i)\n",
    "                if useGpu:\n",
    "                    inputs=Variable(inputs.cuda())\n",
    "                    label=Variable(label.cuda())\n",
    "                output=lstmModel(inputs.reshape((inputs.shape[0],inputs.shape[1],-1)))\n",
    "\n",
    "        #         print(output[0,0:10,:],secondObjects[0,0:10,:])\n",
    "                loss=MSELoss(output,label.squeeze().reshape((label.shape[0],-1)))\n",
    "                if i<5:\n",
    "                    print('epoch ',epoch, ' i', i,' loss',loss)\n",
    "        #         print(loss)\n",
    "                losses.append(loss.item())\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                outputView=output.reshape((7,6,250)).cpu()\n",
    "                for j in range(10):\n",
    "                    inputs=inputs.cpu()\n",
    "                    resultImage=visualizeTensorData(inputs[0,j,0,:],inputs[0,j,1,:],normalizationDict=normalizationDict)\n",
    "                    fileName='./predictWithLSTMOnly/'+str((epoch+1)*10000000+i*100000+j)+'.png'\n",
    "                    cv2.imwrite(fileName,resultImage)\n",
    "                resultImage=visualizeTensorData(outputView[0,0,:],outputView[0,1,:],normalizationDict=normalizationDict)\n",
    "                fileName='./predictWithLSTMOnly/'+str((epoch+1)*10000000+i*100000+50)+'.png' #the predicted image is named with string which last two number is 50(because j < 50)\n",
    "                cv2.imwrite(fileName,resultImage)\n",
    "            lrSchedule.step()\n",
    "        plt.figure(figsize=(30,30))\n",
    "        plt.plot(losses)\n",
    "        plt.savefig('./losses.png')\n",
    "        torch.save(wholeNet.state_dict(),'./wholeNet_300epoch_50perEpoch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#draw properties of a single tensor over time\n",
    "#output results step by step\n",
    "plt.subplot(321)\n",
    "plt.plot(datas[0])\n",
    "plt.subplot(322)\n",
    "plt.plot(datas[1])\n",
    "plt.subplot(323)\n",
    "plt.plot(datas[2])\n",
    "plt.subplot(324)\n",
    "plt.plot(datas[3])\n",
    "plt.subplot(325)\n",
    "plt.plot(datas[4])\n",
    "plt.subplot(326)\n",
    "plt.plot(datas[5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
