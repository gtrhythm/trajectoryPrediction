{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def getValueByLable(lableList,valueList):\n",
    "    \"\"\"\n",
    "    For instance, given a lable list ['Local_X','Local_Y'] and a value list [2.0, 24.0, 437.0, 1118846981300.0, 16.254, \n",
    "    79.349, 6451167.199, 1873312.382, 14.5, 4.9, 2.0, 39.14, -5.73, 2.0, 0.0, 13.0, 0.0, 0.0] which values sorted by the \n",
    "    order of allLableList below, the function return a value Dict {'Local_X':16.254, 'Local_Y':79.349}\n",
    "    Args:\n",
    "        lableList: the list of lables you've required, such as['Vehicle_ID', 'Total_Frames','Global_Time']\n",
    "        valueList: the list contains all legally value, sorted by:['Vehicle_ID', 'Frame_ID','Total_Frames','Global_Time','Local_X','Local_Y','Global_X','Global_Y',\\\n",
    "                      'v_Length','v_Width','v_Class','v_Vel','v_Acc','Lane_ID','Preceding','Following','Space_Headway',\\\n",
    "                      'Time_Headway']\n",
    "    Returns: \n",
    "        value dict of the input lables\n",
    "    For instance, given a lable list ['Local_X','Local_Y'] and a value list [2.0, 24.0, 437.0, 1118846981300.0, 16.254, \n",
    "    79.349, 6451167.199, 1873312.382, 14.5, 4.9, 2.0, 39.14, -5.73, 2.0, 0.0, 13.0, 0.0, 0.0] which values sorted by the \n",
    "    order of allLableList above, the function return a value List [16.254, 79.349]\n",
    "\n",
    "    \"\"\"\n",
    "    allLableList=['Vehicle_ID', 'Frame_ID','Total_Frames','Global_Time','Local_X','Local_Y','Global_X','Global_Y',\\\n",
    "                  'v_Length','v_Width','v_Class','v_Vel','v_Acc','Lane_ID','Preceding','Following','Space_Headway',\\\n",
    "                  'Time_Headway']\n",
    "    valueDictReturn={}\n",
    "    for lableItem in lableList:\n",
    "        valueDictReturn[lableItem]=valueList[allLableList.index(lableItem)]\n",
    "    return valueDictReturn\n",
    "\n",
    "def rearrangeDataByGlobalTime(allValueLists):\n",
    "    '''\n",
    "    Args:\n",
    "        allValueLists: all values have been read from a txt file which have already been converted to a list\n",
    "    Returns:\n",
    "        dict have been arranged by global time. One single global time generally contains several value lists.\n",
    "    '''\n",
    "    valueDict={}\n",
    "    for valueList in allValueLists:\n",
    "        dictKey=getValueByLable(['Global_Time'],valueList)['Global_Time']\n",
    "        if dictKey in valueDict:\n",
    "            # if dictKey already there, then add valueList to the list of the key\n",
    "            valueDict[dictKey].append(valueList)\n",
    "        else:\n",
    "            #else, create a list and append valueList on it\n",
    "            valueDict[dictKey]=[valueList]\n",
    "    return valueDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def readFirstFrame(matrixIndexAndVehicleIDRecordDictParam, valueLists):\n",
    "    \"\"\"\n",
    "    To generate the first set of tensors from the first frame\n",
    "    Args:\n",
    "        matrixIndexAndVehicleIDRecordDictParam: just as its name\n",
    "        valueLists: a list consists of all valuelist at one time\n",
    "    Returns:\n",
    "        several tensors arranged by: positionTensor, speedTensor, accTensor, angleTensor,newVehicleList(type:list)\n",
    "    \n",
    "    \"\"\"\n",
    "    maxMatrixIndex=matrixIndexAndVehicleIDRecordDictParam.keys().__len__()-1\n",
    "    #tensors initialize\n",
    "    positionTensor=torch.zeros(2,maxMatrixIndex)\n",
    "    speedTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    accTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    angleTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    newVehicleIDList=[]\n",
    "    curMatrixIndex=0\n",
    "    matrixIndexAndVehicleIDRecordDictParam['time']=getValueByLable([\"Global_Time\"],valueLists[0])['Global_Time']\n",
    "    #fill out all tensors\n",
    "    for eachValueList in valueLists:\n",
    "        #get values from eachValueList, generate dict\n",
    "        returnedEachValueDict=getValueByLable(['Vehicle_ID','Local_X','Local_Y','v_Vel','v_Acc'],eachValueList)\n",
    "        #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "        #angle Tensor assignment is not neeed for the initial value of each element in it is already zero\n",
    "        positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "        speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "        accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "        #then handle the record matrix\n",
    "        matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['Vehicle_ID']=returnedEachValueDict['Vehicle_ID']\n",
    "        matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['refresh']=0\n",
    "        newVehicleIDList.append(copy.deepcopy(returnedEachValueDict['Vehicle_ID']))\n",
    "        curMatrixIndex=curMatrixIndex+1\n",
    "    return positionTensor,speedTensor,accTensor,angleTensor,newVehicleIDList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMatrixIndexByVehicleID(matrixIndexAndVehicleIDRecordDictParam, vehicle_ID):\n",
    "    for i in range(0, len(matrixIndexAndVehicleIDRecordDictParam)-1):\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']==vehicle_ID:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def findEmptyMatrixIndex(matrixIndexAndVehicleIDRecordDictParam):\n",
    "    for i in range(0, len(matrixIndexAndVehicleIDRecordDictParam)-1):\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']==-1:\n",
    "            #Vehicle_ID=-1 when there is no existed vehicle ID bounding to the index\n",
    "            return i\n",
    "    raise Exception(\"NO EMPTY ELEMENT IN MATRIX\")\n",
    "\n",
    "def readGeneralFrame(matrixIndexAndVehicleIDRecordDictParam, valueLists, prePositionTensor):\n",
    "    \"\"\"\n",
    "    To generate the first set of tensors from the general frame that have a preceding one\n",
    "    Args:\n",
    "        matrixIndexAndVehicleIDRecordDictParam: just as its name\n",
    "        valueLists: a list consists of all valuelist at one time\n",
    "        prePositionTensor: positionTensor from the preceding frame, which is used to calculate angle tensor\n",
    "    Returns:\n",
    "        everal tensors arranged by: positionTensor, speedTensor, accTensor, angleTensor,newVehicleList(type:list),\n",
    "        vanishedVehicleList(type:list)\n",
    "    \n",
    "    \"\"\"\n",
    "    #tensors initialize\n",
    "    maxMatrixIndex=matrixIndexAndVehicleIDRecordDictParam.keys().__len__()-1\n",
    "    positionTensor=torch.zeros(2,maxMatrixIndex)\n",
    "    speedTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    accTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    angleTensor=torch.zeros(1,maxMatrixIndex)\n",
    "    newVehicleIDList=[]\n",
    "    vanishedVehicleList=[]\n",
    "    curMatrixIndex=0\n",
    "    matrixIndexAndVehicleIDRecordDictParam['time']=getValueByLable([\"Global_Time\"],valueLists[0])['Global_Time']\n",
    "    #fill out all tensors\n",
    "    for eachValueList in valueLists:\n",
    "        #get values from eachValueList, generate dict\n",
    "        returnedEachValueDict=getValueByLable(['Vehicle_ID','Local_X','Local_Y','v_Vel','v_Acc'],eachValueList)\n",
    "        indexOfVehicle=findMatrixIndexByVehicleID(matrixIndexAndVehicleIDRecordDictParam,returnedEachValueDict['Vehicle_ID'])\n",
    "        if indexOfVehicle!=-1:\n",
    "        #index exist then the vehicle already existed in the preceded frame\n",
    "            matrixIndexAndVehicleIDRecordDictParam[indexOfVehicle]['refresh']=1\n",
    "            curMatrixIndex=indexOfVehicle\n",
    "            #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "            positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "            speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "            accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "            angleTensor[:,curMatrixIndex]=math.atan2(positionTensor[0,curMatrixIndex]-\\\n",
    "                                                     prePositionTensor[0,curMatrixIndex],\\\n",
    "                                                    positionTensor[1,curMatrixIndex]-prePositionTensor[1,curMatrixIndex])\n",
    "        else:\n",
    "        #a new vehicle ID\n",
    "            newVehicleIDList.append(copy.deepcopy(returnedEachValueDict['Vehicle_ID']))\n",
    "            curMatrixIndex=findEmptyMatrixIndex(matrixIndexAndVehicleIDRecordDictParam)\n",
    "            matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['Vehicle_ID']=copy.deepcopy(returnedEachValueDict['Vehicle_ID'])\n",
    "            matrixIndexAndVehicleIDRecordDictParam[curMatrixIndex]['refresh']=1\n",
    "            #assign to the curMatrixIndex-th row of corresponding tensor\n",
    "            positionTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['Local_X'],returnedEachValueDict['Local_Y']))\n",
    "            speedTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Vel']))\n",
    "            accTensor[:,curMatrixIndex]=torch.tensor((returnedEachValueDict['v_Acc']))\n",
    "            angleTensor[:,curMatrixIndex]=math.atan2(positionTensor[0,curMatrixIndex]-\\\n",
    "                                                     prePositionTensor[0,curMatrixIndex],\\\n",
    "                                                    positionTensor[1,curMatrixIndex]-prePositionTensor[1,curMatrixIndex])\n",
    "    for i in range(0,maxMatrixIndex):\n",
    "    #find vanished vehicle and remove from dict\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['refresh']==0:\n",
    "            #if refresh=0 then the corresponding vehicle ID was not found in this frame\n",
    "            vanishedVehicleList.append(copy.deepcopy(matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']))\n",
    "            matrixIndexAndVehicleIDRecordDictParam[i]['refresh']=-1\n",
    "            matrixIndexAndVehicleIDRecordDictParam[i]['Vehicle_ID']=-1\n",
    "    \n",
    "    for i in range(0,maxMatrixIndex):\n",
    "    #set all refrshed which equivalent to 1 to 0 to prepare for the next frame\n",
    "        if matrixIndexAndVehicleIDRecordDictParam[i]['refresh']==1:\n",
    "                #if refresh=0 then the corresponding vehicle ID was not found in this frame\n",
    "                matrixIndexAndVehicleIDRecordDictParam[i]['refresh']=0\n",
    "\n",
    "    return positionTensor,speedTensor,accTensor,angleTensor,newVehicleIDList,vanishedVehicleList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as utils\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromDirGenerateDict(trajectoryDir):\n",
    "    trajectoryDataFile=open('/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt')\n",
    "    count=0\n",
    "    allLineList=[]\n",
    "    count=0\n",
    "    for count,line in enumerate(trajectoryDataFile):\n",
    "        #read a single line, remove space and enter\n",
    "        lineList=line.split(' ')\n",
    "        try:\n",
    "            while True:\n",
    "                lineList.remove('')\n",
    "        except:\n",
    "            try:\n",
    "                lineList.remove('\\n')\n",
    "            except:\n",
    "                pass\n",
    "            pass\n",
    "        for i in range(0,lineList.__len__()):\n",
    "            # convert string to float\n",
    "            lineList[i]=float(lineList[i])\n",
    "        allLineList.append(lineList)\n",
    "    valueDict=rearrangeDataByGlobalTime(allLineList)\n",
    "    return valueDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valueDict=fromDirGenerateDict(1)\n",
    "theKey=list(valueDict.keys())[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "def visualizeData(valueVisualize, maxLength=1000,maxWidth=100,blocksize=10):\n",
    "    \"\"\"\n",
    "    visualize a frame on an white image\n",
    "    Args:\n",
    "        valueVisualize: a list of values, each item in the list can be obtained by function \n",
    "        getValueByLable\n",
    "    Returns:\n",
    "        the image of the input frame\n",
    "    \"\"\"\n",
    "    image=np.ones((maxLength,maxWidth,3),dtype=np.int8)\n",
    "    image=image*255\n",
    "#     figure=plt.figure(figsize=(10,50))\n",
    "#     axe=figure.add_subplot(1,1,1)\n",
    "    \n",
    "    for item in valueVisualize:\n",
    "        infoList=getValueByLable(['Vehicle_ID','Local_X','Local_Y'],item)\n",
    "        vehicleID=infoList['Vehicle_ID']\n",
    "        x=int(infoList['Local_X'])\n",
    "        y=int(infoList['Local_Y'])\n",
    "        colorR=int((vehicleID+100)%255)\n",
    "        colorG=int((vehicleID+150)%255)\n",
    "        colorB=int((vehicleID+200)%255)\n",
    "        cv2.circle(image,(x,y),int(blocksize/2),(colorB,colorG,colorR),-1) #\n",
    "#     axe.imshow(image)\n",
    "    return image\n",
    "# visualizeData(valueDict[theKey])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save visulized images\n",
    "for key in list(valueDict.keys())[1:10000]:\n",
    "    image=visualizeData(valueDict[key])\n",
    "    cv2.imwrite('visualizeFolder/image'+str(key)+'.png',image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensorsDataset(Dataset):\n",
    "    def __init__(self, trajectoryFileList, numberOfTensorsEachBatch=100,lableTensorEachBatch=2):\n",
    "        if(numberOfTensorsEachBatch<5):\n",
    "            raise Exception(\"THE NUMBER OF TENSORS IN EACH BATCH IS TOO SMALL\")\n",
    "        self.valueDictList=[]\n",
    "        self.sizeList=[]\n",
    "        self.numberOfTensorsEachBatch=numberOfTensorsEachBatch\n",
    "        self.lableTensorEachBatch=lableTensorEachBatch\n",
    "        self.allTensorsEachBatch=numberOfTensorsEachBatch+lableTensorEachBatch\n",
    "        self.keysList=[]\n",
    "        for eachFile in trajectoryFileList:\n",
    "            valueDict=fromDirGenerateDict(eachFile)\n",
    "            self.valueDictList.append(copy.deepcopy(valueDict))\n",
    "            self.sizeList.append(valueDict.keys().__len__()-self.allTensorsEachBatch)\n",
    "            sortedKeys=list(valueDict.keys())\n",
    "            sortedKeys.sort()\n",
    "            self.keysList.append(copy.deepcopy(sortedKeys))\n",
    "\n",
    "    def __len__(self):\n",
    "        allLen=0\n",
    "        for length in self.sizeList:\n",
    "            allLen=allLen+length\n",
    "        return allLen\n",
    "    \n",
    "    def fromIdxMapToList(self,idx):\n",
    "        \"\"\"\n",
    "        since there are several lists,we have to know which list shoud we use and the ture index in the list\n",
    "        Return:\n",
    "            the trueIndex, listIndex\n",
    "        \"\"\"\n",
    "        countSection=0\n",
    "        for i in range(0,self.sizeList.__len__()):\n",
    "            countSection+=self.sizeList[i]\n",
    "            if(idx<countSection):\n",
    "                return idx-countSection+self.sizeList[i],i\n",
    "            \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #initialize trueIndex, listIndex\n",
    "        #\n",
    "        #this dict record the relation between matrix index and vehicle ID in the last frame\n",
    "        matrixIndexAndVehicleIDRecordDict={}\n",
    "        #initialize dict above\n",
    "        for i in range(0,maxMatrixIndex):\n",
    "            matrixIndexAndVehicleIDRecordDict[i]={'Vehicle_ID':-1,'refresh':-1}\n",
    "        matrixIndexAndVehicleIDRecordDict['time']=-1\n",
    "        trueIndex,listIndex=self.fromIdxMapToList(idx)\n",
    "        itemDict={'positionTensorList':[],'speedTensorList':[],'accTensorList':[],'angleTensorList':[],'time':[]}\n",
    "        valueDict=self.valueDictList[listIndex] #valueDict is the Dict of many frames\n",
    "        dictKeys=self.keysList[listIndex]\n",
    "        #generate tensors of first frame\n",
    "        positionTensor,speedTensor,accTensor,angleTensor,newVehicleList\\\n",
    "        =readFirstFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[trueIndex]])\n",
    "        angleSinTensor=torch.sin(angleTensor)\n",
    "        angleCosTensor=torch.cos(angleTensor)\n",
    "        combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                 speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                 accTensor.mul(angleCosTensor)),0)\n",
    "        allCombineTensorTrain=combinedTensor.unsqueeze(0)\n",
    "        allCombineTensorValid=0\n",
    "#         itemDict['positionTensorList'].append(positionTensor)\n",
    "#         itemDict['speedTensorList'].append(speedTensor)\n",
    "#         itemDict['accTensorList'].append(accTensor)\n",
    "#         itemDict['angleTensorList'].append(angleTensor)\n",
    "#         time=getValueByLable(['Global_Time'],valueDict[dictKeys[trueIndex]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]][0]\n",
    "#         itemDict['time'].append(time)\n",
    "        \n",
    "        for i in range(trueIndex+1,trueIndex+self.allTensorsEachBatch):\n",
    "            positionTensor,speedTensor,accTensor,angleTensor,newVehicleList,vanishedVehicleList\\\n",
    "            =readGeneralFrame(matrixIndexAndVehicleIDRecordDict,valueDict[dictKeys[i]],positionTensor)\n",
    "            angleSinTensor=torch.sin(angleTensor)\n",
    "            angleCosTensor=torch.cos(angleTensor)\n",
    "            combinedTensor=torch.cat((positionTensor,speedTensor.mul(angleSinTensor),\\\n",
    "                                     speedTensor.mul(angleCosTensor),accTensor.mul(angleSinTensor),\\\n",
    "                                     accTensor.mul(angleCosTensor)),0)\n",
    "            if i<self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorTrain=torch.cat((allCombineTensorTrain,combinedTensor.unsqueeze(0)),0)\n",
    "            elif i==self.numberOfTensorsEachBatch+trueIndex:\n",
    "                allCombineTensorValid=combinedTensor.unsqueeze(0)\n",
    "            else:\n",
    "                allCombineTensorValid=torch.cat((allCombineTensorValid,combinedTensor.unsqueeze(0)),0)\n",
    "#             itemDict['positionTensorList'].append(positionTensor)\n",
    "#             itemDict['speedTensorList'].append(speedTensor)\n",
    "#             itemDict['accTensorList'].append(accTensor)\n",
    "#             itemDict['angleTensorList'].append(angleTensor)\n",
    "#             time=getValueByLable(['Global_Time'],valueDict[dictKeys[i]][0]) #valueDict[sortedDitKey[theIndexOfSortedDictKey]]\n",
    "#             itemDict['time'].append(time)\n",
    "        return allCombineTensorTrain,allCombineTensorValid\n",
    "# trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0805am-0820am/trajectories-0805am-0820am.txt',\\\n",
    "#                    '/home/wangyuchen/trajectory_dataset/US101/0820am-0835am/trajectories-0820am-0835am.txt']\n",
    "trajectoryFileList=['/home/wangyuchen/trajectory_dataset/US101/0750am-0805am/trajectories-0750am-0805am.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def generateAdjacencyMatrix(batchedPositionTensor,lambdaX,lambdaY,omegaX,omegaY,m):\n",
    "    \"\"\"\n",
    "    Using batched position tensor generate batched adjacency matrix\n",
    "    Args:\n",
    "        batchedPositionTensor: a batch of position tensor, which size in (batch, timeSequence,2,vehicles), the \n",
    "        value 2 in dim=2 is the position of x and y. \n",
    "        lambda1,lambda2,omega1,omega2,m are parameters of the function. m<1\n",
    "        see detail in my notebook\n",
    "    Returns:\n",
    "        a batch of adjacency matrix\n",
    "    Example:\n",
    "        if given a batch of combined tensor, named theTensor, which size as below:\n",
    "            (4,100,6,250)\n",
    "        which means 4 batches, 100 time step, 6 dimension which respectively of positonx, positony, velocityx, \n",
    "        velocityy, accx,accy.\n",
    "        then we apply the function in such way:\n",
    "        generateAdjacencyMatrix(theTensor(:,:,0:1,:))\n",
    "    \"\"\"\n",
    "    print(batchedPositionTensor.size())\n",
    "    sizeOfEachMatrix=batchedPositionTensor[0,0,0,:].size()[0]\n",
    "    print(sizeOfEachMatrix)\n",
    "    for batchI in range(batchedPositionTensor.size()[0]): #revolve each batch\n",
    "#         print('batchI',batchI)\n",
    "        timeStepsMatrixList=[]\n",
    "        for timeStepI in range(batchedPositionTensor.size()[1]):#revolve each time step\n",
    "#             print('timeStepI:',timeStepI)\n",
    "#             adjacencyMatrix=np.zeros((sizeOfEachMatrix,sizeOfEachMatrix))\n",
    "            adjacencyList=[]\n",
    "            tempPositionList=batchedPositionTensor[batchI,timeStepI,:,:].numpy().tolist()\n",
    "#             start=time.time()\n",
    "            for i in range(sizeOfEachMatrix):\n",
    "                tempLineList=[]\n",
    "                for j in range(sizeOfEachMatrix):\n",
    "#                     adjacencyMatrix[i,j]=1\n",
    "                    if (tempPositionList[1][i]*tempPositionList[1][j]==0):\n",
    "                        toZero=0\n",
    "                    else:\n",
    "                        toZero=1\n",
    "                        \n",
    "                    #calculate original element with linear function\n",
    "#                     tempLineList.append((1-abs(tempPositionList[1][i]-tempPositionList[1][j]))*\\\n",
    "#                         (1-abs(tempPositionList[0][i]-tempPositionList[0][j]))*toZero)\n",
    "                    \n",
    "                    #calculate original element with exponential function\n",
    "                    element=(omegaY/(math.exp(lambdaY*(abs(tempPositionList[1][j]-tempPositionList[1][i])))))*\\\n",
    "                    (omegaX/(math.exp(lambdaX*(abs(tempPositionList[0][j]-tempPositionList[0][i])))))*toZero\n",
    "                    tempLineList.append(element)\n",
    "#                     adjacencyMatrix[i,j]=(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])*\\\n",
    "#                         (batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])\n",
    "#                     (omegaY/math.exp(lambdaX*abs(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])))*\\\n",
    "#                     (omegaX/math.exp(lambdaY*abs(batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])))\n",
    "                    \n",
    "                    #calculate original element with expenential\n",
    "#                     adjacencyMatrix[i,j]=\n",
    "#                     (omegaY/math.exp(lambdaX*abs(batchedPositionTensor[batchI,timeStepI,1,i]-batchedPositionTensor[batchI,timeStepI,1,j])))*\\\n",
    "#                     (omegaX/math.exp(lambdaY*abs(batchedPositionTensor[batchI,timeStepI,0,i]-batchedPositionTensor[batchI,timeStepI,0,j])))\n",
    "                    if(tempPositionList[1][j]-tempPositionList[1][i]<0):\n",
    "                        #if i follows j, then multiple m, m<1\n",
    "                        tempLineList[j]=tempLineList[j]*m\n",
    "                adjacencyList.append(tempLineList)\n",
    "            \n",
    "#             end=time.time()\n",
    "#             print(end-start)\n",
    "            adjacencyMatrix=torch.tensor(adjacencyList).unsqueeze(0)\n",
    "            if timeStepI==0:\n",
    "                matrixSequenceInTimeStepDim=adjacencyMatrix\n",
    "            else:\n",
    "                matrixSequenceInTimeStepDim=\\\n",
    "                torch.cat((matrixSequenceInTimeStepDim,adjacencyMatrix),0)\n",
    "        matrixSequenceInTimeStepDim=matrixSequenceInTimeStepDim.unsqueeze(0)\n",
    "        if batchI==0:\n",
    "            matrixSequenceInBatchDim=matrixSequenceInTimeStepDim\n",
    "        else:\n",
    "            matrixSequenceInBatchDim=torch.cat((matrixSequenceInBatchDim,matrixSequenceInTimeStepDim),0)            \n",
    "    return matrixSequenceInBatchDim\n",
    "\n",
    "def tensorNormalization(inputTensor,minValue,maxValue):\n",
    "    inputTensor.div_(maxValue)\n",
    "    \n",
    "def batchNormalizationForCombinedTensor(inputBatchedTensor,minX,maxX,minY,maxY,minV,maxV,minA,maxA):\n",
    "    tensorNormalization(inputBatchedTensor[:,:,0,:],minX,maxX)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,1,:],minY,maxY)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,2:4,:],minV,maxV)\n",
    "    tensorNormalization(inputBatchedTensor[:,:,4:6,:],minA,maxA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    __init__(self, input_size, cell_size, hidden_size, output_last = True)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, cell_size, hidden_size, output_last = True):\n",
    "        \"\"\"\n",
    "        cell_size is the size of cell_state.\n",
    "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, input, Hidden_State, Cell_State):\n",
    "        combined = torch.cat((input, Hidden_State), 1)\n",
    "        f = F.sigmoid(self.fl(combined))\n",
    "        i = F.sigmoid(self.il(combined))\n",
    "        o = F.sigmoid(self.ol(combined))\n",
    "        C = F.tanh(self.Cl(combined))\n",
    "        Cell_State = f * Cell_State + i * C\n",
    "        Hidden_State = o * F.tanh(Cell_State)\n",
    "        \n",
    "        return Hidden_State, Cell_State\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        time_step = inputs.size(1)\n",
    "        Hidden_State, Cell_State = self.initHidden(batch_size)\n",
    "        \n",
    "        if self.output_last:\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:]), Hidden_State, Cell_State)  \n",
    "            return Hidden_State\n",
    "        else:\n",
    "            outputs = None\n",
    "            for i in range(time_step):\n",
    "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:]), Hidden_State, Cell_State)  \n",
    "                if outputs is None:\n",
    "                    outputs = Hidden_State.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "            return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        if use_gpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            return Hidden_State, Cell_State\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            return Hidden_State, Cell_State\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-537da2307367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "maxMatrixIndex=250\n",
    "trajectorDataSet=tensorsDataset(trajectoryFileList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111\n",
      "torch.Size([1, 100, 6, 250])\n",
      "tensor([2086.5491, 2091.0959, 2095.7219, 2100.3831, 2105.0171, 2109.5540,\n",
      "        2114.0630, 2118.5769, 2123.5779, 2128.5779,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,   34.3370,   38.8370,   43.8370,\n",
      "          48.3370,   52.8370,   57.3370,   61.8370,   66.3370,   70.8370,\n",
      "          75.3360,   79.8510,   84.3370,   88.7620,   93.1630,   97.5870,\n",
      "         102.0720,  106.5860,  111.0860,  115.5850,  120.0850,  124.5850,\n",
      "         129.0850,  133.5850,  138.0840,  142.5840,  147.0830,  151.5810,\n",
      "         156.0980,  160.6000,  165.0390,  169.4130,  173.7160,  177.9790,\n",
      "         182.2460,  186.5390,  190.8710,  195.1590,  199.3130,  203.3530,\n",
      "         207.3780,  211.4850,  215.7260,  220.0740,  224.5130,  228.8920,\n",
      "         233.1030,  237.2560,  241.4850,  245.8470,  250.1680,  254.3230,\n",
      "         258.3220,  262.2570,  266.2310,  270.2930,  274.4410,  278.6440,\n",
      "         282.8690,  287.0770,  291.2440,  295.3850,  299.5270,  303.6540,\n",
      "         307.7330,  311.7570,  315.7470,  319.7410,  323.7550,  327.7640,\n",
      "         331.7230,  335.5860,  339.3330,  343.0240,  346.6370,  350.1540,\n",
      "         353.6470,  357.1830,  360.8160,  364.4820,  368.1130,  371.7670,\n",
      "         375.5030,  379.3650,  383.3320,  387.3460])\n",
      "11111\n",
      "torch.Size([1, 100, 6, 250])\n",
      "tensor([2122.1279, 2124.6279,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,   30.4360,   34.4350,   38.4350,   42.4360,   46.4360,\n",
      "          50.4360,   54.4360,   58.4360,   62.4350,   66.4350,   70.4370,\n",
      "          74.4330,   78.4180,   82.4310,   86.4680,   90.5190,   94.5790,\n",
      "          98.6140,  102.6460,  106.6970,  110.8100,  115.0300,  119.2930,\n",
      "         123.5130,  127.6260,  131.6620,  135.7090,  139.8290,  143.9920,\n",
      "         148.1100,  152.1590,  156.2220,  160.3260,  164.4220,  168.4610,\n",
      "         172.4410,  176.4240,  180.4210,  184.4160,  188.4160,  192.4150,\n",
      "         196.4130,  200.4120,  204.4110,  208.4090,  212.4080,  216.4060,\n",
      "         220.4050,  224.4460,  228.3980,  232.1840,  235.8950,  239.6730,\n",
      "         243.6200,  247.6540,  251.6440,  255.6330,  259.6260,  263.6270,\n",
      "         267.6480,  271.6940,  275.7490,  279.7760,  283.7510,  287.6960,\n",
      "         291.6750,  295.7170,  299.7710,  303.7360,  307.5530,  311.2510,\n",
      "         314.8020,  318.2940,  321.7770,  325.2720,  328.7770,  332.2940,\n",
      "         335.8030,  339.2560,  342.5850,  345.7610,  348.7950,  351.6960,\n",
      "         354.6430,  357.7180,  360.8580,  363.9760,  367.0630,  370.0970,\n",
      "         373.1060,  376.1780,  379.3690,  382.6100])\n",
      "11111\n",
      "torch.Size([1, 100, 6, 250])\n",
      "tensor([1955.7469, 1961.5610, 1967.2810, 1972.9720, 1978.7010, 1984.4330,\n",
      "        1990.1050, 1995.7240, 2001.3440, 2006.9980, 2012.6230, 2018.1630,\n",
      "        2023.6600, 2029.2000, 2034.8250, 2040.4930, 2046.1260, 2051.6760,\n",
      "        2057.1609, 2062.6289, 2068.1101, 2073.6089, 2079.1121, 2084.6121,\n",
      "        2090.1121, 2095.6121, 2101.1130, 2106.6140, 2112.1150, 2117.6150,\n",
      "        2123.1140, 2128.6140,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,   33.9640,   37.9650,   41.4640,\n",
      "          44.9640,   48.4640,   51.9640,   55.4640,   58.9640,   62.4640,\n",
      "          65.9640,   69.4640,   72.9640,   76.4640,   79.9640,   83.4910,\n",
      "          86.9650,   90.3230,   93.6380,   96.9980,  100.4670,  103.9800,\n",
      "         107.4760,  111.0280,  114.6600,  118.3320,  121.9640,  125.5160,\n",
      "         129.0120,  132.4970,  135.9930,  139.4940,  142.9940,  146.4940,\n",
      "         149.9930,  153.4930,  156.9960,  160.5130,  163.9780,  167.3540,\n",
      "         170.6980,  174.0730,  177.5450,  181.1130,  184.8050,  188.6110,\n",
      "         192.5590,  196.5690,  200.5860,  204.5830,  208.5780,  212.5940,\n",
      "         216.6370,  220.7170,  224.8990,  228.9240,  232.8120,  236.5390,\n",
      "         240.4480,  244.4990,  248.7050,  252.7230])\n",
      "11111\n",
      "torch.Size([1, 100, 6, 250])\n",
      "tensor([1989.2050, 1993.0620, 1996.9830, 2000.9960, 2005.0690, 2009.1600,\n",
      "        2013.2740, 2017.4020, 2021.5020, 2025.5031, 2029.3831, 2033.2159,\n",
      "        2037.0720, 2040.9680, 2044.8800, 2048.7900, 2052.6841, 2056.5481,\n",
      "        2060.3979, 2064.2629, 2068.1721, 2072.1230, 2076.1050, 2080.1189,\n",
      "        2084.1270, 2088.0659, 2091.8850, 2095.6140, 2099.3430, 2103.1079,\n",
      "        2106.8760, 2110.6021, 2114.3601, 2118.1140, 2122.1140, 2125.6140,\n",
      "           0.0000,   35.9940,   37.9940,   39.9940,   41.9940,   43.9940,\n",
      "          45.9940,   47.9940,   49.9940,   51.9940,   53.9940,   55.9940,\n",
      "          57.9940,   59.9940,   61.9930,   63.9930,   65.9930,   67.9930,\n",
      "          69.9930,   71.9930,   73.9930,   75.9930,   77.9930,   79.9910,\n",
      "          81.9950,   84.0090,   86.0150,   87.9950,   89.8370,   91.5280,\n",
      "          93.1590,   94.8590,   96.6740,   98.4850,  100.1550,  101.7070,\n",
      "         103.1990,  104.6830,  106.1750,  107.6630,  109.1580,  110.6980,\n",
      "         112.3250,  114.0300,  115.7710,  117.5290,  119.3230,  121.1960,\n",
      "         123.1560,  125.1610,  127.1750,  129.1790,  131.1770,  133.1770,\n",
      "         135.1770,  137.1770,  139.1770,  141.1770,  143.1770,  145.1760,\n",
      "         147.1760,  149.1760,  151.1760,  153.1760])\n",
      "11111\n",
      "torch.Size([1, 100, 6, 250])\n",
      "tensor([2068.4050, 2074.7720, 2081.1680, 2087.6030, 2094.0779, 2100.5759,\n",
      "        2107.0830, 2113.5879, 2120.0879, 2126.5879,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,   33.9650,   39.4640,   44.9640,   50.4640,\n",
      "          55.9640,   61.4350,   66.7470,   71.9470,   76.9940,   82.0000,\n",
      "          86.9660,   91.8780,   96.7710,  101.6860,  106.6720,  111.6880,\n",
      "         116.6760,  121.6620,  126.6780,  131.7650,  136.9460,  142.2070,\n",
      "         147.5090,  152.8350,  158.1240,  163.3610,  168.5960,  173.8280,\n",
      "         178.9840,  183.9750,  188.8330,  193.5150,  198.2250,  203.0940,\n",
      "         208.0960,  213.1930,  218.3130,  223.4590,  228.5400,  233.4850,\n",
      "         238.3380,  243.1280,  247.8730,  252.4900,  256.9520,  261.3880,\n",
      "         265.8650,  270.4230,  275.0780,  279.8220,  284.6080,  289.3760,\n",
      "         294.1300,  298.9400,  303.8360,  308.7550,  313.5770,  318.2620,\n",
      "         322.8120,  327.3030,  331.7870,  336.2820,  340.8060,  345.2870,\n",
      "         349.6830,  354.0290,  358.3500,  362.6600,  366.9100,  371.0820,\n",
      "         375.2200,  379.3270,  383.4270,  387.5540,  391.7110,  395.8400,\n",
      "         399.8930,  403.8900,  407.8750,  411.8690,  415.8700,  419.8710,\n",
      "         423.8660,  427.8500,  431.8480,  435.8960])\n",
      "11111\n",
      "torch.Size([1, 100, 6, 250])\n",
      "tensor([2098.1499, 2101.5530, 2105.0911, 2108.7771, 2112.5869, 2116.4199,\n",
      "        2120.1169, 2124.1179, 2127.6169,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,   38.1040,   41.6040,\n",
      "          44.6040,   48.1040,   51.1040,   54.3080,   57.4470,   60.5740,\n",
      "          63.6220,   66.6210,   69.6060,   72.6020,   75.6030,   78.6030,\n",
      "          81.6030,   84.6560,   87.5670,   90.3340,   92.9800,   95.7470,\n",
      "          98.6580,  101.7110,  104.7110,  107.7040,  110.6960,  113.7080,\n",
      "         116.7580,  119.8600,  123.0090,  126.1890,  129.3800,  132.5680,\n",
      "         135.7490,  138.9430,  142.1510,  145.3210,  148.3420,  151.2430,\n",
      "         153.9870,  156.6540,  159.3540,  162.0960,  164.7750,  167.4230,\n",
      "         170.3240,  173.3940,  176.6640,  180.1470,  183.7480,  187.3680,\n",
      "         190.9740,  194.5850,  198.2130,  201.8030,  205.2170,  208.4970,\n",
      "         211.6390,  214.6080,  217.5570,  220.5370,  223.5460,  226.6000,\n",
      "         229.3280,  231.9300,  234.2440,  236.8320,  239.5420,  242.5260,\n",
      "         245.5820,  248.7840,  252.1020,  255.5670,  259.1170,  262.6860,\n",
      "         266.2430,  269.7960,  273.3630,  276.9230,  280.4330,  283.8370,\n",
      "         287.1280,  290.3730,  293.6560,  297.0130])\n",
      "11111\n",
      "torch.Size([1, 100, 6, 250])\n",
      "tensor([2045.9930, 2050.5239, 2054.9929, 2059.3330, 2063.6250, 2067.9661,\n",
      "        2072.4360, 2076.9661, 2081.4661, 2085.9670, 2090.4670, 2094.9651,\n",
      "        2099.4661, 2103.9761, 2108.4890, 2112.9990, 2117.4990, 2121.9990,\n",
      "        2126.4980,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,   39.0390,   43.0390,   47.0390,   51.0390,\n",
      "          55.0390,   59.0690,   63.0200,   66.8250,   70.5180,   74.0490,\n",
      "          77.5280,   81.0730,   84.7940,   88.4920,   92.0170,   95.4000,\n",
      "          98.8260,  102.4370,  106.0990,  109.6490,  113.1470,  116.6320,\n",
      "         120.1270,  123.6290,  127.1290,  130.6280,  134.1280,  137.6280,\n",
      "         141.1270,  144.6270,  148.1270,  151.6270,  155.1220,  158.6600,\n",
      "         162.1300,  165.4570,  168.7640,  172.2530,  175.8550,  179.6430,\n",
      "         183.5030,  187.4130,  191.2850,  195.0630,  198.7850,  202.5600,\n",
      "         206.4520,  210.4410,  214.4670,  218.4830,  222.5370,  226.4490,\n",
      "         230.2030,  233.7190,  237.5210,  241.4760,  245.6520,  250.0620,\n",
      "         254.4200,  258.8240,  263.2980,  267.8220,  272.3620,  276.8970,\n",
      "         281.4220,  285.9210,  290.3770,  294.8130,  299.2700,  303.7660,\n",
      "         308.2860,  312.8020,  317.3050,  321.8030])\n",
      "11111\n",
      "torch.Size([1, 100, 6, 250])\n",
      "tensor([2067.5149, 2072.0081, 2076.5149, 2081.0149, 2085.5149, 2090.0149,\n",
      "        2094.5139, 2099.0149, 2103.5171, 2108.0200, 2112.5220, 2117.0220,\n",
      "        2121.5220, 2126.0220,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111\n",
      "torch.Size([1, 100, 6, 250])\n",
      "tensor([2079.5911, 2084.0911, 2088.5911, 2093.0891, 2097.5911, 2102.1001,\n",
      "        2106.6121, 2111.1211, 2115.6211, 2120.1211, 2124.6211,    0.0000,\n",
      "           0.0000,   32.5070,   36.5060,   40.5060,   44.5070,   48.5060,\n",
      "          52.5060,   56.5060,   60.5060,   64.5060,   68.5060,   72.5060,\n",
      "          76.5060,   80.5200,   84.5210,   88.4330,   92.2690,   96.1180,\n",
      "         100.0390,  103.9840,  107.8310,  111.5550,  115.2260,  118.8790,\n",
      "         122.4920,  126.0420,  129.5400,  133.0250,  136.5200,  140.0220,\n",
      "         143.5220,  147.0210,  150.5210,  154.0210,  157.5450,  161.0210,\n",
      "         164.3980,  167.7360,  171.1120,  174.5870,  178.1130,  181.6080,\n",
      "         185.0850,  188.5740,  192.1670,  195.9140,  199.7960,  203.8280,\n",
      "         207.9780,  212.1490,  216.3070,  220.4700,  224.6730,  228.5710,\n",
      "         232.2910,  235.8300,  239.0900,  242.5030,  246.0140,  249.4910,\n",
      "         253.0060,  256.5030,  259.9340,  263.3020,  266.6700,  270.1280,\n",
      "         273.7260,  277.4390,  281.1950,  284.9150,  288.5680,  292.1950,\n",
      "         295.8550,  299.5490,  303.2190,  306.8110,  310.3280,  313.8160,\n",
      "         317.3070,  320.8060,  324.3060,  327.8070,  331.3090,  334.8040,\n",
      "         338.2890,  341.8310,  345.3390,  348.7370])\n",
      "11111\n",
      "torch.Size([1, 100, 6, 250])\n",
      "tensor([2116.4199, 2120.1169, 2124.1179, 2127.6169,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,   38.1040,\n",
      "          41.6040,   44.6040,   48.1040,   51.1040,   54.3080,   57.4470,\n",
      "          60.5740,   63.6220,   66.6210,   69.6060,   72.6020,   75.6030,\n",
      "          78.6030,   81.6030,   84.6560,   87.5670,   90.3340,   92.9800,\n",
      "          95.7470,   98.6580,  101.7110,  104.7110,  107.7040,  110.6960,\n",
      "         113.7080,  116.7580,  119.8600,  123.0090,  126.1890,  129.3800,\n",
      "         132.5680,  135.7490,  138.9430,  142.1510,  145.3210,  148.3420,\n",
      "         151.2430,  153.9870,  156.6540,  159.3540,  162.0960,  164.7750,\n",
      "         167.4230,  170.3240,  173.3940,  176.6640,  180.1470,  183.7480,\n",
      "         187.3680,  190.9740,  194.5850,  198.2130,  201.8030,  205.2170,\n",
      "         208.4970,  211.6390,  214.6080,  217.5570,  220.5370,  223.5460,\n",
      "         226.6000,  229.3280,  231.9300,  234.2440,  236.8320,  239.5420,\n",
      "         242.5260,  245.5820,  248.7840,  252.1020,  255.5670,  259.1170,\n",
      "         262.6860,  266.2430,  269.7960,  273.3630,  276.9230,  280.4330,\n",
      "         283.8370,  287.1280,  290.3730,  293.6560,  297.0130,  300.4280,\n",
      "         303.8730,  307.3420,  310.8360,  314.3410])\n",
      "11111\n",
      "torch.Size([1, 100, 6, 250])\n",
      "tensor([1936.8260, 1943.0110, 1949.2120, 1955.4301, 1961.6960, 1967.9800,\n",
      "        1974.2480, 1980.5090, 1986.7930, 1993.1650, 1999.6260, 2006.1240,\n",
      "        2012.6379, 2019.1420, 2025.6400, 2032.1390, 2038.6390, 2045.1440,\n",
      "        2051.6379, 2058.1089, 2064.5720, 2071.0439, 2077.5391, 2084.0439,\n",
      "        2090.5439, 2097.0439, 2103.5459, 2110.0481, 2116.5491, 2123.0500,\n",
      "        2129.5491,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "          33.5740,   37.5740,   41.5740,   45.5740,   49.5740,   53.5740,\n",
      "          57.5740,   61.5730,   65.5730,   69.5730,   73.5730,   77.5740,\n",
      "          81.6130,   85.5750,   89.3730,   93.1100,   96.9080,  100.8700,\n",
      "         104.9080,  108.9060,  112.9100,  116.9250,  120.9260,  124.8770,\n",
      "         128.7590,  132.5900,  136.4050,  140.2220,  144.0700,  147.9670,\n",
      "         151.8640,  155.6820,  159.4370,  163.1170,  166.7640,  170.4860,\n",
      "         174.3250,  178.3050,  182.3410,  186.3580,  190.4050,  194.4850,\n",
      "         198.6000,  202.7440,  206.9070,  211.0680,  215.2330,  219.4060,\n",
      "         223.5500,  227.3920,  231.0700,  234.4680,  237.9580,  241.5730,\n",
      "         245.4490,  249.2790,  253.1100,  256.9360,  260.7570,  264.5700,\n",
      "         268.3890,  272.2200,  276.0420,  279.8200])\n",
      "11111\n"
     ]
    }
   ],
   "source": [
    "dataLoader=DataLoader(trajectorDataSet,batch_size=1,shuffle=True,num_workers=4)\n",
    "for i,data in enumerate(dataLoader):\n",
    "    print('11111')\n",
    "    if(i>10):\n",
    "        break\n",
    "    print(data[0].shape)\n",
    "    print(data[0][0,:,1,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch] *",
   "language": "python",
   "name": "conda-env-.conda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
